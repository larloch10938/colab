{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stragiotti_andrea_2011.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lich070192/colab/blob/main/stragiotti_andrea_2011.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DNj4zyIGKa"
      },
      "source": [
        "# Assignment 1\n",
        "The first assignment has two parts. The first part concerns PyTorch and the second part is about feature engineering for a basic NLP task.\n",
        "## Instructions\n",
        "\n",
        "1.   Make a copy of this notebook \n",
        "  - Click on \"File -> Save a copy in Drive\" and open it in Colab afterwards\n",
        "  - Alternatively, download the notebook and work on it on your local machine. However, keep in mind that you will have to make sure it still runs on Colab afterwards and does not depend on any packages that you installed locally\n",
        "2.   Rename your notebook to **surname_forename_studentnumber.ipynb**\n",
        "  - Make sure to exactly follow this naming scheme (don't replace `_` with `-` or something like that)\n",
        "  - **Failure to comply with this scheme results in -10 points!**\n",
        "3.   For math exercises, use $\\LaTeX$  to typset your answer\n",
        "4.   For coding exercises, insert your code at `# TODO` statements\n",
        "5.   For multiple-choice questions, choose an answer from the drop-down list\n",
        "6.   Before submitting your notebook, **make sure that it runs without errors when executed from start to end on Colab**\n",
        "  - To check this, reload your notebook and the Python kernel, and run the notebook from the first to the last cell\n",
        "  - **If your notebook throws any errors, you will be penalized by -25 points in addition to any penalities from incorrect answers**\n",
        "  - We are not going to fix any errors (no matter how small) to make your code work\n",
        "7.  Download your notebook and submit it on Moodle\n",
        "  - Click on \"File -> Download .ipynb\"\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvB4c3R4II91"
      },
      "source": [
        "## Notebook Setup [don't change!]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0w7l3EpExU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df41275-b3ae-4eac-8221-8cb6c259cea8"
      },
      "source": [
        "%%shell\n",
        "pip install torch"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqig2nu9Eztp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82fe0223-5337-4e3e-e24b-b335bc0626c1"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "torch.__version__ "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDNBpMBGhsj"
      },
      "source": [
        "# Part I: PyTorch [50 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucOi5QgPG3Jg"
      },
      "source": [
        "## Linear Algebra [30 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJ4t0Mk0jlz"
      },
      "source": [
        "### PyTorch Tensors [5 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZNaDhZHKYFL"
      },
      "source": [
        "#### Construct Scaled Identity Matrix [1 point]\n",
        "Given $n \\in \\mathbb{N}$ and $c \\in \\mathbb{R}$, construct a matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\ \\times\\ n}$ where $\\mathbf{X}$ has $c$ on its diagonal and zeros everywhere else."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwYNaam7L0Aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb84944-8d36-44a6-d0aa-3136c5a2401e"
      },
      "source": [
        "def construct_scaled_identity(n, c):\n",
        "  a = torch.zeros((n, n))\n",
        "  # np.fill_diagonal(a, c)\n",
        "  torch.diagonal(a).fill_(c)\n",
        "  return a\n",
        "\n",
        "construct_scaled_identity(4, 3.2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 3.2000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 3.2000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 3.2000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LN669ufJMsq"
      },
      "source": [
        "#### Mean Diagonal [1 point]\n",
        "Given a square matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\ \\times\\ n}$, return the mean of its diagonal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cqjyj6rJ6Pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e71ff6b-0f5f-4bc8-ca9e-5c090b28e414"
      },
      "source": [
        "def mean_diagonal(x):\n",
        "  a = torch.mean(torch.diagonal(x))\n",
        "  return a\n",
        "\n",
        "x = torch.arange(0, 16, dtype=torch.float).view(4, 4)\n",
        "mean_diagonal(x)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv9v5j0AMfpO"
      },
      "source": [
        "#### Indexing [1 point]\n",
        "Given a matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\ \\times\\ m}$ and $i,j \\in \\mathbb{N}$, return the submatrix $\\mathbf{Y}\\in\\mathbb{R}^{i\\ \\times\\ j}$ of the last i rows and last j columns of $\\mathbf{X}$ (i.e. the bottom right submatrix of the given size). You can assume that $i \\leq n$ and $j \\leq m$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXTcoO6Nj_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b2793b-0374-4ff0-c24b-f66b409d1cbf"
      },
      "source": [
        "def bottom_right_matrix(x, i, j):\n",
        "  a = x[-i:, -j:]\n",
        "  return a\n",
        "\n",
        "x = torch.arange(0, 12).view(3, 4)\n",
        "print(x)\n",
        "bottom_right_matrix(x, 2, 2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6,  7],\n",
              "        [10, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWooDl4O0ooy"
      },
      "source": [
        "#### Transpose Sum [2 points]\n",
        "Given a tensor $\\mathcal{X}\\in\\mathbb{R}^{i\\ \\times\\ j\\ \\times\\ k}$, return a transposed tensor $\\mathcal{y}\\in\\mathbb{R}^{j\\ \\times\\ i}$ whose values in the third dimension are summed up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_o8MrXj0nQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2c8194-d99e-4bcd-df32-83139eee67d8"
      },
      "source": [
        "def transpose_sum(x):\n",
        "  a = torch.sum(x, dim=2)\n",
        "  a = torch.transpose(a, 0, 1)\n",
        "  return a\n",
        "\n",
        "x = torch.arange(0, 12).view(2, 3, 2)\n",
        "transpose_sum(x)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1, 13],\n",
              "        [ 5, 17],\n",
              "        [ 9, 21]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhuGQ1I_klmv"
      },
      "source": [
        "### Matrix-vector Multiplication [10 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ7YPXIrIpOg"
      },
      "source": [
        "Implement five unique ways for multiplying a matrix A with a vector b. **Each PyTorch function is allowed to be used in only one of the five implementations**. For instance, if you use `unsqueeze` in one of the methods, you are not allowed to use it for the other five implementations. Furthermore, functions in `torch` and in `torch.Tensor` are treated as the same function (i.e. using `torch.add(x, y)`, `x.add(y)` and `x + y` are all treated as the same function and hence are not allowed to be used in more than one implementation). Your code needs to be applicable to any matrix $A \\in \\mathbb{R}^{n\\ \\times\\ m }$ and vector $b\\in\\mathbb{R}^m$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FPwXxLEIdPL"
      },
      "source": [
        "def matrixvector1(A, b):  \n",
        "  return torch.matmul(A, b)\n",
        "\n",
        "def matrixvector2(A, b):\n",
        "  return A @ b\n",
        "\n",
        "def matrixvector3(A, b):\n",
        "  return A.mm(b.unsqueeze(1)).squeeze(-1)\n",
        "\n",
        "def matrixvector4(A, b):\n",
        "  return torch.einsum('ij,j->i', A, b)\n",
        "\n",
        "def matrixvector5(A, b):\n",
        "  return torch.mv(A, b)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG2Uk7oRkqeO"
      },
      "source": [
        "### Backprop [15 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-AL4Dl8xJF8"
      },
      "source": [
        "#### Forward [2 points]\n",
        "Implement $\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)$ in PyTorch without using a linear layer implementation (i.e. do the matrix-vector mulitplication and addition of a bias term yourself). Note that we are not looking for a batched implementation, so assume $\\mathbf{y},\\mathbf{b} \\in \\mathbb{R}^n, \\mathbf{x}\\in\\mathbb{R}^m$ and $\\mathbf{W}\\in\\mathbb{R}^{n\\ \\times\\ m}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcCqnNPtv0nu"
      },
      "source": [
        "def fw(y, W, x, b):\n",
        "  result = y * torch.tanh(torch.matmul(W, x) + b)\n",
        "  return result"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xav_9KwaxkFL"
      },
      "source": [
        "#### Gradient [10 points]\n",
        "Derive $\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}}\\left[\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)\\right]$ analytically. Here $\\mathbf{z}$ is an _upstream (error) gradient_ and we are interested in calculating the _downstream gradient_ for $\\mathbf{x}$. Make sure to write down all intermediate steps and not just the final result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn3Bu-y8y1Co"
      },
      "source": [
        "Let's define:\n",
        "\\begin{equation}\n",
        "h = \\text{tanh}(z)\n",
        "\\\\\n",
        "z = Wx + b\n",
        "\\end{equation}\n",
        "We can therefore write:\n",
        "\\begin{align}\n",
        "\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}}\\left[\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)\\right] \n",
        "&=\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}]\n",
        "\\end{align}\n",
        "Also, by using the chain rule:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial \\mathbf{x}} = \\frac{\\partial}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}}\n",
        "\\end{equation}\n",
        "So finally:\n",
        "\\begin{equation}\n",
        "\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}] = \\mathbf{z}^\\top\\frac{\\partial}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}]\n",
        "\\\\\n",
        "\\mathbf{z}^\\top\\frac{\\partial}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}] = \\mathbf{z}^\\top diag(\\mathbf{y}\\odot\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}}[h])\n",
        "\\\\\n",
        "\\mathbf{z}^\\top diag(\\mathbf{y}\\odot\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}}[\\text{tanh}(z)] = \\mathbf{z}^\\top diag(\\mathbf{y}\\odot(1 - \\text{tanh}(z)^2))\\frac{\\partial z}{\\partial \\mathbf{x}}[z]\n",
        "\\\\\n",
        "\\mathbf{z}^\\top diag(\\mathbf{y}\\odot(1 - \\text{tanh}(z)^2))\\frac{\\partial z}{\\partial \\mathbf{x}}[z] = \\mathbf{z}^\\top diag(\\mathbf{y}\\odot(1 - \\text{tanh}(z)^2))W\n",
        "\\end{equation}\n",
        "Where:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial h} = diag(\\mathbf{y})\n",
        "\\\\\n",
        "\\frac{\\partial h}{\\partial z} = 1 - tanh(z)^2\n",
        "\\\\\n",
        "\\frac{\\partial z}{\\partial \\mathbf{x}} = W\n",
        "\\end{equation}\n",
        "The first derivative is defined as such by using a property of the Hadamard product where:\n",
        "\\begin{equation}\n",
        "\\mathbf{A}\\odot\\mathbf{X} = diag(\\mathbf{A})\\mathbf{X}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by45qCyMxrBr"
      },
      "source": [
        "#### Backward [3 points]\n",
        "Implement the calculation for $\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}}\\left[\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)\\right]$  in PyTorch (i.e. without using PyTorch Autograd's `.backward`) using your derivation above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqIFSUV7yd6m"
      },
      "source": [
        "def bw(y, W, x, b, grad_output):\n",
        "  result = torch.diag(y * (1 - torch.tanh(torch.matmul(W, x) + b)**2))\n",
        "  result = grad_output.T.matmul(result.matmul(W))\n",
        "  return result"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvBLkJKhG6Rg"
      },
      "source": [
        "## SortBy PyTorch Autograd Function [10 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsLf0kvGG9W4"
      },
      "source": [
        "Implement a PyTorch Autograd function `SortBy` which takes two inputs:\n",
        "- `x` is a matrix of size `m x n` \n",
        "- `s` is an accompanying vector of size `m`\n",
        "\n",
        "`SortBy` should sort the position of the row vectors in `x` using the accompanying scores in `s` in ascending order. For example, given\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{X} &= \\left[\\begin{matrix}\n",
        "0.2 & -0.4 & 0.3\\\\\n",
        "1.2 & 2.3 & -2.1\\\\\n",
        "0.1 & -0.1 & 2\n",
        "\\end{matrix}\\right]\n",
        "&\\mathbf{s} &=\\left[\\begin{matrix}\n",
        "0.2\\\\\n",
        "-0.1\\\\\n",
        "3\n",
        "\\end{matrix}\\right]\n",
        "\\end{align}\n",
        "$$ the forward pass of `SortBy` should return\n",
        "$$\n",
        "\\mathbf{Y} = \\left[\\begin{matrix}\n",
        "1.2 & 2.3 & -2.1\\\\\n",
        "0.2 & -0.4 & 0.3\\\\\n",
        "0.1 & -0.1 & 2\n",
        "\\end{matrix}\\right]\n",
        "$$.\n",
        "\n",
        "Furthermore, given an upstream gradient `grad_output`  (i.e. a matrix of the same size as X), the backward pass of `SortBy` should calculate the gradient of `x`, effectively rerouting the gradient to the original position of the vectors before sorting. For example, if the first row vector of the upstream gradient  in our example above is a vector $\\mathbf{z}$, the gradient of `x` would have $\\mathbf{z}$ as its second row vector.\n",
        "\n",
        "Note that, `SortBy` will only be differentiable w.r.t. to x, and is not be differentiable w.r.t. the sorting procedure to provie a gradient for `s`. **You are not allowed to use any Python loops in your implementation. If you use Python loops for your solution, we will only give you half of the points!**\n",
        "\n",
        "Hints:\n",
        "- You are allowed to use `torch.sort` in your implementation of the forward pass.\n",
        "- Similarly to the example we had in the lecture, you can use the context `ctx` to save tensors on the forward pass that you might need to reuse on the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-YgSz5uQI_Q"
      },
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class SortBy(Function): \n",
        "  @staticmethod\n",
        "  def forward(ctx, x, s):\n",
        "    result = x[s.sort().indices]\n",
        "    ctx.save_for_backward(result, s)\n",
        "    return result\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    result, s = ctx.saved_tensors\n",
        "    return grad_output[s.sort().indices], None"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARIcjqSDsIcm",
        "outputId": "056b890a-e82e-4e92-8311-21ed3599d3d4"
      },
      "source": [
        "x = torch.tensor([[0.2, -0.4, 0.3], [1.2, 2.3, -2.1], [0.1, -0.1, 2.0]], requires_grad = True)\r\n",
        "s = torch.tensor([0.2, 0.1, -3])\r\n",
        "print(s.sort().indices)\r\n",
        "y_test = torch.tensor([[0.1, -0.1, 2.0], [1.2, 2.3, -2.1], [0.2, -0.4, 0.3]], requires_grad = True)\r\n",
        "y = SortBy.apply(x, s)\r\n",
        "print(y)\r\n",
        "y.backward(y_test)\r\n",
        "print(x)\r\n",
        "x_test = x.grad\r\n",
        "x_test"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 1, 0])\n",
            "tensor([[ 0.1000, -0.1000,  2.0000],\n",
            "        [ 1.2000,  2.3000, -2.1000],\n",
            "        [ 0.2000, -0.4000,  0.3000]], grad_fn=<SortByBackward>)\n",
            "tensor([[ 0.2000, -0.4000,  0.3000],\n",
            "        [ 1.2000,  2.3000, -2.1000],\n",
            "        [ 0.1000, -0.1000,  2.0000]], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2000, -0.4000,  0.3000],\n",
              "        [ 1.2000,  2.3000, -2.1000],\n",
              "        [ 0.1000, -0.1000,  2.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_BF7G3UHFcp"
      },
      "source": [
        "## Multiple Choice Quiz [10 points]\n",
        "\n",
        "Answer the following questions by selecting the correct most specific answer (or `None` in case all answers are wrong).\n",
        "\n",
        "1. Which of the following operations cannot be calculated using `@`?\n",
        "2. What is gradient checking for?\n",
        "3. Why don't we use the finite differences method of gradient checking to calculate gradients instead of using backpropagation?\n",
        "4. Which of the following operations cannot be expressed as a single einsum string?\n",
        "5. When should you prefer using `view` instead of `reshape`?\n",
        "6. Which of the following statements is true if you construct a PyTorch tensor from a NumPy array using `torch.from_numpy`?\n",
        "7. Which one is a sufficient condition for being able to broadcast an operation between two tensors?\n",
        "8. What is the difference between a torch.Tensor and a torch.nn.Parameter?\n",
        "9. Given a convex loss function and a sufficiently small learning rate, stochastic gradient descent is guaranteed to?\n",
        "10. Given a non-convex loss function and a very large learning rate, stochastic gradient descent is guaranteed to?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWbew_PsFG5u"
      },
      "source": [
        "#@title Answers { run: \"auto\" }\n",
        "Q1 = \"None of the above\" #@param [\"Matrix-matrix multiplication\", \"Matrix-vector multiplication\", \"Vector-vector multiplication\", \"Tensor-matrix multiplication\", \"Tensor-vector multiplication\", \"None of the above\"]\n",
        "Q2 = \"It tests whether the forward pass of a function is consistent with the backward pass\" #@param [\"It tests whether the forward pass of a function is consistent with the backward pass\", \"It is used at runtime to check for numerical instabilities in the backward pass\", \"It tests wether the function and its gradient have been implemented correctly\", \"It tests whether the norm of the gradients of a function are bounded\", \"None of the above\"]\n",
        "Q3 = \"It would be too slow\" #@param [\"It cannot be used to approximate the gradient accurately enough\", \"It can only be used to calculate the gradient of single functions and not for chained functions which are commonly used in deep learning models\", \"It would be too slow\", \"None of the above\"]\n",
        "Q4 = \"None of the above\" #@param [\"The transpose of an order-three tensor\", \"The sum of the diagonal of a square matrix\", \"The outer product of two matrices\", \"None of the above\"]\n",
        "Q5 = \"When the tensor is contiguous\" #@param [\"When the tensor is non-contiguous\", \"When the tensor is contiguous\", \"None of the above\"]\n",
        "Q6 = \"They point to the same memory and altering one will change the other\" #@param [\"Gradients can be calculated using both, the PyTorch tensor and the NumPy array\", \"They point to the same memory and altering one will change the other\", \"The PyTorch tensor cannot be mapped back to a NumPy array\", \"None of the above\"]\n",
        "Q7 = \"One of the two tensors is a scalar\" #@param [\"One of the two tensors is a scalar\", \"One of the tensors has a singleton dimension\", \"The two tensors have the same number of dimensions\", \"None of the above\"]\n",
        "Q8 = \"Parameters get associated with a model when assigned to a member of the model's modules\" #@param [\"Parameters are mutable and tensors are not\", \"Parameters get associated with a model when assigned to a member of the model's modules\", \"Parameters need to be flattened into vectors whereas tensors can be high-dimensional\", \"None of the above\"]\n",
        "Q9 = \"All of the above\" #@param [\"Find a local optimum\", \"Find the global optimum\", \"All of the above\", \"None of the above\"]\n",
        "Q10 = \"None of the above\" #@param [\"Find a local optimum\", \"Find the global optimum\", \"Converge to a saddle point\", \"All of the above\", \"None of the above\"]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZwY626nGlbb"
      },
      "source": [
        "# Part II: Feature Engineering [50 points]\n",
        "\n",
        "In this section you will develop a logistic regression model for sentiment prediction.  \n",
        "\n",
        "## Setup \n",
        "First we download the [sentence polarity dataset v1.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz) from this [website](http://www.cs.cornell.edu/people/pabo/movie-review-data/) using a few shell commands. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwR7cadCuLD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c984cb69-dfc1-4bf4-e3e0-7e35e132b3c4"
      },
      "source": [
        "%%shell\n",
        "wget http://www.cs.cornell.edu/People/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
        "tar -xzf rt-polaritydata.tar.gz\n",
        "mv rt-polaritydata.README.1.0.txt rt-polaritydata\n",
        "cd rt-polaritydata\n",
        "iconv -f cp1252 -t utf-8 < rt-polarity.neg > rt-polarity.neg.utf8\n",
        "iconv -f cp1252 -t utf-8 < rt-polarity.pos > rt-polarity.pos.utf8\n",
        "perl -ne 'print \"neg\\t\" . $_' <  rt-polarity.neg.utf8 > rt-polarity.neg.utf8.tsv\n",
        "perl -ne 'print \"pos\\t\" . $_' <  rt-polarity.pos.utf8 > rt-polarity.pos.utf8.tsv\n",
        "cat rt-polarity.neg.utf8.tsv rt-polarity.pos.utf8.tsv > rt-polarity.utf8.tsv"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-30 15:21:21--  http://www.cs.cornell.edu/People/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz [following]\n",
            "--2021-01-30 15:21:23--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
            "Reusing existing connection to www.cs.cornell.edu:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 487770 (476K) [application/x-gzip]\n",
            "Saving to: ‘rt-polaritydata.tar.gz.1’\n",
            "\n",
            "rt-polaritydata.tar 100%[===================>] 476.34K   508KB/s    in 0.9s    \n",
            "\n",
            "2021-01-30 15:21:24 (508 KB/s) - ‘rt-polaritydata.tar.gz.1’ saved [487770/487770]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2MTMQSMWPr9"
      },
      "source": [
        "Now we install [AllenNLP](https://allennlp.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf64vki1zt0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1928f799-4c36-48c7-eee9-4fa8d6739c01"
      },
      "source": [
        "%%shell\n",
        "pip install allennlp==0.9"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp==0.9 in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.8.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.5.0)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.2.2)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.0.10)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.6.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.10.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.23.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.7.0+cu101)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (21.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.19.5)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.7.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.16.63)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.1.9)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.1.0)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.4.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.17.0)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.2.5)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2018.9)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.2)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.12.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.5.3)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (4.41.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (5.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.22.2.post1)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp==0.9) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==0.9) (3.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (0.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (8.6.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (20.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (51.3.3)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (1.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==0.9) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp==0.9) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp==0.9) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp==0.9) (0.8)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9) (5.2.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9) (4.5.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9) (1.0.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==0.9) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==0.9) (0.3.4)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.63 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==0.9) (1.19.63)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (2.0.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (1.0.5)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (0.8.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (7.0.8)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp==0.9) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp==0.9) (0.1.95)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp==0.9) (1.8.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp==0.9) (0.2.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp==0.9) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==0.9) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp==0.9) (1.1.1)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (2.6.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (20.8)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (2.9.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (0.16)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (1.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (1.1.4)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.26.3\n",
            "    Uninstalling urllib3-1.26.3:\n",
            "      Successfully uninstalled urllib3-1.26.3\n",
            "Successfully installed urllib3-1.25.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkhShc8WX9F"
      },
      "source": [
        "Next we implement a AllenNLP data loader for this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34_HDMys1sLg"
      },
      "source": [
        "from typing import Iterator, List, Dict, Optional\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "\n",
        "class PolarityDatasetReader(DatasetReader):\n",
        "    \"\"\"\n",
        "    DatasetReader for polarity data like \n",
        "\n",
        "        neg\\tI had better gone to Imperial\n",
        "    \"\"\"\n",
        "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None,\n",
        "                 tokenize_and_preprocess = lambda text: text.split(\" \")) -> None:\n",
        "        super().__init__(lazy=False)\n",
        "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "        self.tokenize_and_preprocess = tokenize_and_preprocess\n",
        "\n",
        "    def text_to_instance(self, tokens: List[Token], label: Optional[str] = None) -> Instance:\n",
        "        sentence_field = TextField(tokens, self.token_indexers)\n",
        "        fields = {\"sentence\": sentence_field}\n",
        "\n",
        "        if label:\n",
        "            label_field = LabelField(label=label)\n",
        "            fields[\"label\"] = label_field\n",
        "\n",
        "        return Instance(fields)\n",
        "\n",
        "    def _tokenize_and_preprocess(text):\n",
        "        return text.split(\" \")\n",
        "\n",
        "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                label, text = line.split(\"\\t\")\n",
        "                tokens = [Token(word) for word in self.tokenize_and_preprocess(text)]\n",
        "                yield self.text_to_instance(tokens, label)\n",
        "\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isDjFO7LXYf-"
      },
      "source": [
        "# Preprocessing [7pts]\n",
        "\n",
        "In order to fit our model, we will need to tokenize and preprocess the data. Write a dataset loader that preprocesses the data.\n",
        "\n",
        "Tokenization is an important field of NLP, and can make a large difference to downstream performance. Luckily fo us, the dataset has already been tokenized, so we just need to split the input text by whitespace to get the tokens. The tokenization is not perfect though. Your preprocessing function should should fix all instances where \"Mr.\" have been tokenized as two tokens to instances where \"Mr.\" is a single token.\n",
        "\n",
        "Implement the above by changing and possibly extending the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3As4zBmSyZhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a171197e-8eba-47c1-b54e-9bd59a59c491"
      },
      "source": [
        "def collapse_mr_dot(text):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      tokens: a list of tokens\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"mr .\", \"mr.\")\n",
        "    text = text.replace(\"mr  .\", \"mr.\")\n",
        "    result = text.split(\" \")    \n",
        "    return result\n",
        "\n",
        "torch.manual_seed(1)\n",
        "reader = PolarityDatasetReader(tokenize_and_preprocess=collapse_mr_dot)\n",
        "data_pre = reader.read(cached_path(\"rt-polaritydata/rt-polarity.utf8.tsv\"))\n",
        "rt_polarity_pre = data_pre\n",
        "for instance in data_pre:\n",
        "  if \"mr.\" in [t.text for t in instance['sentence']]:\n",
        "    print(instance['sentence'][:])\n",
        "    break"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662it [00:01, 7761.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[as, is, often, the, case, with, ambitious, ,, eager, first-time, filmmakers, ,, mr., murray, ,, a, prolific, director, of, music, videos, ,, stuffs, his, debut, with, more, plot, than, it, can, comfortably, hold, ., \n",
            "]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-JYoGSCDjuj"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Below we provide a simple implementation of a model, that combined with the corresponding loss, amounts to logistic regression.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBogFeKugF0v"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Model\n",
        "class LogisticRegression(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Logistic Regression implementation based on torchtext input format.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.normal(torch.zeros(num_features)), \n",
        "                                requires_grad=True)\n",
        "            \n",
        "    def forward(self, sentence):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sentence: a dictionary of ...\n",
        "        \"\"\"\n",
        "        tokens = sentence['tokens']\n",
        "        active_tokens_mask = get_text_field_mask(sentence)\n",
        "        # retrieve weights and set those to zero that come from padding cells \n",
        "        filtered = active_tokens_mask * self.weights[tokens]\n",
        "        # sum pooling along the token position dimension \n",
        "        logits = filtered.sum(dim=1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# model = LogisticRegression(vocab.get_vocab_size(\"tokens\"))\n",
        "# model.forward(sentence)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKGwAaYEEm0-"
      },
      "source": [
        "## Formulation [5pts]\n",
        "\n",
        "In the class we have presented the model as encoder $f(\\mathbf{x})$ follwed by a linear decoder\n",
        "$$s(\\mathbf{x}) = \\boldsymbol{\\theta}^T  f(\\mathbf{x}) = \\boldsymbol{\\theta}^T \\sum_{w\\in \\mathbf{x}} f(w) $$ where $f(\\mathbf{x})$ is the representation of the input text. \n",
        "\n",
        "The implementation here achieves the same output, but the calculation is performed slightly differently due to technical reasons when working with pytorch. Can you give a mathematical description of this implementation here that captures the order in which computation happens? Below $f(w)$ is a one-hot representation of a word, as per lecture 2. \n",
        "\n",
        "The candidate answers are:\n",
        "\n",
        "1. $s(\\mathbf{x}) = \\left[\\sum_{w\\in \\mathbf{x}}  f(w)  \\right]^T \\boldsymbol{\\theta} $\n",
        "2. $s(\\mathbf{x}) = \\sum_{w\\in \\mathbf{x}}  \\boldsymbol{\\theta}^T f(w)$\n",
        "3. $s(\\mathbf{x}) = \\frac{1}{|\\mathbf{x}|}\\boldsymbol{\\theta}^T \\sum_{\\mathbf{x}\\in x}  f(w)$\n",
        "4. $s(\\mathbf{x}) = \\left[\\sum_{w\\in \\mathbf{x}}  f(w)  \\right]^T \\boldsymbol{\\theta} \\frac{1}{|\\mathbf{x}|} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjKT0eJzvyw8"
      },
      "source": [
        "#@title Answers { run: \"auto\" }\n",
        "QFormulation = \"Eq 2\" #@param [\"Eq 1\", \"Eq 2\", \"Eq 3\", \"Eq 4\", \"None of the above\"]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ybvSIOFiLR"
      },
      "source": [
        "## Mean Pooling [8pts]\n",
        "\n",
        "Create a new version of the logistic regression module, using mean pooling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPYSGdhmFpQH"
      },
      "source": [
        "# Model\n",
        "class LogisticRegressionMeanPooling(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Logistic Regression implementation based on torchtext input format.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        super(LogisticRegressionMeanPooling, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.normal(torch.zeros(num_features)), \n",
        "                                requires_grad=True)\n",
        "            \n",
        "    def forward(self, sentence):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sentence: a dictionary of ...\n",
        "        \"\"\"\n",
        "        tokens = sentence['tokens']\n",
        "        active_tokens_mask = get_text_field_mask(sentence)\n",
        "        # retrieve weights and set those to zero that come from padding cells \n",
        "        filtered = active_tokens_mask * self.weights[tokens]\n",
        "        # sum pooling along the token position dimension \n",
        "        logits = filtered.sum(dim=1)\n",
        "        # Get the number of words for each sentence\n",
        "        n = active_tokens_mask.sum(dim=1)\n",
        "        # Divide by the number of words\n",
        "        mean_logits = logits/n\n",
        "        \n",
        "        return mean_logits\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fpGZ88V5YxR",
        "outputId": "9fa1fa47-904d-412c-fb9d-7b4be321e0f4"
      },
      "source": [
        "reader = PolarityDatasetReader()\r\n",
        "data = reader.read(cached_path(\"rt-polaritydata/rt-polarity.utf8.tsv\"))\r\n",
        "vocab = Vocabulary.from_instances(data)\r\n",
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"sentence\", \"num_tokens\")])\r\n",
        "iterator.index_with(vocab)\r\n",
        "for batch in iterator(data[:8], num_epochs=1):\r\n",
        "    sentence = batch['sentence']\r\n",
        "    label = batch['label']"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662it [00:00, 13886.79it/s]\n",
            "100%|██████████| 10662/10662 [00:00<00:00, 57428.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBRZn7KVFWoh",
        "outputId": "fdd39904-e92d-4d20-f6a6-18772f5c4dcf"
      },
      "source": [
        "model = LogisticRegressionMeanPooling(vocab.get_vocab_size(\"tokens\"))\r\n",
        "model.forward(sentence)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.1186, -0.6823, -0.6010, -0.0936, -0.0950, -0.3917,  0.1201, -0.3760],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9QcbnWG6PK"
      },
      "source": [
        "## Add Features [20pts]\n",
        "\n",
        "Add the features below to the preprocessing pipeline shown below. \n",
        "\n",
        "### Bias Feature [6 pts]\n",
        "\n",
        "It is common practice to add a *bias* term of linear classifiers:\n",
        "\n",
        "$$s(\\mathbf{x}) = \\boldsymbol{\\theta}^T  f(\\mathbf{x}) + b $$\n",
        "\n",
        "One way to achieve this in general is to augment $f(\\mathbf{x}) $ with an extra component that is always set to $1$. In our implementation, this can be achieved by augmenting the sentence field appropriately when loading the data, and setting the `add_features` argument in the dataset loader. Implement this below.  \n",
        "\n",
        "### Bigram Feature [7 pts]\n",
        "\n",
        "Use the `add_features` pipeline to implement a feature that captures whether word *pairs* $w_1, w_2$ appear consecutively in the sentence.  This feature should be *combined* with the standard unigram and bias features.   \n",
        "\n",
        "### Max Pooling [7 pts]\n",
        "\n",
        "Use the `add_features` pipeline to implement max pooling such that any feature appearing more than once in the sentence is only counted once, as per the lecture slides of week 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmLgBoZFHms6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d92c0fa-7eb9-49e4-bb80-784aea125265"
      },
      "source": [
        "def add_features(text):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    features: a list of tokens\n",
        "  \"\"\"\n",
        "  # TODO implement this function based on instructions above. \n",
        "  toks = collapse_mr_dot(text)\n",
        "\n",
        "  # Bias Feature\n",
        "  toks.append('--ciao--')\n",
        "\n",
        "  # Biagram\n",
        "  sentence_len = len(toks)\n",
        "  for i in range(sentence_len - 2):\n",
        "    toks.append(toks[i] + ' ' + toks[i + 1])\n",
        "\n",
        "  # Max Pooling\n",
        "  unique_toks = list(dict.fromkeys(toks))\n",
        "  result = list(unique_toks)\n",
        "\n",
        "  return result\n",
        "\n",
        "reader = PolarityDatasetReader(tokenize_and_preprocess=add_features)\n",
        "data_pre_2 = reader.read(cached_path(\"rt-polaritydata/rt-polarity.utf8.tsv\"))\n",
        "data_pre_2[10]['sentence'][:]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662it [00:01, 7520.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[a,\n",
              " sentimental,\n",
              " mess,\n",
              " that,\n",
              " never,\n",
              " rings,\n",
              " true,\n",
              " .,\n",
              " \n",
              ",\n",
              " --ciao--,\n",
              " a sentimental,\n",
              " sentimental mess,\n",
              " mess that,\n",
              " that never,\n",
              " never rings,\n",
              " rings true,\n",
              " true .,\n",
              " . \n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy8JnMkZJQEw"
      },
      "source": [
        "## Hyperparameters Search and Analysis [10 pts]\n",
        "\n",
        "### Early Stopping [5pts]\n",
        "\n",
        "Finding the right number of iterations is important (why can running to convergence be bad?). One way to to do this is to iterate for a max number K, and then choose the iteration with the largest dev set performance. But this can be slow and unnecessary if we assume that dev-set performance doesn't go up again once it starts to go down (dev set performance concave). Implement a variant of the training loop that implements this idea.  Specifically, the loop should terminate if there has been no increase in development set accuracy when comparing the current accuracy to that from 10 epochs ago. \n",
        "\n",
        "### Grid Search [5pts]\n",
        "Using all the features you developed in the above \"Add Features\" section (or the base model in case you could not address the question), find the best combination of \n",
        "\n",
        "* Learning Rate in {1.0, 0.1}\n",
        "* Number of Training epochs (via early stopping, use 1000 as maximum)\n",
        "* L2 regularisation weight in {0.001, 0.0001, 0}\n",
        "\n",
        "After grid search, the value of the variables `best_acc`, `best_l2`, `best_lr` and `best_epochs` should be appropriately. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwHMvGhWo6ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ced1aa-cdf8-49f8-82d9-2f0f9ace1e06"
      },
      "source": [
        "def accuracy(dataset, model, iterator):\n",
        "  # Testing the model and returning the accuracy on the given dataset\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for batch in iterator(dataset, num_epochs=1):\n",
        "      sentence = batch['sentence']\n",
        "      label = batch['label']\n",
        "      output = model(sentence)\n",
        "      total += len(label)\n",
        "      prediction = (output > 0).long()\n",
        "      correct += (prediction == label).sum()\n",
        "\n",
        "  return float(correct) / total  \n",
        "\n",
        "def training_loop(model, iterator, train_set, dev_set, num_epochs=100,\n",
        "                  lr=0.1, weight_decay=0.0):\n",
        "  \"\"\"\n",
        "  Should return the best dev_set accuracy and the number of epochs used. \n",
        "  \"\"\"\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, \n",
        "                              weight_decay=weight_decay)  \n",
        "  # Training the Model\n",
        "  epoch_accuracies = []\n",
        "  best_epoch = 0\n",
        "  best_accuracy = 0.0\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, batch in enumerate(iterator(train_set,num_epochs=1)):\n",
        "          sentence = batch['sentence']\n",
        "          label = batch['label'].float()\n",
        "\n",
        "          # Forward + Backward + Optimize\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(sentence)\n",
        "          loss = criterion(logits, label)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Dev: %.4f' \n",
        "                     % (epoch+1, num_epochs, i+1, len(train_set)//iterator._batch_size, \n",
        "                        loss.data, accuracy(dev_set, model, iterator)))\n",
        "      \n",
        "      epoch_accuracies.append(accuracy(dev_set, model, iterator))\n",
        "      if epoch_accuracies[-1] > best_accuracy:\n",
        "          best_accuracy = epoch_accuracies[-1]\n",
        "          best_epoch = epoch\n",
        "          \n",
        "      # TODO: implement early stopping here  \n",
        "      if len(epoch_accuracies) > 10:\n",
        "        if epoch_accuracies[-1] <= sum(epoch_accuracies[-10:-2])/9:\n",
        "          print(\"Early Stop\")\n",
        "          break            \n",
        "\n",
        "  return best_accuracy, best_epoch\n",
        "\n",
        "reader = PolarityDatasetReader()\n",
        "data = reader.read(cached_path(\"rt-polaritydata/rt-polarity.utf8.tsv\"))\n",
        "training_data = data[0:-1000]\n",
        "dev_data = data[-1000:]\n",
        "vocab = Vocabulary.from_instances(training_data)\n",
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "print(len(training_data))\n",
        "print(len(dev_data))\n",
        "print(len(data))\n",
        "\n",
        "best_acc = 0.0 # best accuracy achieved \n",
        "best_lr = 0.0 # best learning rate at best accuracy \n",
        "best_l2 = 0.0 # best l2 regularizing weight\n",
        "best_epochs = 0 # best number of epochs\n",
        "# TODO: implement grid search to make sure the above 4 variable have \n",
        "for lr in [1.0, 0.1]:\n",
        "  for l2 in [0.001, 0.0001, 0]:\n",
        "    model = LogisticRegression(num_features=vocab.get_vocab_size(\"tokens\"))\n",
        "    acc, epochs = training_loop(model, iterator, training_data, dev_data, num_epochs = 1000, lr=lr, weight_decay = l2)\n",
        "    \n",
        "    if acc > best_acc:\n",
        "      print(epochs)\n",
        "      best_acc = acc\n",
        "      best_lr = lr\n",
        "      best_l2 = l2\n",
        "      best_epochs = epochs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662it [00:00, 12790.49it/s]\n",
            "100%|██████████| 9662/9662 [00:00<00:00, 55395.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9662\n",
            "1000\n",
            "10662\n",
            "Epoch: [1/1000], Step: [100/301], Loss: 0.6128, Dev: 0.3750\n",
            "Epoch: [1/1000], Step: [200/301], Loss: 0.7273, Dev: 0.3630\n",
            "Epoch: [1/1000], Step: [300/301], Loss: 1.2373, Dev: 0.1600\n",
            "Epoch: [2/1000], Step: [100/301], Loss: 0.6713, Dev: 0.5120\n",
            "Epoch: [2/1000], Step: [200/301], Loss: 1.1640, Dev: 0.7320\n",
            "Epoch: [2/1000], Step: [300/301], Loss: 0.5474, Dev: 0.4710\n",
            "Epoch: [3/1000], Step: [100/301], Loss: 0.3635, Dev: 0.3410\n",
            "Epoch: [3/1000], Step: [200/301], Loss: 0.4715, Dev: 0.3600\n",
            "Epoch: [3/1000], Step: [300/301], Loss: 0.5551, Dev: 0.4880\n",
            "Epoch: [4/1000], Step: [100/301], Loss: 0.7258, Dev: 0.7750\n",
            "Epoch: [4/1000], Step: [200/301], Loss: 0.6739, Dev: 0.3180\n",
            "Epoch: [4/1000], Step: [300/301], Loss: 0.6321, Dev: 0.7170\n",
            "Epoch: [5/1000], Step: [100/301], Loss: 0.3911, Dev: 0.5400\n",
            "Epoch: [5/1000], Step: [200/301], Loss: 0.5337, Dev: 0.7030\n",
            "Epoch: [5/1000], Step: [300/301], Loss: 0.4735, Dev: 0.7110\n",
            "Epoch: [6/1000], Step: [100/301], Loss: 0.4410, Dev: 0.6810\n",
            "Epoch: [6/1000], Step: [200/301], Loss: 0.4452, Dev: 0.2100\n",
            "Epoch: [6/1000], Step: [300/301], Loss: 0.5733, Dev: 0.6030\n",
            "Epoch: [7/1000], Step: [100/301], Loss: 0.3849, Dev: 0.7840\n",
            "Epoch: [7/1000], Step: [200/301], Loss: 0.4348, Dev: 0.4990\n",
            "Epoch: [7/1000], Step: [300/301], Loss: 0.4536, Dev: 0.6380\n",
            "Epoch: [8/1000], Step: [100/301], Loss: 0.7702, Dev: 0.1300\n",
            "Epoch: [8/1000], Step: [200/301], Loss: 0.5994, Dev: 0.3740\n",
            "Epoch: [8/1000], Step: [300/301], Loss: 0.5066, Dev: 0.3700\n",
            "Epoch: [9/1000], Step: [100/301], Loss: 0.5475, Dev: 0.5040\n",
            "Epoch: [9/1000], Step: [200/301], Loss: 0.4321, Dev: 0.7780\n",
            "Epoch: [9/1000], Step: [300/301], Loss: 0.5892, Dev: 0.8820\n",
            "Epoch: [10/1000], Step: [100/301], Loss: 0.5760, Dev: 0.7750\n",
            "Epoch: [10/1000], Step: [200/301], Loss: 0.4898, Dev: 0.7470\n",
            "Epoch: [10/1000], Step: [300/301], Loss: 1.5354, Dev: 0.3860\n",
            "Epoch: [11/1000], Step: [100/301], Loss: 0.3332, Dev: 0.8390\n",
            "Epoch: [11/1000], Step: [200/301], Loss: 0.4385, Dev: 0.7300\n",
            "Epoch: [11/1000], Step: [300/301], Loss: 0.3640, Dev: 0.7800\n",
            "Epoch: [12/1000], Step: [100/301], Loss: 0.4754, Dev: 0.6390\n",
            "Epoch: [12/1000], Step: [200/301], Loss: 0.3681, Dev: 0.5350\n",
            "Epoch: [12/1000], Step: [300/301], Loss: 0.4739, Dev: 0.2600\n",
            "Epoch: [13/1000], Step: [100/301], Loss: 0.4401, Dev: 0.7370\n",
            "Epoch: [13/1000], Step: [200/301], Loss: 0.5588, Dev: 0.6230\n",
            "Epoch: [13/1000], Step: [300/301], Loss: 0.4125, Dev: 0.5270\n",
            "Epoch: [14/1000], Step: [100/301], Loss: 0.5152, Dev: 0.6020\n",
            "Epoch: [14/1000], Step: [200/301], Loss: 0.4314, Dev: 0.6290\n",
            "Epoch: [14/1000], Step: [300/301], Loss: 0.5778, Dev: 0.5060\n",
            "Early Stop\n",
            "12\n",
            "Epoch: [1/1000], Step: [100/301], Loss: 1.1661, Dev: 0.2450\n",
            "Epoch: [1/1000], Step: [200/301], Loss: 1.2717, Dev: 0.4130\n",
            "Epoch: [1/1000], Step: [300/301], Loss: 0.7242, Dev: 0.4140\n",
            "Epoch: [2/1000], Step: [100/301], Loss: 1.4389, Dev: 0.2840\n",
            "Epoch: [2/1000], Step: [200/301], Loss: 0.7803, Dev: 0.4580\n",
            "Epoch: [2/1000], Step: [300/301], Loss: 0.4556, Dev: 0.3400\n",
            "Epoch: [3/1000], Step: [100/301], Loss: 0.8061, Dev: 0.3880\n",
            "Epoch: [3/1000], Step: [200/301], Loss: 0.8507, Dev: 0.4110\n",
            "Epoch: [3/1000], Step: [300/301], Loss: 0.6761, Dev: 0.3690\n",
            "Epoch: [4/1000], Step: [100/301], Loss: 0.7057, Dev: 0.3650\n",
            "Epoch: [4/1000], Step: [200/301], Loss: 0.5029, Dev: 0.4060\n",
            "Epoch: [4/1000], Step: [300/301], Loss: 0.7822, Dev: 0.4500\n",
            "Epoch: [5/1000], Step: [100/301], Loss: 0.6151, Dev: 0.3310\n",
            "Epoch: [5/1000], Step: [200/301], Loss: 0.3280, Dev: 0.4090\n",
            "Epoch: [5/1000], Step: [300/301], Loss: 0.4200, Dev: 0.2910\n",
            "Epoch: [6/1000], Step: [100/301], Loss: 0.4204, Dev: 0.3830\n",
            "Epoch: [6/1000], Step: [200/301], Loss: 0.5026, Dev: 0.4270\n",
            "Epoch: [6/1000], Step: [300/301], Loss: 0.5069, Dev: 0.5280\n",
            "Epoch: [7/1000], Step: [100/301], Loss: 0.3123, Dev: 0.3960\n",
            "Epoch: [7/1000], Step: [200/301], Loss: 0.5222, Dev: 0.4060\n",
            "Epoch: [7/1000], Step: [300/301], Loss: 0.3320, Dev: 0.3790\n",
            "Epoch: [8/1000], Step: [100/301], Loss: 0.1464, Dev: 0.4500\n",
            "Epoch: [8/1000], Step: [200/301], Loss: 0.5036, Dev: 0.4860\n",
            "Epoch: [8/1000], Step: [300/301], Loss: 0.4376, Dev: 0.4380\n",
            "Epoch: [9/1000], Step: [100/301], Loss: 0.4526, Dev: 0.4380\n",
            "Epoch: [9/1000], Step: [200/301], Loss: 0.6951, Dev: 0.4460\n",
            "Epoch: [9/1000], Step: [300/301], Loss: 0.4785, Dev: 0.4870\n",
            "Epoch: [10/1000], Step: [100/301], Loss: 0.4697, Dev: 0.4160\n",
            "Epoch: [10/1000], Step: [200/301], Loss: 0.3347, Dev: 0.4210\n",
            "Epoch: [10/1000], Step: [300/301], Loss: 0.3682, Dev: 0.5590\n",
            "Epoch: [11/1000], Step: [100/301], Loss: 0.2128, Dev: 0.4340\n",
            "Epoch: [11/1000], Step: [200/301], Loss: 0.3889, Dev: 0.4570\n",
            "Epoch: [11/1000], Step: [300/301], Loss: 0.4844, Dev: 0.4470\n",
            "Epoch: [12/1000], Step: [100/301], Loss: 0.3557, Dev: 0.5360\n",
            "Epoch: [12/1000], Step: [200/301], Loss: 0.4504, Dev: 0.4780\n",
            "Epoch: [12/1000], Step: [300/301], Loss: 0.2829, Dev: 0.4690\n",
            "Epoch: [13/1000], Step: [100/301], Loss: 0.2157, Dev: 0.4810\n",
            "Epoch: [13/1000], Step: [200/301], Loss: 0.2697, Dev: 0.3990\n",
            "Epoch: [13/1000], Step: [300/301], Loss: 0.2026, Dev: 0.4720\n",
            "Epoch: [14/1000], Step: [100/301], Loss: 0.3159, Dev: 0.4410\n",
            "Epoch: [14/1000], Step: [200/301], Loss: 0.3454, Dev: 0.4980\n",
            "Epoch: [14/1000], Step: [300/301], Loss: 0.3934, Dev: 0.5100\n",
            "Epoch: [15/1000], Step: [100/301], Loss: 0.3790, Dev: 0.4850\n",
            "Epoch: [15/1000], Step: [200/301], Loss: 0.3230, Dev: 0.4470\n",
            "Epoch: [15/1000], Step: [300/301], Loss: 0.2756, Dev: 0.4000\n",
            "Epoch: [16/1000], Step: [100/301], Loss: 0.2571, Dev: 0.4600\n",
            "Epoch: [16/1000], Step: [200/301], Loss: 0.2124, Dev: 0.5480\n",
            "Epoch: [16/1000], Step: [300/301], Loss: 0.1784, Dev: 0.5200\n",
            "Epoch: [17/1000], Step: [100/301], Loss: 0.3327, Dev: 0.5050\n",
            "Epoch: [17/1000], Step: [200/301], Loss: 0.2233, Dev: 0.4810\n",
            "Epoch: [17/1000], Step: [300/301], Loss: 0.4127, Dev: 0.4800\n",
            "Epoch: [18/1000], Step: [100/301], Loss: 0.3591, Dev: 0.5020\n",
            "Epoch: [18/1000], Step: [200/301], Loss: 0.1906, Dev: 0.4980\n",
            "Epoch: [18/1000], Step: [300/301], Loss: 0.3257, Dev: 0.5050\n",
            "Epoch: [19/1000], Step: [100/301], Loss: 0.2219, Dev: 0.4850\n",
            "Epoch: [19/1000], Step: [200/301], Loss: 0.1554, Dev: 0.4470\n",
            "Epoch: [19/1000], Step: [300/301], Loss: 0.1470, Dev: 0.4570\n",
            "Epoch: [20/1000], Step: [100/301], Loss: 0.2636, Dev: 0.5320\n",
            "Epoch: [20/1000], Step: [200/301], Loss: 0.1565, Dev: 0.5080\n",
            "Epoch: [20/1000], Step: [300/301], Loss: 0.2105, Dev: 0.5290\n",
            "Epoch: [21/1000], Step: [100/301], Loss: 0.1925, Dev: 0.4180\n",
            "Epoch: [21/1000], Step: [200/301], Loss: 0.3191, Dev: 0.4570\n",
            "Epoch: [21/1000], Step: [300/301], Loss: 0.1736, Dev: 0.5240\n",
            "Epoch: [22/1000], Step: [100/301], Loss: 0.1994, Dev: 0.4630\n",
            "Epoch: [22/1000], Step: [200/301], Loss: 0.3193, Dev: 0.5410\n",
            "Epoch: [22/1000], Step: [300/301], Loss: 0.1703, Dev: 0.4010\n",
            "Epoch: [23/1000], Step: [100/301], Loss: 0.2921, Dev: 0.5630\n",
            "Epoch: [23/1000], Step: [200/301], Loss: 0.2136, Dev: 0.4570\n",
            "Epoch: [23/1000], Step: [300/301], Loss: 0.2002, Dev: 0.4900\n",
            "Epoch: [24/1000], Step: [100/301], Loss: 0.2013, Dev: 0.4760\n",
            "Epoch: [24/1000], Step: [200/301], Loss: 0.1667, Dev: 0.4720\n",
            "Epoch: [24/1000], Step: [300/301], Loss: 0.1945, Dev: 0.5440\n",
            "Epoch: [25/1000], Step: [100/301], Loss: 0.1576, Dev: 0.4530\n",
            "Epoch: [25/1000], Step: [200/301], Loss: 0.2179, Dev: 0.4900\n",
            "Epoch: [25/1000], Step: [300/301], Loss: 0.1939, Dev: 0.4670\n",
            "Epoch: [26/1000], Step: [100/301], Loss: 0.2267, Dev: 0.5290\n",
            "Epoch: [26/1000], Step: [200/301], Loss: 0.0918, Dev: 0.5140\n",
            "Epoch: [26/1000], Step: [300/301], Loss: 0.1782, Dev: 0.4840\n",
            "Epoch: [27/1000], Step: [100/301], Loss: 0.2516, Dev: 0.5310\n",
            "Epoch: [27/1000], Step: [200/301], Loss: 0.3354, Dev: 0.6160\n",
            "Epoch: [27/1000], Step: [300/301], Loss: 0.2102, Dev: 0.4570\n",
            "Epoch: [28/1000], Step: [100/301], Loss: 0.1272, Dev: 0.5320\n",
            "Epoch: [28/1000], Step: [200/301], Loss: 0.2357, Dev: 0.6130\n",
            "Epoch: [28/1000], Step: [300/301], Loss: 0.1211, Dev: 0.5850\n",
            "Epoch: [29/1000], Step: [100/301], Loss: 0.2243, Dev: 0.5000\n",
            "Epoch: [29/1000], Step: [200/301], Loss: 0.1895, Dev: 0.4850\n",
            "Epoch: [29/1000], Step: [300/301], Loss: 0.2904, Dev: 0.5260\n",
            "Epoch: [30/1000], Step: [100/301], Loss: 0.2352, Dev: 0.5190\n",
            "Epoch: [30/1000], Step: [200/301], Loss: 0.2827, Dev: 0.5410\n",
            "Epoch: [30/1000], Step: [300/301], Loss: 0.1980, Dev: 0.4930\n",
            "Epoch: [31/1000], Step: [100/301], Loss: 0.3460, Dev: 0.4640\n",
            "Epoch: [31/1000], Step: [200/301], Loss: 0.2779, Dev: 0.6330\n",
            "Epoch: [31/1000], Step: [300/301], Loss: 0.2414, Dev: 0.5630\n",
            "Epoch: [32/1000], Step: [100/301], Loss: 0.1762, Dev: 0.5540\n",
            "Epoch: [32/1000], Step: [200/301], Loss: 0.3141, Dev: 0.6460\n",
            "Epoch: [32/1000], Step: [300/301], Loss: 0.2441, Dev: 0.4870\n",
            "Epoch: [33/1000], Step: [100/301], Loss: 0.1691, Dev: 0.4980\n",
            "Epoch: [33/1000], Step: [200/301], Loss: 0.1544, Dev: 0.6340\n",
            "Epoch: [33/1000], Step: [300/301], Loss: 0.3254, Dev: 0.5150\n",
            "Epoch: [34/1000], Step: [100/301], Loss: 0.2200, Dev: 0.5640\n",
            "Epoch: [34/1000], Step: [200/301], Loss: 0.2707, Dev: 0.5330\n",
            "Epoch: [34/1000], Step: [300/301], Loss: 0.3938, Dev: 0.6300\n",
            "Epoch: [35/1000], Step: [100/301], Loss: 0.2703, Dev: 0.5100\n",
            "Epoch: [35/1000], Step: [200/301], Loss: 0.3043, Dev: 0.6120\n",
            "Epoch: [35/1000], Step: [300/301], Loss: 0.2643, Dev: 0.5430\n",
            "Epoch: [36/1000], Step: [100/301], Loss: 0.1885, Dev: 0.6030\n",
            "Epoch: [36/1000], Step: [200/301], Loss: 0.2234, Dev: 0.5230\n",
            "Epoch: [36/1000], Step: [300/301], Loss: 0.3695, Dev: 0.5570\n",
            "Epoch: [37/1000], Step: [100/301], Loss: 0.2477, Dev: 0.5740\n",
            "Epoch: [37/1000], Step: [200/301], Loss: 0.2839, Dev: 0.5710\n",
            "Epoch: [37/1000], Step: [300/301], Loss: 0.2412, Dev: 0.6430\n",
            "Epoch: [38/1000], Step: [100/301], Loss: 0.0914, Dev: 0.5870\n",
            "Epoch: [38/1000], Step: [200/301], Loss: 0.1675, Dev: 0.5410\n",
            "Epoch: [38/1000], Step: [300/301], Loss: 0.2440, Dev: 0.5010\n",
            "Epoch: [39/1000], Step: [100/301], Loss: 0.2968, Dev: 0.6650\n",
            "Epoch: [39/1000], Step: [200/301], Loss: 0.1956, Dev: 0.4880\n",
            "Epoch: [39/1000], Step: [300/301], Loss: 0.2652, Dev: 0.5850\n",
            "Epoch: [40/1000], Step: [100/301], Loss: 0.2199, Dev: 0.5960\n",
            "Epoch: [40/1000], Step: [200/301], Loss: 0.3575, Dev: 0.5120\n",
            "Epoch: [40/1000], Step: [300/301], Loss: 0.3012, Dev: 0.6370\n",
            "Epoch: [41/1000], Step: [100/301], Loss: 0.1737, Dev: 0.6280\n",
            "Epoch: [41/1000], Step: [200/301], Loss: 0.2267, Dev: 0.5110\n",
            "Epoch: [41/1000], Step: [300/301], Loss: 0.2591, Dev: 0.6060\n",
            "Epoch: [42/1000], Step: [100/301], Loss: 0.2684, Dev: 0.6460\n",
            "Epoch: [42/1000], Step: [200/301], Loss: 0.1543, Dev: 0.5200\n",
            "Epoch: [42/1000], Step: [300/301], Loss: 0.1476, Dev: 0.4600\n",
            "Epoch: [43/1000], Step: [100/301], Loss: 0.1764, Dev: 0.5470\n",
            "Epoch: [43/1000], Step: [200/301], Loss: 0.2527, Dev: 0.5620\n",
            "Epoch: [43/1000], Step: [300/301], Loss: 0.2736, Dev: 0.5190\n",
            "Epoch: [44/1000], Step: [100/301], Loss: 0.2004, Dev: 0.4980\n",
            "Epoch: [44/1000], Step: [200/301], Loss: 0.2565, Dev: 0.6600\n",
            "Epoch: [44/1000], Step: [300/301], Loss: 0.2718, Dev: 0.6620\n",
            "Epoch: [45/1000], Step: [100/301], Loss: 0.1767, Dev: 0.5510\n",
            "Epoch: [45/1000], Step: [200/301], Loss: 0.1450, Dev: 0.5590\n",
            "Epoch: [45/1000], Step: [300/301], Loss: 0.2582, Dev: 0.5410\n",
            "Epoch: [46/1000], Step: [100/301], Loss: 0.1039, Dev: 0.6480\n",
            "Epoch: [46/1000], Step: [200/301], Loss: 0.1077, Dev: 0.6000\n",
            "Epoch: [46/1000], Step: [300/301], Loss: 0.2800, Dev: 0.6150\n",
            "Epoch: [47/1000], Step: [100/301], Loss: 0.1912, Dev: 0.5160\n",
            "Epoch: [47/1000], Step: [200/301], Loss: 0.1529, Dev: 0.6000\n",
            "Epoch: [47/1000], Step: [300/301], Loss: 0.2022, Dev: 0.6070\n",
            "Epoch: [48/1000], Step: [100/301], Loss: 0.2728, Dev: 0.5690\n",
            "Epoch: [48/1000], Step: [200/301], Loss: 0.1352, Dev: 0.6030\n",
            "Epoch: [48/1000], Step: [300/301], Loss: 0.2668, Dev: 0.6540\n",
            "Epoch: [49/1000], Step: [100/301], Loss: 0.1729, Dev: 0.5750\n",
            "Epoch: [49/1000], Step: [200/301], Loss: 0.2410, Dev: 0.5460\n",
            "Epoch: [49/1000], Step: [300/301], Loss: 0.2863, Dev: 0.5780\n",
            "Epoch: [50/1000], Step: [100/301], Loss: 0.1742, Dev: 0.6770\n",
            "Epoch: [50/1000], Step: [200/301], Loss: 0.1348, Dev: 0.6270\n",
            "Epoch: [50/1000], Step: [300/301], Loss: 0.1755, Dev: 0.6080\n",
            "Epoch: [51/1000], Step: [100/301], Loss: 0.1564, Dev: 0.6480\n",
            "Epoch: [51/1000], Step: [200/301], Loss: 0.1857, Dev: 0.6070\n",
            "Epoch: [51/1000], Step: [300/301], Loss: 0.2254, Dev: 0.6000\n",
            "Epoch: [52/1000], Step: [100/301], Loss: 0.2215, Dev: 0.6180\n",
            "Epoch: [52/1000], Step: [200/301], Loss: 0.2343, Dev: 0.6480\n",
            "Epoch: [52/1000], Step: [300/301], Loss: 0.2158, Dev: 0.5410\n",
            "Epoch: [53/1000], Step: [100/301], Loss: 0.1565, Dev: 0.6950\n",
            "Epoch: [53/1000], Step: [200/301], Loss: 0.1949, Dev: 0.6880\n",
            "Epoch: [53/1000], Step: [300/301], Loss: 0.2876, Dev: 0.6130\n",
            "Epoch: [54/1000], Step: [100/301], Loss: 0.2357, Dev: 0.6920\n",
            "Epoch: [54/1000], Step: [200/301], Loss: 0.1613, Dev: 0.5800\n",
            "Epoch: [54/1000], Step: [300/301], Loss: 0.1991, Dev: 0.6650\n",
            "Epoch: [55/1000], Step: [100/301], Loss: 0.1456, Dev: 0.6110\n",
            "Epoch: [55/1000], Step: [200/301], Loss: 0.2290, Dev: 0.6350\n",
            "Epoch: [55/1000], Step: [300/301], Loss: 0.3417, Dev: 0.6100\n",
            "Epoch: [56/1000], Step: [100/301], Loss: 0.2750, Dev: 0.5710\n",
            "Epoch: [56/1000], Step: [200/301], Loss: 0.3040, Dev: 0.6880\n",
            "Epoch: [56/1000], Step: [300/301], Loss: 0.1655, Dev: 0.6470\n",
            "Epoch: [57/1000], Step: [100/301], Loss: 0.2027, Dev: 0.6720\n",
            "Epoch: [57/1000], Step: [200/301], Loss: 0.2221, Dev: 0.5890\n",
            "Epoch: [57/1000], Step: [300/301], Loss: 0.2082, Dev: 0.5230\n",
            "Epoch: [58/1000], Step: [100/301], Loss: 0.1300, Dev: 0.5760\n",
            "Epoch: [58/1000], Step: [200/301], Loss: 0.1554, Dev: 0.5560\n",
            "Epoch: [58/1000], Step: [300/301], Loss: 0.3473, Dev: 0.6030\n",
            "Epoch: [59/1000], Step: [100/301], Loss: 0.1615, Dev: 0.7060\n",
            "Epoch: [59/1000], Step: [200/301], Loss: 0.1040, Dev: 0.6810\n",
            "Epoch: [59/1000], Step: [300/301], Loss: 0.2455, Dev: 0.6310\n",
            "Epoch: [60/1000], Step: [100/301], Loss: 0.2019, Dev: 0.6210\n",
            "Epoch: [60/1000], Step: [200/301], Loss: 0.1451, Dev: 0.6020\n",
            "Epoch: [60/1000], Step: [300/301], Loss: 0.3574, Dev: 0.6030\n",
            "Epoch: [61/1000], Step: [100/301], Loss: 0.1170, Dev: 0.5980\n",
            "Epoch: [61/1000], Step: [200/301], Loss: 0.2107, Dev: 0.5900\n",
            "Epoch: [61/1000], Step: [300/301], Loss: 0.2186, Dev: 0.5890\n",
            "Epoch: [62/1000], Step: [100/301], Loss: 0.4006, Dev: 0.6810\n",
            "Epoch: [62/1000], Step: [200/301], Loss: 0.1601, Dev: 0.5930\n",
            "Epoch: [62/1000], Step: [300/301], Loss: 0.1813, Dev: 0.6680\n",
            "Epoch: [63/1000], Step: [100/301], Loss: 0.2540, Dev: 0.5490\n",
            "Epoch: [63/1000], Step: [200/301], Loss: 0.1633, Dev: 0.5820\n",
            "Epoch: [63/1000], Step: [300/301], Loss: 0.1637, Dev: 0.6650\n",
            "Epoch: [64/1000], Step: [100/301], Loss: 0.2871, Dev: 0.5990\n",
            "Epoch: [64/1000], Step: [200/301], Loss: 0.2580, Dev: 0.7110\n",
            "Epoch: [64/1000], Step: [300/301], Loss: 0.2746, Dev: 0.6720\n",
            "Epoch: [65/1000], Step: [100/301], Loss: 0.1785, Dev: 0.5660\n",
            "Epoch: [65/1000], Step: [200/301], Loss: 0.1915, Dev: 0.6040\n",
            "Epoch: [65/1000], Step: [300/301], Loss: 0.2626, Dev: 0.7040\n",
            "Epoch: [66/1000], Step: [100/301], Loss: 0.1696, Dev: 0.7870\n",
            "Epoch: [66/1000], Step: [200/301], Loss: 0.2010, Dev: 0.6080\n",
            "Epoch: [66/1000], Step: [300/301], Loss: 0.0959, Dev: 0.6870\n",
            "Epoch: [67/1000], Step: [100/301], Loss: 0.2378, Dev: 0.6260\n",
            "Epoch: [67/1000], Step: [200/301], Loss: 0.1672, Dev: 0.6720\n",
            "Epoch: [67/1000], Step: [300/301], Loss: 0.1746, Dev: 0.6480\n",
            "Epoch: [68/1000], Step: [100/301], Loss: 0.3225, Dev: 0.6120\n",
            "Epoch: [68/1000], Step: [200/301], Loss: 0.2405, Dev: 0.6800\n",
            "Epoch: [68/1000], Step: [300/301], Loss: 0.2035, Dev: 0.7200\n",
            "Epoch: [69/1000], Step: [100/301], Loss: 0.1716, Dev: 0.6000\n",
            "Epoch: [69/1000], Step: [200/301], Loss: 0.1956, Dev: 0.5830\n",
            "Epoch: [69/1000], Step: [300/301], Loss: 0.2650, Dev: 0.6160\n",
            "Epoch: [70/1000], Step: [100/301], Loss: 0.1733, Dev: 0.6660\n",
            "Epoch: [70/1000], Step: [200/301], Loss: 0.1265, Dev: 0.7090\n",
            "Epoch: [70/1000], Step: [300/301], Loss: 0.1676, Dev: 0.6010\n",
            "Epoch: [71/1000], Step: [100/301], Loss: 0.2178, Dev: 0.5670\n",
            "Epoch: [71/1000], Step: [200/301], Loss: 0.2157, Dev: 0.5680\n",
            "Epoch: [71/1000], Step: [300/301], Loss: 0.1302, Dev: 0.7570\n",
            "Epoch: [72/1000], Step: [100/301], Loss: 0.2167, Dev: 0.7460\n",
            "Epoch: [72/1000], Step: [200/301], Loss: 0.2481, Dev: 0.6490\n",
            "Epoch: [72/1000], Step: [300/301], Loss: 0.2517, Dev: 0.6480\n",
            "Epoch: [73/1000], Step: [100/301], Loss: 0.1237, Dev: 0.6580\n",
            "Epoch: [73/1000], Step: [200/301], Loss: 0.1890, Dev: 0.5670\n",
            "Epoch: [73/1000], Step: [300/301], Loss: 0.2338, Dev: 0.7680\n",
            "Epoch: [74/1000], Step: [100/301], Loss: 0.1983, Dev: 0.6580\n",
            "Epoch: [74/1000], Step: [200/301], Loss: 0.1696, Dev: 0.6930\n",
            "Epoch: [74/1000], Step: [300/301], Loss: 0.1515, Dev: 0.5800\n",
            "Epoch: [75/1000], Step: [100/301], Loss: 0.1332, Dev: 0.7140\n",
            "Epoch: [75/1000], Step: [200/301], Loss: 0.1310, Dev: 0.6100\n",
            "Epoch: [75/1000], Step: [300/301], Loss: 0.3092, Dev: 0.7310\n",
            "Epoch: [76/1000], Step: [100/301], Loss: 0.3061, Dev: 0.6030\n",
            "Epoch: [76/1000], Step: [200/301], Loss: 0.2522, Dev: 0.7730\n",
            "Epoch: [76/1000], Step: [300/301], Loss: 0.1305, Dev: 0.6780\n",
            "Epoch: [77/1000], Step: [100/301], Loss: 0.1810, Dev: 0.7760\n",
            "Epoch: [77/1000], Step: [200/301], Loss: 0.2094, Dev: 0.7050\n",
            "Epoch: [77/1000], Step: [300/301], Loss: 0.1608, Dev: 0.7490\n",
            "Epoch: [78/1000], Step: [100/301], Loss: 0.1194, Dev: 0.6710\n",
            "Epoch: [78/1000], Step: [200/301], Loss: 0.2024, Dev: 0.5650\n",
            "Epoch: [78/1000], Step: [300/301], Loss: 0.1579, Dev: 0.6430\n",
            "Epoch: [79/1000], Step: [100/301], Loss: 0.1616, Dev: 0.6580\n",
            "Epoch: [79/1000], Step: [200/301], Loss: 0.2205, Dev: 0.7120\n",
            "Epoch: [79/1000], Step: [300/301], Loss: 0.1246, Dev: 0.6910\n",
            "Epoch: [80/1000], Step: [100/301], Loss: 0.1256, Dev: 0.5490\n",
            "Epoch: [80/1000], Step: [200/301], Loss: 0.1656, Dev: 0.6100\n",
            "Epoch: [80/1000], Step: [300/301], Loss: 0.1623, Dev: 0.7540\n",
            "Epoch: [81/1000], Step: [100/301], Loss: 0.2079, Dev: 0.6890\n",
            "Epoch: [81/1000], Step: [200/301], Loss: 0.2994, Dev: 0.6970\n",
            "Epoch: [81/1000], Step: [300/301], Loss: 0.1351, Dev: 0.7770\n",
            "Epoch: [82/1000], Step: [100/301], Loss: 0.1577, Dev: 0.7480\n",
            "Epoch: [82/1000], Step: [200/301], Loss: 0.1716, Dev: 0.7100\n",
            "Epoch: [82/1000], Step: [300/301], Loss: 0.1189, Dev: 0.6630\n",
            "Epoch: [83/1000], Step: [100/301], Loss: 0.2745, Dev: 0.6600\n",
            "Epoch: [83/1000], Step: [200/301], Loss: 0.2035, Dev: 0.6070\n",
            "Epoch: [83/1000], Step: [300/301], Loss: 0.1764, Dev: 0.7300\n",
            "Early Stop\n",
            "Epoch: [1/1000], Step: [100/301], Loss: 1.2402, Dev: 0.3780\n",
            "Epoch: [1/1000], Step: [200/301], Loss: 0.8963, Dev: 0.5480\n",
            "Epoch: [1/1000], Step: [300/301], Loss: 0.9980, Dev: 0.5710\n",
            "Epoch: [2/1000], Step: [100/301], Loss: 0.9012, Dev: 0.5390\n",
            "Epoch: [2/1000], Step: [200/301], Loss: 0.9230, Dev: 0.6400\n",
            "Epoch: [2/1000], Step: [300/301], Loss: 0.8192, Dev: 0.6700\n",
            "Epoch: [3/1000], Step: [100/301], Loss: 0.8557, Dev: 0.5050\n",
            "Epoch: [3/1000], Step: [200/301], Loss: 1.6851, Dev: 0.4640\n",
            "Epoch: [3/1000], Step: [300/301], Loss: 0.8164, Dev: 0.5780\n",
            "Epoch: [4/1000], Step: [100/301], Loss: 0.7923, Dev: 0.5320\n",
            "Epoch: [4/1000], Step: [200/301], Loss: 0.7674, Dev: 0.5960\n",
            "Epoch: [4/1000], Step: [300/301], Loss: 0.5177, Dev: 0.6150\n",
            "Epoch: [5/1000], Step: [100/301], Loss: 0.6956, Dev: 0.6910\n",
            "Epoch: [5/1000], Step: [200/301], Loss: 0.5767, Dev: 0.6120\n",
            "Epoch: [5/1000], Step: [300/301], Loss: 0.4065, Dev: 0.6840\n",
            "Epoch: [6/1000], Step: [100/301], Loss: 0.8687, Dev: 0.6780\n",
            "Epoch: [6/1000], Step: [200/301], Loss: 1.0116, Dev: 0.6400\n",
            "Epoch: [6/1000], Step: [300/301], Loss: 0.3402, Dev: 0.6760\n",
            "Epoch: [7/1000], Step: [100/301], Loss: 0.4305, Dev: 0.6600\n",
            "Epoch: [7/1000], Step: [200/301], Loss: 0.5795, Dev: 0.7070\n",
            "Epoch: [7/1000], Step: [300/301], Loss: 0.5494, Dev: 0.6650\n",
            "Epoch: [8/1000], Step: [100/301], Loss: 0.6778, Dev: 0.6010\n",
            "Epoch: [8/1000], Step: [200/301], Loss: 0.9018, Dev: 0.6330\n",
            "Epoch: [8/1000], Step: [300/301], Loss: 0.5276, Dev: 0.6210\n",
            "Epoch: [9/1000], Step: [100/301], Loss: 0.4831, Dev: 0.6750\n",
            "Epoch: [9/1000], Step: [200/301], Loss: 0.1945, Dev: 0.7120\n",
            "Epoch: [9/1000], Step: [300/301], Loss: 0.4733, Dev: 0.6370\n",
            "Epoch: [10/1000], Step: [100/301], Loss: 0.3770, Dev: 0.7030\n",
            "Epoch: [10/1000], Step: [200/301], Loss: 0.5386, Dev: 0.8000\n",
            "Epoch: [10/1000], Step: [300/301], Loss: 0.3924, Dev: 0.6950\n",
            "Epoch: [11/1000], Step: [100/301], Loss: 0.4185, Dev: 0.6820\n",
            "Epoch: [11/1000], Step: [200/301], Loss: 0.3290, Dev: 0.6810\n",
            "Epoch: [11/1000], Step: [300/301], Loss: 0.4454, Dev: 0.6870\n",
            "Epoch: [12/1000], Step: [100/301], Loss: 0.4609, Dev: 0.6170\n",
            "Epoch: [12/1000], Step: [200/301], Loss: 0.3440, Dev: 0.6310\n",
            "Epoch: [12/1000], Step: [300/301], Loss: 0.3340, Dev: 0.6110\n",
            "Epoch: [13/1000], Step: [100/301], Loss: 0.3330, Dev: 0.7280\n",
            "Epoch: [13/1000], Step: [200/301], Loss: 0.3760, Dev: 0.6890\n",
            "Epoch: [13/1000], Step: [300/301], Loss: 0.4486, Dev: 0.6710\n",
            "Epoch: [14/1000], Step: [100/301], Loss: 0.4857, Dev: 0.7520\n",
            "Epoch: [14/1000], Step: [200/301], Loss: 0.2126, Dev: 0.6830\n",
            "Epoch: [14/1000], Step: [300/301], Loss: 0.2680, Dev: 0.6770\n",
            "Epoch: [15/1000], Step: [100/301], Loss: 0.6110, Dev: 0.7090\n",
            "Epoch: [15/1000], Step: [200/301], Loss: 0.3517, Dev: 0.7410\n",
            "Epoch: [15/1000], Step: [300/301], Loss: 0.4700, Dev: 0.6540\n",
            "Epoch: [16/1000], Step: [100/301], Loss: 0.3782, Dev: 0.7450\n",
            "Epoch: [16/1000], Step: [200/301], Loss: 0.3666, Dev: 0.6800\n",
            "Epoch: [16/1000], Step: [300/301], Loss: 0.3091, Dev: 0.6410\n",
            "Epoch: [17/1000], Step: [100/301], Loss: 0.2825, Dev: 0.7140\n",
            "Epoch: [17/1000], Step: [200/301], Loss: 0.1846, Dev: 0.7150\n",
            "Epoch: [17/1000], Step: [300/301], Loss: 0.4259, Dev: 0.7510\n",
            "Epoch: [18/1000], Step: [100/301], Loss: 0.3954, Dev: 0.6990\n",
            "Epoch: [18/1000], Step: [200/301], Loss: 0.1915, Dev: 0.6180\n",
            "Epoch: [18/1000], Step: [300/301], Loss: 0.3296, Dev: 0.7800\n",
            "Epoch: [19/1000], Step: [100/301], Loss: 0.2180, Dev: 0.7220\n",
            "Epoch: [19/1000], Step: [200/301], Loss: 0.1101, Dev: 0.7390\n",
            "Epoch: [19/1000], Step: [300/301], Loss: 0.1317, Dev: 0.6820\n",
            "Epoch: [20/1000], Step: [100/301], Loss: 0.2410, Dev: 0.7230\n",
            "Epoch: [20/1000], Step: [200/301], Loss: 0.2228, Dev: 0.6830\n",
            "Epoch: [20/1000], Step: [300/301], Loss: 0.2181, Dev: 0.6930\n",
            "Epoch: [21/1000], Step: [100/301], Loss: 0.1548, Dev: 0.7380\n",
            "Epoch: [21/1000], Step: [200/301], Loss: 0.3717, Dev: 0.7330\n",
            "Epoch: [21/1000], Step: [300/301], Loss: 0.3106, Dev: 0.7160\n",
            "Epoch: [22/1000], Step: [100/301], Loss: 0.2732, Dev: 0.7170\n",
            "Epoch: [22/1000], Step: [200/301], Loss: 0.1785, Dev: 0.7500\n",
            "Epoch: [22/1000], Step: [300/301], Loss: 0.1805, Dev: 0.6460\n",
            "Epoch: [23/1000], Step: [100/301], Loss: 0.2078, Dev: 0.6550\n",
            "Epoch: [23/1000], Step: [200/301], Loss: 0.2862, Dev: 0.6880\n",
            "Epoch: [23/1000], Step: [300/301], Loss: 0.2239, Dev: 0.7230\n",
            "Epoch: [24/1000], Step: [100/301], Loss: 0.3253, Dev: 0.6810\n",
            "Epoch: [24/1000], Step: [200/301], Loss: 0.2060, Dev: 0.7190\n",
            "Epoch: [24/1000], Step: [300/301], Loss: 0.2556, Dev: 0.7090\n",
            "Epoch: [25/1000], Step: [100/301], Loss: 0.2914, Dev: 0.6900\n",
            "Epoch: [25/1000], Step: [200/301], Loss: 0.2180, Dev: 0.6890\n",
            "Epoch: [25/1000], Step: [300/301], Loss: 0.1789, Dev: 0.7160\n",
            "Epoch: [26/1000], Step: [100/301], Loss: 0.1474, Dev: 0.6930\n",
            "Epoch: [26/1000], Step: [200/301], Loss: 0.3301, Dev: 0.6750\n",
            "Epoch: [26/1000], Step: [300/301], Loss: 0.1473, Dev: 0.6420\n",
            "Early Stop\n",
            "Epoch: [1/1000], Step: [100/301], Loss: 1.0634, Dev: 0.3280\n",
            "Epoch: [1/1000], Step: [200/301], Loss: 1.6066, Dev: 0.2890\n",
            "Epoch: [1/1000], Step: [300/301], Loss: 2.0399, Dev: 0.2570\n",
            "Epoch: [2/1000], Step: [100/301], Loss: 1.0485, Dev: 0.3080\n",
            "Epoch: [2/1000], Step: [200/301], Loss: 1.5865, Dev: 0.2840\n",
            "Epoch: [2/1000], Step: [300/301], Loss: 0.7702, Dev: 0.3360\n",
            "Epoch: [3/1000], Step: [100/301], Loss: 1.0251, Dev: 0.3040\n",
            "Epoch: [3/1000], Step: [200/301], Loss: 1.2541, Dev: 0.2960\n",
            "Epoch: [3/1000], Step: [300/301], Loss: 1.2694, Dev: 0.3340\n",
            "Epoch: [4/1000], Step: [100/301], Loss: 0.7871, Dev: 0.2990\n",
            "Epoch: [4/1000], Step: [200/301], Loss: 1.8611, Dev: 0.3390\n",
            "Epoch: [4/1000], Step: [300/301], Loss: 1.0974, Dev: 0.3110\n",
            "Epoch: [5/1000], Step: [100/301], Loss: 1.4427, Dev: 0.3010\n",
            "Epoch: [5/1000], Step: [200/301], Loss: 1.1788, Dev: 0.3450\n",
            "Epoch: [5/1000], Step: [300/301], Loss: 0.7992, Dev: 0.3150\n",
            "Epoch: [6/1000], Step: [100/301], Loss: 1.1010, Dev: 0.3570\n",
            "Epoch: [6/1000], Step: [200/301], Loss: 0.8783, Dev: 0.3300\n",
            "Epoch: [6/1000], Step: [300/301], Loss: 0.8302, Dev: 0.3170\n",
            "Epoch: [7/1000], Step: [100/301], Loss: 1.2687, Dev: 0.2970\n",
            "Epoch: [7/1000], Step: [200/301], Loss: 0.9447, Dev: 0.3530\n",
            "Epoch: [7/1000], Step: [300/301], Loss: 1.3307, Dev: 0.2970\n",
            "Epoch: [8/1000], Step: [100/301], Loss: 0.4677, Dev: 0.3610\n",
            "Epoch: [8/1000], Step: [200/301], Loss: 0.5728, Dev: 0.3270\n",
            "Epoch: [8/1000], Step: [300/301], Loss: 0.8102, Dev: 0.3730\n",
            "Epoch: [9/1000], Step: [100/301], Loss: 1.1853, Dev: 0.3570\n",
            "Epoch: [9/1000], Step: [200/301], Loss: 1.3921, Dev: 0.3350\n",
            "Epoch: [9/1000], Step: [300/301], Loss: 0.6745, Dev: 0.3540\n",
            "Epoch: [10/1000], Step: [100/301], Loss: 0.7398, Dev: 0.3430\n",
            "Epoch: [10/1000], Step: [200/301], Loss: 1.0373, Dev: 0.3240\n",
            "Epoch: [10/1000], Step: [300/301], Loss: 1.0605, Dev: 0.3290\n",
            "Epoch: [11/1000], Step: [100/301], Loss: 1.0343, Dev: 0.3300\n",
            "Epoch: [11/1000], Step: [200/301], Loss: 0.8902, Dev: 0.3430\n",
            "Epoch: [11/1000], Step: [300/301], Loss: 0.8903, Dev: 0.3430\n",
            "Epoch: [12/1000], Step: [100/301], Loss: 0.9051, Dev: 0.3240\n",
            "Epoch: [12/1000], Step: [200/301], Loss: 0.6149, Dev: 0.3290\n",
            "Epoch: [12/1000], Step: [300/301], Loss: 0.7122, Dev: 0.3670\n",
            "Epoch: [13/1000], Step: [100/301], Loss: 0.7696, Dev: 0.3530\n",
            "Epoch: [13/1000], Step: [200/301], Loss: 0.8627, Dev: 0.3450\n",
            "Epoch: [13/1000], Step: [300/301], Loss: 1.2872, Dev: 0.3380\n",
            "Epoch: [14/1000], Step: [100/301], Loss: 0.6832, Dev: 0.3570\n",
            "Epoch: [14/1000], Step: [200/301], Loss: 0.5417, Dev: 0.3740\n",
            "Epoch: [14/1000], Step: [300/301], Loss: 0.8157, Dev: 0.3640\n",
            "Epoch: [15/1000], Step: [100/301], Loss: 0.6460, Dev: 0.3840\n",
            "Epoch: [15/1000], Step: [200/301], Loss: 0.9277, Dev: 0.3260\n",
            "Epoch: [15/1000], Step: [300/301], Loss: 0.5159, Dev: 0.3060\n",
            "Epoch: [16/1000], Step: [100/301], Loss: 0.3982, Dev: 0.3350\n",
            "Epoch: [16/1000], Step: [200/301], Loss: 0.6532, Dev: 0.3390\n",
            "Epoch: [16/1000], Step: [300/301], Loss: 0.8059, Dev: 0.3740\n",
            "Epoch: [17/1000], Step: [100/301], Loss: 0.7842, Dev: 0.3680\n",
            "Epoch: [17/1000], Step: [200/301], Loss: 0.4717, Dev: 0.3760\n",
            "Epoch: [17/1000], Step: [300/301], Loss: 0.4765, Dev: 0.3710\n",
            "Epoch: [18/1000], Step: [100/301], Loss: 0.7228, Dev: 0.3560\n",
            "Epoch: [18/1000], Step: [200/301], Loss: 0.3942, Dev: 0.3550\n",
            "Epoch: [18/1000], Step: [300/301], Loss: 0.6855, Dev: 0.3740\n",
            "Epoch: [19/1000], Step: [100/301], Loss: 0.6566, Dev: 0.3720\n",
            "Epoch: [19/1000], Step: [200/301], Loss: 0.7104, Dev: 0.3840\n",
            "Epoch: [19/1000], Step: [300/301], Loss: 0.6804, Dev: 0.3580\n",
            "Epoch: [20/1000], Step: [100/301], Loss: 0.3502, Dev: 0.3760\n",
            "Epoch: [20/1000], Step: [200/301], Loss: 0.5156, Dev: 0.3940\n",
            "Epoch: [20/1000], Step: [300/301], Loss: 0.7332, Dev: 0.4090\n",
            "Epoch: [21/1000], Step: [100/301], Loss: 0.5396, Dev: 0.3650\n",
            "Epoch: [21/1000], Step: [200/301], Loss: 0.6126, Dev: 0.3800\n",
            "Epoch: [21/1000], Step: [300/301], Loss: 0.6784, Dev: 0.3850\n",
            "Epoch: [22/1000], Step: [100/301], Loss: 0.5718, Dev: 0.3880\n",
            "Epoch: [22/1000], Step: [200/301], Loss: 0.4841, Dev: 0.3950\n",
            "Epoch: [22/1000], Step: [300/301], Loss: 0.6385, Dev: 0.3810\n",
            "Epoch: [23/1000], Step: [100/301], Loss: 0.8601, Dev: 0.4080\n",
            "Epoch: [23/1000], Step: [200/301], Loss: 0.5833, Dev: 0.4020\n",
            "Epoch: [23/1000], Step: [300/301], Loss: 0.5884, Dev: 0.3940\n",
            "Epoch: [24/1000], Step: [100/301], Loss: 0.4889, Dev: 0.3790\n",
            "Epoch: [24/1000], Step: [200/301], Loss: 0.5394, Dev: 0.4240\n",
            "Epoch: [24/1000], Step: [300/301], Loss: 0.5573, Dev: 0.3870\n",
            "Epoch: [25/1000], Step: [100/301], Loss: 0.5415, Dev: 0.4130\n",
            "Epoch: [25/1000], Step: [200/301], Loss: 0.6838, Dev: 0.3810\n",
            "Epoch: [25/1000], Step: [300/301], Loss: 0.6962, Dev: 0.3930\n",
            "Epoch: [26/1000], Step: [100/301], Loss: 0.4846, Dev: 0.3710\n",
            "Epoch: [26/1000], Step: [200/301], Loss: 0.4840, Dev: 0.3970\n",
            "Epoch: [26/1000], Step: [300/301], Loss: 0.6838, Dev: 0.3970\n",
            "Epoch: [27/1000], Step: [100/301], Loss: 0.6310, Dev: 0.4090\n",
            "Epoch: [27/1000], Step: [200/301], Loss: 0.4395, Dev: 0.3860\n",
            "Epoch: [27/1000], Step: [300/301], Loss: 0.6076, Dev: 0.3850\n",
            "Epoch: [28/1000], Step: [100/301], Loss: 0.5380, Dev: 0.3610\n",
            "Epoch: [28/1000], Step: [200/301], Loss: 0.5335, Dev: 0.3950\n",
            "Epoch: [28/1000], Step: [300/301], Loss: 0.5451, Dev: 0.4460\n",
            "Epoch: [29/1000], Step: [100/301], Loss: 0.5609, Dev: 0.4220\n",
            "Epoch: [29/1000], Step: [200/301], Loss: 0.5449, Dev: 0.4220\n",
            "Epoch: [29/1000], Step: [300/301], Loss: 0.5632, Dev: 0.4120\n",
            "Epoch: [30/1000], Step: [100/301], Loss: 0.4374, Dev: 0.4220\n",
            "Epoch: [30/1000], Step: [200/301], Loss: 0.6050, Dev: 0.4430\n",
            "Epoch: [30/1000], Step: [300/301], Loss: 0.3917, Dev: 0.3660\n",
            "Epoch: [31/1000], Step: [100/301], Loss: 0.5418, Dev: 0.4090\n",
            "Epoch: [31/1000], Step: [200/301], Loss: 0.5037, Dev: 0.3930\n",
            "Epoch: [31/1000], Step: [300/301], Loss: 0.6110, Dev: 0.3980\n",
            "Epoch: [32/1000], Step: [100/301], Loss: 0.4808, Dev: 0.4270\n",
            "Epoch: [32/1000], Step: [200/301], Loss: 0.5094, Dev: 0.3810\n",
            "Epoch: [32/1000], Step: [300/301], Loss: 0.5123, Dev: 0.3990\n",
            "Epoch: [33/1000], Step: [100/301], Loss: 0.4295, Dev: 0.4280\n",
            "Epoch: [33/1000], Step: [200/301], Loss: 0.5132, Dev: 0.4240\n",
            "Epoch: [33/1000], Step: [300/301], Loss: 0.4772, Dev: 0.4130\n",
            "Epoch: [34/1000], Step: [100/301], Loss: 0.4674, Dev: 0.4260\n",
            "Epoch: [34/1000], Step: [200/301], Loss: 0.3810, Dev: 0.4170\n",
            "Epoch: [34/1000], Step: [300/301], Loss: 0.5074, Dev: 0.3850\n",
            "Epoch: [35/1000], Step: [100/301], Loss: 0.3837, Dev: 0.4430\n",
            "Epoch: [35/1000], Step: [200/301], Loss: 0.6129, Dev: 0.4170\n",
            "Epoch: [35/1000], Step: [300/301], Loss: 0.3583, Dev: 0.3930\n",
            "Epoch: [36/1000], Step: [100/301], Loss: 0.3778, Dev: 0.4250\n",
            "Epoch: [36/1000], Step: [200/301], Loss: 0.4647, Dev: 0.4780\n",
            "Epoch: [36/1000], Step: [300/301], Loss: 0.3990, Dev: 0.4080\n",
            "Epoch: [37/1000], Step: [100/301], Loss: 0.5130, Dev: 0.4480\n",
            "Epoch: [37/1000], Step: [200/301], Loss: 0.5644, Dev: 0.4120\n",
            "Epoch: [37/1000], Step: [300/301], Loss: 0.4321, Dev: 0.4370\n",
            "Epoch: [38/1000], Step: [100/301], Loss: 0.5731, Dev: 0.4250\n",
            "Epoch: [38/1000], Step: [200/301], Loss: 0.6206, Dev: 0.4150\n",
            "Epoch: [38/1000], Step: [300/301], Loss: 0.4924, Dev: 0.3970\n",
            "Epoch: [39/1000], Step: [100/301], Loss: 0.5484, Dev: 0.4350\n",
            "Epoch: [39/1000], Step: [200/301], Loss: 0.5407, Dev: 0.5120\n",
            "Epoch: [39/1000], Step: [300/301], Loss: 0.4527, Dev: 0.4350\n",
            "Epoch: [40/1000], Step: [100/301], Loss: 0.4377, Dev: 0.4360\n",
            "Epoch: [40/1000], Step: [200/301], Loss: 0.4739, Dev: 0.4960\n",
            "Epoch: [40/1000], Step: [300/301], Loss: 0.4536, Dev: 0.4360\n",
            "Epoch: [41/1000], Step: [100/301], Loss: 0.4879, Dev: 0.4590\n",
            "Epoch: [41/1000], Step: [200/301], Loss: 0.5132, Dev: 0.4030\n",
            "Epoch: [41/1000], Step: [300/301], Loss: 0.4943, Dev: 0.4540\n",
            "Epoch: [42/1000], Step: [100/301], Loss: 0.4404, Dev: 0.4280\n",
            "Epoch: [42/1000], Step: [200/301], Loss: 0.3976, Dev: 0.4380\n",
            "Epoch: [42/1000], Step: [300/301], Loss: 0.5775, Dev: 0.4630\n",
            "Epoch: [43/1000], Step: [100/301], Loss: 0.3877, Dev: 0.4180\n",
            "Epoch: [43/1000], Step: [200/301], Loss: 0.4892, Dev: 0.4570\n",
            "Epoch: [43/1000], Step: [300/301], Loss: 0.3604, Dev: 0.4280\n",
            "Epoch: [44/1000], Step: [100/301], Loss: 0.4225, Dev: 0.4760\n",
            "Epoch: [44/1000], Step: [200/301], Loss: 0.4679, Dev: 0.4650\n",
            "Epoch: [44/1000], Step: [300/301], Loss: 0.4144, Dev: 0.4100\n",
            "Epoch: [45/1000], Step: [100/301], Loss: 0.4505, Dev: 0.4920\n",
            "Epoch: [45/1000], Step: [200/301], Loss: 0.4296, Dev: 0.4740\n",
            "Epoch: [45/1000], Step: [300/301], Loss: 0.5098, Dev: 0.4440\n",
            "Epoch: [46/1000], Step: [100/301], Loss: 0.3939, Dev: 0.4630\n",
            "Epoch: [46/1000], Step: [200/301], Loss: 0.4524, Dev: 0.4520\n",
            "Epoch: [46/1000], Step: [300/301], Loss: 0.5388, Dev: 0.4480\n",
            "Epoch: [47/1000], Step: [100/301], Loss: 0.3658, Dev: 0.4920\n",
            "Epoch: [47/1000], Step: [200/301], Loss: 0.4570, Dev: 0.4340\n",
            "Epoch: [47/1000], Step: [300/301], Loss: 0.3759, Dev: 0.4830\n",
            "Epoch: [48/1000], Step: [100/301], Loss: 0.5011, Dev: 0.4440\n",
            "Epoch: [48/1000], Step: [200/301], Loss: 0.5884, Dev: 0.4620\n",
            "Epoch: [48/1000], Step: [300/301], Loss: 0.3302, Dev: 0.4820\n",
            "Epoch: [49/1000], Step: [100/301], Loss: 0.5212, Dev: 0.4900\n",
            "Epoch: [49/1000], Step: [200/301], Loss: 0.3224, Dev: 0.5160\n",
            "Epoch: [49/1000], Step: [300/301], Loss: 0.3513, Dev: 0.4430\n",
            "Epoch: [50/1000], Step: [100/301], Loss: 0.5330, Dev: 0.4600\n",
            "Epoch: [50/1000], Step: [200/301], Loss: 0.3773, Dev: 0.4340\n",
            "Epoch: [50/1000], Step: [300/301], Loss: 0.4168, Dev: 0.4680\n",
            "Epoch: [51/1000], Step: [100/301], Loss: 0.5211, Dev: 0.4980\n",
            "Epoch: [51/1000], Step: [200/301], Loss: 0.4069, Dev: 0.5520\n",
            "Epoch: [51/1000], Step: [300/301], Loss: 0.4284, Dev: 0.4640\n",
            "Epoch: [52/1000], Step: [100/301], Loss: 0.4634, Dev: 0.4940\n",
            "Epoch: [52/1000], Step: [200/301], Loss: 0.3587, Dev: 0.4510\n",
            "Epoch: [52/1000], Step: [300/301], Loss: 0.4171, Dev: 0.4860\n",
            "Epoch: [53/1000], Step: [100/301], Loss: 0.4866, Dev: 0.4730\n",
            "Epoch: [53/1000], Step: [200/301], Loss: 0.4590, Dev: 0.4840\n",
            "Epoch: [53/1000], Step: [300/301], Loss: 0.4349, Dev: 0.4760\n",
            "Epoch: [54/1000], Step: [100/301], Loss: 0.4614, Dev: 0.5170\n",
            "Epoch: [54/1000], Step: [200/301], Loss: 0.5302, Dev: 0.4970\n",
            "Epoch: [54/1000], Step: [300/301], Loss: 0.4799, Dev: 0.5190\n",
            "Epoch: [55/1000], Step: [100/301], Loss: 0.5196, Dev: 0.5360\n",
            "Epoch: [55/1000], Step: [200/301], Loss: 0.4773, Dev: 0.4970\n",
            "Epoch: [55/1000], Step: [300/301], Loss: 0.3407, Dev: 0.4820\n",
            "Epoch: [56/1000], Step: [100/301], Loss: 0.3297, Dev: 0.4950\n",
            "Epoch: [56/1000], Step: [200/301], Loss: 0.4882, Dev: 0.4800\n",
            "Epoch: [56/1000], Step: [300/301], Loss: 0.4058, Dev: 0.5190\n",
            "Epoch: [57/1000], Step: [100/301], Loss: 0.5092, Dev: 0.4860\n",
            "Epoch: [57/1000], Step: [200/301], Loss: 0.4312, Dev: 0.4510\n",
            "Epoch: [57/1000], Step: [300/301], Loss: 0.5151, Dev: 0.4910\n",
            "Epoch: [58/1000], Step: [100/301], Loss: 0.3541, Dev: 0.5270\n",
            "Epoch: [58/1000], Step: [200/301], Loss: 0.4517, Dev: 0.4950\n",
            "Epoch: [58/1000], Step: [300/301], Loss: 0.4553, Dev: 0.4960\n",
            "Epoch: [59/1000], Step: [100/301], Loss: 0.5102, Dev: 0.5160\n",
            "Epoch: [59/1000], Step: [200/301], Loss: 0.4361, Dev: 0.5030\n",
            "Epoch: [59/1000], Step: [300/301], Loss: 0.4634, Dev: 0.5120\n",
            "Epoch: [60/1000], Step: [100/301], Loss: 0.4723, Dev: 0.5510\n",
            "Epoch: [60/1000], Step: [200/301], Loss: 0.4076, Dev: 0.5610\n",
            "Epoch: [60/1000], Step: [300/301], Loss: 0.3041, Dev: 0.5340\n",
            "Epoch: [61/1000], Step: [100/301], Loss: 0.3004, Dev: 0.5180\n",
            "Epoch: [61/1000], Step: [200/301], Loss: 0.4108, Dev: 0.4550\n",
            "Epoch: [61/1000], Step: [300/301], Loss: 0.3594, Dev: 0.5740\n",
            "Epoch: [62/1000], Step: [100/301], Loss: 0.4970, Dev: 0.5300\n",
            "Epoch: [62/1000], Step: [200/301], Loss: 0.4075, Dev: 0.5660\n",
            "Epoch: [62/1000], Step: [300/301], Loss: 0.4134, Dev: 0.5000\n",
            "Epoch: [63/1000], Step: [100/301], Loss: 0.4259, Dev: 0.4730\n",
            "Epoch: [63/1000], Step: [200/301], Loss: 0.5313, Dev: 0.5660\n",
            "Epoch: [63/1000], Step: [300/301], Loss: 0.4389, Dev: 0.5760\n",
            "Epoch: [64/1000], Step: [100/301], Loss: 0.4349, Dev: 0.5090\n",
            "Epoch: [64/1000], Step: [200/301], Loss: 0.5274, Dev: 0.5530\n",
            "Epoch: [64/1000], Step: [300/301], Loss: 0.4275, Dev: 0.5080\n",
            "Epoch: [65/1000], Step: [100/301], Loss: 0.4751, Dev: 0.5410\n",
            "Epoch: [65/1000], Step: [200/301], Loss: 0.5601, Dev: 0.5640\n",
            "Epoch: [65/1000], Step: [300/301], Loss: 0.4191, Dev: 0.5530\n",
            "Epoch: [66/1000], Step: [100/301], Loss: 0.3667, Dev: 0.5520\n",
            "Epoch: [66/1000], Step: [200/301], Loss: 0.4486, Dev: 0.5500\n",
            "Epoch: [66/1000], Step: [300/301], Loss: 0.4267, Dev: 0.5300\n",
            "Epoch: [67/1000], Step: [100/301], Loss: 0.4011, Dev: 0.5370\n",
            "Epoch: [67/1000], Step: [200/301], Loss: 0.3158, Dev: 0.5100\n",
            "Epoch: [67/1000], Step: [300/301], Loss: 0.4060, Dev: 0.5730\n",
            "Epoch: [68/1000], Step: [100/301], Loss: 0.4200, Dev: 0.5620\n",
            "Epoch: [68/1000], Step: [200/301], Loss: 0.4256, Dev: 0.5750\n",
            "Epoch: [68/1000], Step: [300/301], Loss: 0.3884, Dev: 0.5670\n",
            "Epoch: [69/1000], Step: [100/301], Loss: 0.4432, Dev: 0.5770\n",
            "Epoch: [69/1000], Step: [200/301], Loss: 0.4867, Dev: 0.5500\n",
            "Epoch: [69/1000], Step: [300/301], Loss: 0.5698, Dev: 0.5460\n",
            "Epoch: [70/1000], Step: [100/301], Loss: 0.4600, Dev: 0.5640\n",
            "Epoch: [70/1000], Step: [200/301], Loss: 0.3795, Dev: 0.5450\n",
            "Epoch: [70/1000], Step: [300/301], Loss: 0.4280, Dev: 0.5740\n",
            "Epoch: [71/1000], Step: [100/301], Loss: 0.4910, Dev: 0.5310\n",
            "Epoch: [71/1000], Step: [200/301], Loss: 0.4665, Dev: 0.5700\n",
            "Epoch: [71/1000], Step: [300/301], Loss: 0.4889, Dev: 0.5830\n",
            "Epoch: [72/1000], Step: [100/301], Loss: 0.3053, Dev: 0.5720\n",
            "Epoch: [72/1000], Step: [200/301], Loss: 0.3826, Dev: 0.5590\n",
            "Epoch: [72/1000], Step: [300/301], Loss: 0.4924, Dev: 0.5690\n",
            "Epoch: [73/1000], Step: [100/301], Loss: 0.4947, Dev: 0.5130\n",
            "Epoch: [73/1000], Step: [200/301], Loss: 0.5193, Dev: 0.5680\n",
            "Epoch: [73/1000], Step: [300/301], Loss: 0.4896, Dev: 0.5690\n",
            "Epoch: [74/1000], Step: [100/301], Loss: 0.4050, Dev: 0.5820\n",
            "Epoch: [74/1000], Step: [200/301], Loss: 0.4206, Dev: 0.5980\n",
            "Epoch: [74/1000], Step: [300/301], Loss: 0.4919, Dev: 0.5350\n",
            "Epoch: [75/1000], Step: [100/301], Loss: 0.5051, Dev: 0.5320\n",
            "Epoch: [75/1000], Step: [200/301], Loss: 0.4250, Dev: 0.5950\n",
            "Epoch: [75/1000], Step: [300/301], Loss: 0.3882, Dev: 0.5360\n",
            "Epoch: [76/1000], Step: [100/301], Loss: 0.5035, Dev: 0.5120\n",
            "Epoch: [76/1000], Step: [200/301], Loss: 0.4424, Dev: 0.5740\n",
            "Epoch: [76/1000], Step: [300/301], Loss: 0.3962, Dev: 0.5070\n",
            "Epoch: [77/1000], Step: [100/301], Loss: 0.3354, Dev: 0.5410\n",
            "Epoch: [77/1000], Step: [200/301], Loss: 0.3175, Dev: 0.5500\n",
            "Epoch: [77/1000], Step: [300/301], Loss: 0.4665, Dev: 0.5440\n",
            "Epoch: [78/1000], Step: [100/301], Loss: 0.3951, Dev: 0.4970\n",
            "Epoch: [78/1000], Step: [200/301], Loss: 0.3579, Dev: 0.5530\n",
            "Epoch: [78/1000], Step: [300/301], Loss: 0.4575, Dev: 0.5690\n",
            "Epoch: [79/1000], Step: [100/301], Loss: 0.4395, Dev: 0.5960\n",
            "Epoch: [79/1000], Step: [200/301], Loss: 0.3258, Dev: 0.6020\n",
            "Epoch: [79/1000], Step: [300/301], Loss: 0.3853, Dev: 0.5480\n",
            "Epoch: [80/1000], Step: [100/301], Loss: 0.4319, Dev: 0.5280\n",
            "Epoch: [80/1000], Step: [200/301], Loss: 0.3080, Dev: 0.5630\n",
            "Epoch: [80/1000], Step: [300/301], Loss: 0.4688, Dev: 0.5930\n",
            "Epoch: [81/1000], Step: [100/301], Loss: 0.4785, Dev: 0.5800\n",
            "Epoch: [81/1000], Step: [200/301], Loss: 0.3509, Dev: 0.5680\n",
            "Epoch: [81/1000], Step: [300/301], Loss: 0.4351, Dev: 0.5440\n",
            "Epoch: [82/1000], Step: [100/301], Loss: 0.3815, Dev: 0.5810\n",
            "Epoch: [82/1000], Step: [200/301], Loss: 0.4408, Dev: 0.5690\n",
            "Epoch: [82/1000], Step: [300/301], Loss: 0.4323, Dev: 0.5920\n",
            "Epoch: [83/1000], Step: [100/301], Loss: 0.4140, Dev: 0.5550\n",
            "Epoch: [83/1000], Step: [200/301], Loss: 0.4632, Dev: 0.5610\n",
            "Epoch: [83/1000], Step: [300/301], Loss: 0.3968, Dev: 0.6010\n",
            "Epoch: [84/1000], Step: [100/301], Loss: 0.5188, Dev: 0.6350\n",
            "Epoch: [84/1000], Step: [200/301], Loss: 0.4669, Dev: 0.5990\n",
            "Epoch: [84/1000], Step: [300/301], Loss: 0.3555, Dev: 0.5730\n",
            "Epoch: [85/1000], Step: [100/301], Loss: 0.4260, Dev: 0.5980\n",
            "Epoch: [85/1000], Step: [200/301], Loss: 0.5176, Dev: 0.6280\n",
            "Epoch: [85/1000], Step: [300/301], Loss: 0.4338, Dev: 0.6120\n",
            "Epoch: [86/1000], Step: [100/301], Loss: 0.4312, Dev: 0.6040\n",
            "Epoch: [86/1000], Step: [200/301], Loss: 0.4491, Dev: 0.5640\n",
            "Epoch: [86/1000], Step: [300/301], Loss: 0.5183, Dev: 0.6050\n",
            "Epoch: [87/1000], Step: [100/301], Loss: 0.3885, Dev: 0.6380\n",
            "Epoch: [87/1000], Step: [200/301], Loss: 0.3867, Dev: 0.5950\n",
            "Epoch: [87/1000], Step: [300/301], Loss: 0.4080, Dev: 0.5780\n",
            "Epoch: [88/1000], Step: [100/301], Loss: 0.4489, Dev: 0.5920\n",
            "Epoch: [88/1000], Step: [200/301], Loss: 0.4031, Dev: 0.6530\n",
            "Epoch: [88/1000], Step: [300/301], Loss: 0.4299, Dev: 0.5690\n",
            "Epoch: [89/1000], Step: [100/301], Loss: 0.4299, Dev: 0.5610\n",
            "Epoch: [89/1000], Step: [200/301], Loss: 0.4502, Dev: 0.6070\n",
            "Epoch: [89/1000], Step: [300/301], Loss: 0.3451, Dev: 0.6560\n",
            "Epoch: [90/1000], Step: [100/301], Loss: 0.3301, Dev: 0.6190\n",
            "Epoch: [90/1000], Step: [200/301], Loss: 0.3814, Dev: 0.6030\n",
            "Epoch: [90/1000], Step: [300/301], Loss: 0.5427, Dev: 0.6180\n",
            "Epoch: [91/1000], Step: [100/301], Loss: 0.5456, Dev: 0.6130\n",
            "Epoch: [91/1000], Step: [200/301], Loss: 0.4695, Dev: 0.6090\n",
            "Epoch: [91/1000], Step: [300/301], Loss: 0.3775, Dev: 0.6180\n",
            "Epoch: [92/1000], Step: [100/301], Loss: 0.4943, Dev: 0.5560\n",
            "Epoch: [92/1000], Step: [200/301], Loss: 0.4331, Dev: 0.5940\n",
            "Epoch: [92/1000], Step: [300/301], Loss: 0.2492, Dev: 0.5520\n",
            "Early Stop\n",
            "Epoch: [1/1000], Step: [100/301], Loss: 1.8179, Dev: 0.5000\n",
            "Epoch: [1/1000], Step: [200/301], Loss: 1.3238, Dev: 0.5650\n",
            "Epoch: [1/1000], Step: [300/301], Loss: 2.1732, Dev: 0.5690\n",
            "Epoch: [2/1000], Step: [100/301], Loss: 0.9063, Dev: 0.5860\n",
            "Epoch: [2/1000], Step: [200/301], Loss: 2.1614, Dev: 0.5920\n",
            "Epoch: [2/1000], Step: [300/301], Loss: 1.6603, Dev: 0.5720\n",
            "Epoch: [3/1000], Step: [100/301], Loss: 2.2294, Dev: 0.5520\n",
            "Epoch: [3/1000], Step: [200/301], Loss: 1.4348, Dev: 0.5970\n",
            "Epoch: [3/1000], Step: [300/301], Loss: 0.9926, Dev: 0.6060\n",
            "Epoch: [4/1000], Step: [100/301], Loss: 1.4889, Dev: 0.5900\n",
            "Epoch: [4/1000], Step: [200/301], Loss: 1.1567, Dev: 0.5440\n",
            "Epoch: [4/1000], Step: [300/301], Loss: 1.2385, Dev: 0.5830\n",
            "Epoch: [5/1000], Step: [100/301], Loss: 0.8633, Dev: 0.6120\n",
            "Epoch: [5/1000], Step: [200/301], Loss: 0.9215, Dev: 0.6060\n",
            "Epoch: [5/1000], Step: [300/301], Loss: 0.7418, Dev: 0.6050\n",
            "Epoch: [6/1000], Step: [100/301], Loss: 0.8752, Dev: 0.5960\n",
            "Epoch: [6/1000], Step: [200/301], Loss: 0.9830, Dev: 0.5930\n",
            "Epoch: [6/1000], Step: [300/301], Loss: 0.9599, Dev: 0.6060\n",
            "Epoch: [7/1000], Step: [100/301], Loss: 1.0404, Dev: 0.5860\n",
            "Epoch: [7/1000], Step: [200/301], Loss: 1.2361, Dev: 0.5960\n",
            "Epoch: [7/1000], Step: [300/301], Loss: 1.3837, Dev: 0.6120\n",
            "Epoch: [8/1000], Step: [100/301], Loss: 1.1326, Dev: 0.5960\n",
            "Epoch: [8/1000], Step: [200/301], Loss: 1.6601, Dev: 0.6360\n",
            "Epoch: [8/1000], Step: [300/301], Loss: 1.2765, Dev: 0.5940\n",
            "Epoch: [9/1000], Step: [100/301], Loss: 0.8029, Dev: 0.6160\n",
            "Epoch: [9/1000], Step: [200/301], Loss: 1.1038, Dev: 0.6240\n",
            "Epoch: [9/1000], Step: [300/301], Loss: 1.2423, Dev: 0.6350\n",
            "Epoch: [10/1000], Step: [100/301], Loss: 0.8129, Dev: 0.6140\n",
            "Epoch: [10/1000], Step: [200/301], Loss: 0.7464, Dev: 0.6250\n",
            "Epoch: [10/1000], Step: [300/301], Loss: 1.0522, Dev: 0.6300\n",
            "Epoch: [11/1000], Step: [100/301], Loss: 1.0001, Dev: 0.6110\n",
            "Epoch: [11/1000], Step: [200/301], Loss: 0.8030, Dev: 0.6060\n",
            "Epoch: [11/1000], Step: [300/301], Loss: 1.2679, Dev: 0.6110\n",
            "Epoch: [12/1000], Step: [100/301], Loss: 0.9576, Dev: 0.6900\n",
            "Epoch: [12/1000], Step: [200/301], Loss: 1.1470, Dev: 0.6480\n",
            "Epoch: [12/1000], Step: [300/301], Loss: 1.3087, Dev: 0.6120\n",
            "Epoch: [13/1000], Step: [100/301], Loss: 0.9315, Dev: 0.6590\n",
            "Epoch: [13/1000], Step: [200/301], Loss: 0.9445, Dev: 0.6150\n",
            "Epoch: [13/1000], Step: [300/301], Loss: 0.5258, Dev: 0.6350\n",
            "Epoch: [14/1000], Step: [100/301], Loss: 0.6579, Dev: 0.6570\n",
            "Epoch: [14/1000], Step: [200/301], Loss: 0.8621, Dev: 0.6970\n",
            "Epoch: [14/1000], Step: [300/301], Loss: 0.9054, Dev: 0.6320\n",
            "Epoch: [15/1000], Step: [100/301], Loss: 0.7087, Dev: 0.6410\n",
            "Epoch: [15/1000], Step: [200/301], Loss: 0.9171, Dev: 0.6360\n",
            "Epoch: [15/1000], Step: [300/301], Loss: 0.6982, Dev: 0.6430\n",
            "Epoch: [16/1000], Step: [100/301], Loss: 0.6242, Dev: 0.6110\n",
            "Epoch: [16/1000], Step: [200/301], Loss: 1.3083, Dev: 0.6510\n",
            "Epoch: [16/1000], Step: [300/301], Loss: 0.9958, Dev: 0.6390\n",
            "Epoch: [17/1000], Step: [100/301], Loss: 1.1006, Dev: 0.6270\n",
            "Epoch: [17/1000], Step: [200/301], Loss: 1.4948, Dev: 0.6560\n",
            "Epoch: [17/1000], Step: [300/301], Loss: 0.7079, Dev: 0.6570\n",
            "Epoch: [18/1000], Step: [100/301], Loss: 0.7996, Dev: 0.6690\n",
            "Epoch: [18/1000], Step: [200/301], Loss: 0.6219, Dev: 0.6240\n",
            "Epoch: [18/1000], Step: [300/301], Loss: 0.8059, Dev: 0.6440\n",
            "Epoch: [19/1000], Step: [100/301], Loss: 0.7972, Dev: 0.6790\n",
            "Epoch: [19/1000], Step: [200/301], Loss: 0.6129, Dev: 0.6660\n",
            "Epoch: [19/1000], Step: [300/301], Loss: 1.2532, Dev: 0.6270\n",
            "Epoch: [20/1000], Step: [100/301], Loss: 0.8343, Dev: 0.6460\n",
            "Epoch: [20/1000], Step: [200/301], Loss: 0.8405, Dev: 0.6360\n",
            "Epoch: [20/1000], Step: [300/301], Loss: 0.7942, Dev: 0.6830\n",
            "Epoch: [21/1000], Step: [100/301], Loss: 0.9491, Dev: 0.6770\n",
            "Epoch: [21/1000], Step: [200/301], Loss: 1.2578, Dev: 0.6840\n",
            "Epoch: [21/1000], Step: [300/301], Loss: 0.8846, Dev: 0.6730\n",
            "Epoch: [22/1000], Step: [100/301], Loss: 0.9050, Dev: 0.6900\n",
            "Epoch: [22/1000], Step: [200/301], Loss: 0.9834, Dev: 0.6630\n",
            "Epoch: [22/1000], Step: [300/301], Loss: 0.9667, Dev: 0.6550\n",
            "Epoch: [23/1000], Step: [100/301], Loss: 0.6010, Dev: 0.6490\n",
            "Epoch: [23/1000], Step: [200/301], Loss: 0.6008, Dev: 0.6890\n",
            "Epoch: [23/1000], Step: [300/301], Loss: 0.5825, Dev: 0.6610\n",
            "Epoch: [24/1000], Step: [100/301], Loss: 0.7120, Dev: 0.6520\n",
            "Epoch: [24/1000], Step: [200/301], Loss: 1.3845, Dev: 0.7280\n",
            "Epoch: [24/1000], Step: [300/301], Loss: 0.7320, Dev: 0.6690\n",
            "Epoch: [25/1000], Step: [100/301], Loss: 0.6625, Dev: 0.6600\n",
            "Epoch: [25/1000], Step: [200/301], Loss: 0.5676, Dev: 0.6560\n",
            "Epoch: [25/1000], Step: [300/301], Loss: 0.5193, Dev: 0.6380\n",
            "Epoch: [26/1000], Step: [100/301], Loss: 0.7009, Dev: 0.6540\n",
            "Epoch: [26/1000], Step: [200/301], Loss: 0.5568, Dev: 0.6880\n",
            "Epoch: [26/1000], Step: [300/301], Loss: 0.5893, Dev: 0.6780\n",
            "Epoch: [27/1000], Step: [100/301], Loss: 0.6968, Dev: 0.6400\n",
            "Epoch: [27/1000], Step: [200/301], Loss: 0.5781, Dev: 0.6830\n",
            "Epoch: [27/1000], Step: [300/301], Loss: 0.4240, Dev: 0.6690\n",
            "Epoch: [28/1000], Step: [100/301], Loss: 0.7176, Dev: 0.6740\n",
            "Epoch: [28/1000], Step: [200/301], Loss: 0.8099, Dev: 0.6870\n",
            "Epoch: [28/1000], Step: [300/301], Loss: 0.5940, Dev: 0.6990\n",
            "Epoch: [29/1000], Step: [100/301], Loss: 0.6781, Dev: 0.6980\n",
            "Epoch: [29/1000], Step: [200/301], Loss: 0.6960, Dev: 0.7000\n",
            "Epoch: [29/1000], Step: [300/301], Loss: 0.8175, Dev: 0.6690\n",
            "Epoch: [30/1000], Step: [100/301], Loss: 0.8165, Dev: 0.7000\n",
            "Epoch: [30/1000], Step: [200/301], Loss: 0.5783, Dev: 0.6990\n",
            "Epoch: [30/1000], Step: [300/301], Loss: 0.5142, Dev: 0.6580\n",
            "Epoch: [31/1000], Step: [100/301], Loss: 0.4405, Dev: 0.6830\n",
            "Epoch: [31/1000], Step: [200/301], Loss: 0.4904, Dev: 0.7010\n",
            "Epoch: [31/1000], Step: [300/301], Loss: 0.5485, Dev: 0.7250\n",
            "Epoch: [32/1000], Step: [100/301], Loss: 0.7070, Dev: 0.6600\n",
            "Epoch: [32/1000], Step: [200/301], Loss: 0.8016, Dev: 0.6740\n",
            "Epoch: [32/1000], Step: [300/301], Loss: 0.6286, Dev: 0.6740\n",
            "Epoch: [33/1000], Step: [100/301], Loss: 0.7868, Dev: 0.6790\n",
            "Epoch: [33/1000], Step: [200/301], Loss: 0.7504, Dev: 0.6840\n",
            "Epoch: [33/1000], Step: [300/301], Loss: 0.4912, Dev: 0.6770\n",
            "Epoch: [34/1000], Step: [100/301], Loss: 0.7605, Dev: 0.7020\n",
            "Epoch: [34/1000], Step: [200/301], Loss: 0.5469, Dev: 0.6930\n",
            "Epoch: [34/1000], Step: [300/301], Loss: 0.4692, Dev: 0.7350\n",
            "Epoch: [35/1000], Step: [100/301], Loss: 0.7601, Dev: 0.6970\n",
            "Epoch: [35/1000], Step: [200/301], Loss: 0.4987, Dev: 0.7120\n",
            "Epoch: [35/1000], Step: [300/301], Loss: 0.5698, Dev: 0.6690\n",
            "Epoch: [36/1000], Step: [100/301], Loss: 0.5191, Dev: 0.7240\n",
            "Epoch: [36/1000], Step: [200/301], Loss: 0.4707, Dev: 0.6810\n",
            "Epoch: [36/1000], Step: [300/301], Loss: 0.5182, Dev: 0.7060\n",
            "Epoch: [37/1000], Step: [100/301], Loss: 0.6647, Dev: 0.6840\n",
            "Epoch: [37/1000], Step: [200/301], Loss: 0.6503, Dev: 0.6910\n",
            "Epoch: [37/1000], Step: [300/301], Loss: 1.0515, Dev: 0.6890\n",
            "Epoch: [38/1000], Step: [100/301], Loss: 0.8848, Dev: 0.7120\n",
            "Epoch: [38/1000], Step: [200/301], Loss: 0.3620, Dev: 0.7050\n",
            "Epoch: [38/1000], Step: [300/301], Loss: 0.6412, Dev: 0.7230\n",
            "Epoch: [39/1000], Step: [100/301], Loss: 0.5478, Dev: 0.6740\n",
            "Epoch: [39/1000], Step: [200/301], Loss: 0.6395, Dev: 0.7060\n",
            "Epoch: [39/1000], Step: [300/301], Loss: 0.5204, Dev: 0.7120\n",
            "Epoch: [40/1000], Step: [100/301], Loss: 0.5646, Dev: 0.7300\n",
            "Epoch: [40/1000], Step: [200/301], Loss: 0.7477, Dev: 0.7200\n",
            "Epoch: [40/1000], Step: [300/301], Loss: 0.5575, Dev: 0.6980\n",
            "Epoch: [41/1000], Step: [100/301], Loss: 0.6363, Dev: 0.7500\n",
            "Epoch: [41/1000], Step: [200/301], Loss: 0.5807, Dev: 0.7050\n",
            "Epoch: [41/1000], Step: [300/301], Loss: 0.8001, Dev: 0.6930\n",
            "Epoch: [42/1000], Step: [100/301], Loss: 0.7311, Dev: 0.6940\n",
            "Epoch: [42/1000], Step: [200/301], Loss: 0.6061, Dev: 0.7350\n",
            "Epoch: [42/1000], Step: [300/301], Loss: 1.0614, Dev: 0.6870\n",
            "Epoch: [43/1000], Step: [100/301], Loss: 0.3647, Dev: 0.6640\n",
            "Epoch: [43/1000], Step: [200/301], Loss: 0.4705, Dev: 0.7120\n",
            "Epoch: [43/1000], Step: [300/301], Loss: 0.4202, Dev: 0.7150\n",
            "Epoch: [44/1000], Step: [100/301], Loss: 0.4814, Dev: 0.7440\n",
            "Epoch: [44/1000], Step: [200/301], Loss: 0.3595, Dev: 0.7270\n",
            "Epoch: [44/1000], Step: [300/301], Loss: 0.4421, Dev: 0.7070\n",
            "Epoch: [45/1000], Step: [100/301], Loss: 0.5472, Dev: 0.7270\n",
            "Epoch: [45/1000], Step: [200/301], Loss: 0.4281, Dev: 0.6940\n",
            "Epoch: [45/1000], Step: [300/301], Loss: 0.6508, Dev: 0.7390\n",
            "Epoch: [46/1000], Step: [100/301], Loss: 0.4237, Dev: 0.7390\n",
            "Epoch: [46/1000], Step: [200/301], Loss: 0.5496, Dev: 0.7010\n",
            "Epoch: [46/1000], Step: [300/301], Loss: 0.8326, Dev: 0.6850\n",
            "Epoch: [47/1000], Step: [100/301], Loss: 0.5071, Dev: 0.7270\n",
            "Epoch: [47/1000], Step: [200/301], Loss: 0.3011, Dev: 0.6970\n",
            "Epoch: [47/1000], Step: [300/301], Loss: 0.5545, Dev: 0.7070\n",
            "Epoch: [48/1000], Step: [100/301], Loss: 0.7237, Dev: 0.7030\n",
            "Epoch: [48/1000], Step: [200/301], Loss: 0.5378, Dev: 0.7430\n",
            "Epoch: [48/1000], Step: [300/301], Loss: 0.5251, Dev: 0.7350\n",
            "Epoch: [49/1000], Step: [100/301], Loss: 0.3682, Dev: 0.6730\n",
            "Epoch: [49/1000], Step: [200/301], Loss: 0.3969, Dev: 0.7480\n",
            "Epoch: [49/1000], Step: [300/301], Loss: 0.6132, Dev: 0.7370\n",
            "Epoch: [50/1000], Step: [100/301], Loss: 0.5255, Dev: 0.7390\n",
            "Epoch: [50/1000], Step: [200/301], Loss: 0.5013, Dev: 0.6790\n",
            "Epoch: [50/1000], Step: [300/301], Loss: 0.7548, Dev: 0.7130\n",
            "Epoch: [51/1000], Step: [100/301], Loss: 0.8208, Dev: 0.7020\n",
            "Epoch: [51/1000], Step: [200/301], Loss: 0.4923, Dev: 0.7310\n",
            "Epoch: [51/1000], Step: [300/301], Loss: 0.5890, Dev: 0.7210\n",
            "Epoch: [52/1000], Step: [100/301], Loss: 0.7374, Dev: 0.7480\n",
            "Epoch: [52/1000], Step: [200/301], Loss: 0.4245, Dev: 0.6960\n",
            "Epoch: [52/1000], Step: [300/301], Loss: 0.3584, Dev: 0.7050\n",
            "Epoch: [53/1000], Step: [100/301], Loss: 0.8006, Dev: 0.7250\n",
            "Epoch: [53/1000], Step: [200/301], Loss: 0.3999, Dev: 0.7360\n",
            "Epoch: [53/1000], Step: [300/301], Loss: 0.5244, Dev: 0.7510\n",
            "Epoch: [54/1000], Step: [100/301], Loss: 0.5995, Dev: 0.6810\n",
            "Epoch: [54/1000], Step: [200/301], Loss: 0.5387, Dev: 0.7540\n",
            "Epoch: [54/1000], Step: [300/301], Loss: 0.2418, Dev: 0.7260\n",
            "Epoch: [55/1000], Step: [100/301], Loss: 0.4322, Dev: 0.7080\n",
            "Epoch: [55/1000], Step: [200/301], Loss: 0.3257, Dev: 0.7200\n",
            "Epoch: [55/1000], Step: [300/301], Loss: 0.3087, Dev: 0.7280\n",
            "Epoch: [56/1000], Step: [100/301], Loss: 0.3873, Dev: 0.7090\n",
            "Epoch: [56/1000], Step: [200/301], Loss: 0.3028, Dev: 0.7150\n",
            "Epoch: [56/1000], Step: [300/301], Loss: 0.5334, Dev: 0.7410\n",
            "Epoch: [57/1000], Step: [100/301], Loss: 0.6442, Dev: 0.7180\n",
            "Epoch: [57/1000], Step: [200/301], Loss: 0.5037, Dev: 0.7310\n",
            "Epoch: [57/1000], Step: [300/301], Loss: 0.5765, Dev: 0.7380\n",
            "Epoch: [58/1000], Step: [100/301], Loss: 0.3615, Dev: 0.7260\n",
            "Epoch: [58/1000], Step: [200/301], Loss: 0.3812, Dev: 0.7260\n",
            "Epoch: [58/1000], Step: [300/301], Loss: 0.4175, Dev: 0.7170\n",
            "Epoch: [59/1000], Step: [100/301], Loss: 0.3766, Dev: 0.7040\n",
            "Epoch: [59/1000], Step: [200/301], Loss: 0.6322, Dev: 0.7140\n",
            "Epoch: [59/1000], Step: [300/301], Loss: 0.3216, Dev: 0.7310\n",
            "Epoch: [60/1000], Step: [100/301], Loss: 0.3369, Dev: 0.7340\n",
            "Epoch: [60/1000], Step: [200/301], Loss: 0.6886, Dev: 0.7060\n",
            "Epoch: [60/1000], Step: [300/301], Loss: 0.3883, Dev: 0.7340\n",
            "Epoch: [61/1000], Step: [100/301], Loss: 0.2909, Dev: 0.7130\n",
            "Epoch: [61/1000], Step: [200/301], Loss: 0.4328, Dev: 0.7530\n",
            "Epoch: [61/1000], Step: [300/301], Loss: 0.2837, Dev: 0.7230\n",
            "Epoch: [62/1000], Step: [100/301], Loss: 0.5248, Dev: 0.7190\n",
            "Epoch: [62/1000], Step: [200/301], Loss: 0.4336, Dev: 0.7310\n",
            "Epoch: [62/1000], Step: [300/301], Loss: 0.3982, Dev: 0.7300\n",
            "Epoch: [63/1000], Step: [100/301], Loss: 0.6061, Dev: 0.7320\n",
            "Epoch: [63/1000], Step: [200/301], Loss: 0.3012, Dev: 0.7410\n",
            "Epoch: [63/1000], Step: [300/301], Loss: 0.5711, Dev: 0.7190\n",
            "Epoch: [64/1000], Step: [100/301], Loss: 0.6450, Dev: 0.7490\n",
            "Epoch: [64/1000], Step: [200/301], Loss: 0.6308, Dev: 0.7100\n",
            "Epoch: [64/1000], Step: [300/301], Loss: 0.5381, Dev: 0.7380\n",
            "Epoch: [65/1000], Step: [100/301], Loss: 0.3276, Dev: 0.7490\n",
            "Epoch: [65/1000], Step: [200/301], Loss: 0.5545, Dev: 0.7460\n",
            "Epoch: [65/1000], Step: [300/301], Loss: 0.4280, Dev: 0.7330\n",
            "Epoch: [66/1000], Step: [100/301], Loss: 0.2923, Dev: 0.7390\n",
            "Epoch: [66/1000], Step: [200/301], Loss: 0.5109, Dev: 0.7240\n",
            "Epoch: [66/1000], Step: [300/301], Loss: 0.4114, Dev: 0.7310\n",
            "Epoch: [67/1000], Step: [100/301], Loss: 0.5284, Dev: 0.7290\n",
            "Epoch: [67/1000], Step: [200/301], Loss: 0.4205, Dev: 0.7430\n",
            "Epoch: [67/1000], Step: [300/301], Loss: 0.3670, Dev: 0.7410\n",
            "Epoch: [68/1000], Step: [100/301], Loss: 0.4154, Dev: 0.7260\n",
            "Epoch: [68/1000], Step: [200/301], Loss: 0.4158, Dev: 0.7400\n",
            "Epoch: [68/1000], Step: [300/301], Loss: 0.4639, Dev: 0.7360\n",
            "Epoch: [69/1000], Step: [100/301], Loss: 0.6896, Dev: 0.7370\n",
            "Epoch: [69/1000], Step: [200/301], Loss: 0.3567, Dev: 0.7270\n",
            "Epoch: [69/1000], Step: [300/301], Loss: 0.4924, Dev: 0.7040\n",
            "Epoch: [70/1000], Step: [100/301], Loss: 0.3755, Dev: 0.7200\n",
            "Epoch: [70/1000], Step: [200/301], Loss: 0.3937, Dev: 0.7320\n",
            "Epoch: [70/1000], Step: [300/301], Loss: 0.4609, Dev: 0.7350\n",
            "Epoch: [71/1000], Step: [100/301], Loss: 0.2925, Dev: 0.7320\n",
            "Epoch: [71/1000], Step: [200/301], Loss: 0.4115, Dev: 0.7540\n",
            "Epoch: [71/1000], Step: [300/301], Loss: 0.5181, Dev: 0.7370\n",
            "Epoch: [72/1000], Step: [100/301], Loss: 0.4738, Dev: 0.7510\n",
            "Epoch: [72/1000], Step: [200/301], Loss: 0.5240, Dev: 0.7130\n",
            "Epoch: [72/1000], Step: [300/301], Loss: 0.3705, Dev: 0.7180\n",
            "Epoch: [73/1000], Step: [100/301], Loss: 0.6032, Dev: 0.7440\n",
            "Epoch: [73/1000], Step: [200/301], Loss: 0.2315, Dev: 0.7300\n",
            "Epoch: [73/1000], Step: [300/301], Loss: 0.4036, Dev: 0.7210\n",
            "Epoch: [74/1000], Step: [100/301], Loss: 0.2826, Dev: 0.7430\n",
            "Epoch: [74/1000], Step: [200/301], Loss: 0.3439, Dev: 0.7060\n",
            "Epoch: [74/1000], Step: [300/301], Loss: 0.3675, Dev: 0.7190\n",
            "Epoch: [75/1000], Step: [100/301], Loss: 0.5243, Dev: 0.7400\n",
            "Epoch: [75/1000], Step: [200/301], Loss: 0.2771, Dev: 0.7550\n",
            "Epoch: [75/1000], Step: [300/301], Loss: 0.5027, Dev: 0.7390\n",
            "Epoch: [76/1000], Step: [100/301], Loss: 0.2661, Dev: 0.7570\n",
            "Epoch: [76/1000], Step: [200/301], Loss: 0.5058, Dev: 0.7340\n",
            "Epoch: [76/1000], Step: [300/301], Loss: 0.3889, Dev: 0.7420\n",
            "Epoch: [77/1000], Step: [100/301], Loss: 0.4482, Dev: 0.7490\n",
            "Epoch: [77/1000], Step: [200/301], Loss: 0.4667, Dev: 0.7340\n",
            "Epoch: [77/1000], Step: [300/301], Loss: 0.4140, Dev: 0.7090\n",
            "Epoch: [78/1000], Step: [100/301], Loss: 0.3906, Dev: 0.7290\n",
            "Epoch: [78/1000], Step: [200/301], Loss: 0.4130, Dev: 0.7310\n",
            "Epoch: [78/1000], Step: [300/301], Loss: 0.3908, Dev: 0.7380\n",
            "Epoch: [79/1000], Step: [100/301], Loss: 0.4993, Dev: 0.7280\n",
            "Epoch: [79/1000], Step: [200/301], Loss: 0.4135, Dev: 0.7310\n",
            "Epoch: [79/1000], Step: [300/301], Loss: 0.5146, Dev: 0.7320\n",
            "Epoch: [80/1000], Step: [100/301], Loss: 0.5058, Dev: 0.7380\n",
            "Epoch: [80/1000], Step: [200/301], Loss: 0.4831, Dev: 0.7540\n",
            "Epoch: [80/1000], Step: [300/301], Loss: 0.4496, Dev: 0.7380\n",
            "Epoch: [81/1000], Step: [100/301], Loss: 0.4006, Dev: 0.7190\n",
            "Epoch: [81/1000], Step: [200/301], Loss: 0.3020, Dev: 0.7450\n",
            "Epoch: [81/1000], Step: [300/301], Loss: 0.5119, Dev: 0.7300\n",
            "Epoch: [82/1000], Step: [100/301], Loss: 0.3733, Dev: 0.7330\n",
            "Epoch: [82/1000], Step: [200/301], Loss: 0.3817, Dev: 0.7230\n",
            "Epoch: [82/1000], Step: [300/301], Loss: 0.4147, Dev: 0.7390\n",
            "Epoch: [83/1000], Step: [100/301], Loss: 0.4156, Dev: 0.7290\n",
            "Epoch: [83/1000], Step: [200/301], Loss: 0.4379, Dev: 0.7470\n",
            "Epoch: [83/1000], Step: [300/301], Loss: 0.3901, Dev: 0.7280\n",
            "Epoch: [84/1000], Step: [100/301], Loss: 0.2198, Dev: 0.7310\n",
            "Epoch: [84/1000], Step: [200/301], Loss: 0.3753, Dev: 0.7260\n",
            "Epoch: [84/1000], Step: [300/301], Loss: 0.3902, Dev: 0.7330\n",
            "Epoch: [85/1000], Step: [100/301], Loss: 0.4057, Dev: 0.7430\n",
            "Epoch: [85/1000], Step: [200/301], Loss: 0.3351, Dev: 0.7660\n",
            "Epoch: [85/1000], Step: [300/301], Loss: 0.2509, Dev: 0.7340\n",
            "Epoch: [86/1000], Step: [100/301], Loss: 0.4591, Dev: 0.7280\n",
            "Epoch: [86/1000], Step: [200/301], Loss: 0.4045, Dev: 0.7310\n",
            "Epoch: [86/1000], Step: [300/301], Loss: 0.5939, Dev: 0.7310\n",
            "Epoch: [87/1000], Step: [100/301], Loss: 0.4103, Dev: 0.7380\n",
            "Epoch: [87/1000], Step: [200/301], Loss: 0.5535, Dev: 0.7260\n",
            "Epoch: [87/1000], Step: [300/301], Loss: 0.3060, Dev: 0.7470\n",
            "Epoch: [88/1000], Step: [100/301], Loss: 0.2894, Dev: 0.7470\n",
            "Epoch: [88/1000], Step: [200/301], Loss: 0.4716, Dev: 0.7400\n",
            "Epoch: [88/1000], Step: [300/301], Loss: 0.4405, Dev: 0.7170\n",
            "Epoch: [89/1000], Step: [100/301], Loss: 0.3503, Dev: 0.7250\n",
            "Epoch: [89/1000], Step: [200/301], Loss: 0.2044, Dev: 0.7260\n",
            "Epoch: [89/1000], Step: [300/301], Loss: 0.5652, Dev: 0.7340\n",
            "Epoch: [90/1000], Step: [100/301], Loss: 0.4739, Dev: 0.7320\n",
            "Epoch: [90/1000], Step: [200/301], Loss: 0.3277, Dev: 0.7670\n",
            "Epoch: [90/1000], Step: [300/301], Loss: 0.3434, Dev: 0.7200\n",
            "Epoch: [91/1000], Step: [100/301], Loss: 0.2399, Dev: 0.7470\n",
            "Epoch: [91/1000], Step: [200/301], Loss: 0.2134, Dev: 0.7300\n",
            "Epoch: [91/1000], Step: [300/301], Loss: 0.3814, Dev: 0.7420\n",
            "Epoch: [92/1000], Step: [100/301], Loss: 0.3896, Dev: 0.7610\n",
            "Epoch: [92/1000], Step: [200/301], Loss: 0.3767, Dev: 0.7190\n",
            "Epoch: [92/1000], Step: [300/301], Loss: 0.3642, Dev: 0.7490\n",
            "Epoch: [93/1000], Step: [100/301], Loss: 0.4224, Dev: 0.7400\n",
            "Epoch: [93/1000], Step: [200/301], Loss: 0.3804, Dev: 0.7140\n",
            "Epoch: [93/1000], Step: [300/301], Loss: 0.3492, Dev: 0.7490\n",
            "Epoch: [94/1000], Step: [100/301], Loss: 0.3642, Dev: 0.7540\n",
            "Epoch: [94/1000], Step: [200/301], Loss: 0.4488, Dev: 0.7260\n",
            "Epoch: [94/1000], Step: [300/301], Loss: 0.3418, Dev: 0.7780\n",
            "Epoch: [95/1000], Step: [100/301], Loss: 0.3912, Dev: 0.7370\n",
            "Epoch: [95/1000], Step: [200/301], Loss: 0.1789, Dev: 0.7670\n",
            "Epoch: [95/1000], Step: [300/301], Loss: 0.5382, Dev: 0.7220\n",
            "Epoch: [96/1000], Step: [100/301], Loss: 0.2840, Dev: 0.7480\n",
            "Epoch: [96/1000], Step: [200/301], Loss: 0.4454, Dev: 0.7100\n",
            "Epoch: [96/1000], Step: [300/301], Loss: 0.3069, Dev: 0.7340\n",
            "Epoch: [97/1000], Step: [100/301], Loss: 0.2634, Dev: 0.7340\n",
            "Epoch: [97/1000], Step: [200/301], Loss: 0.3823, Dev: 0.7390\n",
            "Epoch: [97/1000], Step: [300/301], Loss: 0.4740, Dev: 0.7270\n",
            "Epoch: [98/1000], Step: [100/301], Loss: 0.1997, Dev: 0.7590\n",
            "Epoch: [98/1000], Step: [200/301], Loss: 0.3229, Dev: 0.7230\n",
            "Epoch: [98/1000], Step: [300/301], Loss: 0.4200, Dev: 0.7230\n",
            "Epoch: [99/1000], Step: [100/301], Loss: 0.3426, Dev: 0.7360\n",
            "Epoch: [99/1000], Step: [200/301], Loss: 0.4195, Dev: 0.7500\n",
            "Epoch: [99/1000], Step: [300/301], Loss: 0.4549, Dev: 0.7420\n",
            "Epoch: [100/1000], Step: [100/301], Loss: 0.3418, Dev: 0.7440\n",
            "Epoch: [100/1000], Step: [200/301], Loss: 0.4898, Dev: 0.7430\n",
            "Epoch: [100/1000], Step: [300/301], Loss: 0.3427, Dev: 0.7490\n",
            "Epoch: [101/1000], Step: [100/301], Loss: 0.2765, Dev: 0.7400\n",
            "Epoch: [101/1000], Step: [200/301], Loss: 0.5420, Dev: 0.7230\n",
            "Epoch: [101/1000], Step: [300/301], Loss: 0.3812, Dev: 0.7320\n",
            "Epoch: [102/1000], Step: [100/301], Loss: 0.2808, Dev: 0.7510\n",
            "Epoch: [102/1000], Step: [200/301], Loss: 0.3407, Dev: 0.7240\n",
            "Epoch: [102/1000], Step: [300/301], Loss: 0.3315, Dev: 0.7450\n",
            "Epoch: [103/1000], Step: [100/301], Loss: 0.2755, Dev: 0.7490\n",
            "Epoch: [103/1000], Step: [200/301], Loss: 0.1545, Dev: 0.7450\n",
            "Epoch: [103/1000], Step: [300/301], Loss: 0.2575, Dev: 0.7150\n",
            "Epoch: [104/1000], Step: [100/301], Loss: 0.3963, Dev: 0.7810\n",
            "Epoch: [104/1000], Step: [200/301], Loss: 0.3314, Dev: 0.7360\n",
            "Epoch: [104/1000], Step: [300/301], Loss: 0.2185, Dev: 0.6950\n",
            "Epoch: [105/1000], Step: [100/301], Loss: 0.3518, Dev: 0.7130\n",
            "Epoch: [105/1000], Step: [200/301], Loss: 0.2920, Dev: 0.7550\n",
            "Epoch: [105/1000], Step: [300/301], Loss: 0.4534, Dev: 0.7540\n",
            "Epoch: [106/1000], Step: [100/301], Loss: 0.2664, Dev: 0.7160\n",
            "Epoch: [106/1000], Step: [200/301], Loss: 0.3597, Dev: 0.7610\n",
            "Epoch: [106/1000], Step: [300/301], Loss: 0.3084, Dev: 0.7520\n",
            "Epoch: [107/1000], Step: [100/301], Loss: 0.3217, Dev: 0.7310\n",
            "Epoch: [107/1000], Step: [200/301], Loss: 0.3106, Dev: 0.7460\n",
            "Epoch: [107/1000], Step: [300/301], Loss: 0.3599, Dev: 0.7450\n",
            "Epoch: [108/1000], Step: [100/301], Loss: 0.4642, Dev: 0.7200\n",
            "Epoch: [108/1000], Step: [200/301], Loss: 0.3699, Dev: 0.7560\n",
            "Epoch: [108/1000], Step: [300/301], Loss: 0.3250, Dev: 0.7340\n",
            "Epoch: [109/1000], Step: [100/301], Loss: 0.3102, Dev: 0.7270\n",
            "Epoch: [109/1000], Step: [200/301], Loss: 0.3162, Dev: 0.7340\n",
            "Epoch: [109/1000], Step: [300/301], Loss: 0.2922, Dev: 0.7310\n",
            "Epoch: [110/1000], Step: [100/301], Loss: 0.3344, Dev: 0.7620\n",
            "Epoch: [110/1000], Step: [200/301], Loss: 0.4109, Dev: 0.7070\n",
            "Epoch: [110/1000], Step: [300/301], Loss: 0.3074, Dev: 0.7440\n",
            "Epoch: [111/1000], Step: [100/301], Loss: 0.3875, Dev: 0.7430\n",
            "Epoch: [111/1000], Step: [200/301], Loss: 0.3490, Dev: 0.7120\n",
            "Epoch: [111/1000], Step: [300/301], Loss: 0.4340, Dev: 0.7540\n",
            "Epoch: [112/1000], Step: [100/301], Loss: 0.2509, Dev: 0.7280\n",
            "Epoch: [112/1000], Step: [200/301], Loss: 0.4891, Dev: 0.7560\n",
            "Epoch: [112/1000], Step: [300/301], Loss: 0.1698, Dev: 0.7410\n",
            "Epoch: [113/1000], Step: [100/301], Loss: 0.1829, Dev: 0.7750\n",
            "Epoch: [113/1000], Step: [200/301], Loss: 0.3150, Dev: 0.7480\n",
            "Epoch: [113/1000], Step: [300/301], Loss: 0.3622, Dev: 0.7430\n",
            "Epoch: [114/1000], Step: [100/301], Loss: 0.2648, Dev: 0.7440\n",
            "Epoch: [114/1000], Step: [200/301], Loss: 0.3297, Dev: 0.7500\n",
            "Epoch: [114/1000], Step: [300/301], Loss: 0.1938, Dev: 0.7450\n",
            "Epoch: [115/1000], Step: [100/301], Loss: 0.3519, Dev: 0.7630\n",
            "Epoch: [115/1000], Step: [200/301], Loss: 0.2955, Dev: 0.7330\n",
            "Epoch: [115/1000], Step: [300/301], Loss: 0.4897, Dev: 0.7330\n",
            "Epoch: [116/1000], Step: [100/301], Loss: 0.4582, Dev: 0.7410\n",
            "Epoch: [116/1000], Step: [200/301], Loss: 0.3178, Dev: 0.7610\n",
            "Epoch: [116/1000], Step: [300/301], Loss: 0.5088, Dev: 0.7440\n",
            "Epoch: [117/1000], Step: [100/301], Loss: 0.2626, Dev: 0.7520\n",
            "Epoch: [117/1000], Step: [200/301], Loss: 0.2535, Dev: 0.7270\n",
            "Epoch: [117/1000], Step: [300/301], Loss: 0.3785, Dev: 0.7660\n",
            "Epoch: [118/1000], Step: [100/301], Loss: 0.2507, Dev: 0.7360\n",
            "Epoch: [118/1000], Step: [200/301], Loss: 0.2323, Dev: 0.7410\n",
            "Epoch: [118/1000], Step: [300/301], Loss: 0.3210, Dev: 0.7410\n",
            "Epoch: [119/1000], Step: [100/301], Loss: 0.4961, Dev: 0.7450\n",
            "Epoch: [119/1000], Step: [200/301], Loss: 0.3031, Dev: 0.7640\n",
            "Epoch: [119/1000], Step: [300/301], Loss: 0.3754, Dev: 0.7750\n",
            "Epoch: [120/1000], Step: [100/301], Loss: 0.2669, Dev: 0.7340\n",
            "Epoch: [120/1000], Step: [200/301], Loss: 0.3716, Dev: 0.7590\n",
            "Epoch: [120/1000], Step: [300/301], Loss: 0.2455, Dev: 0.7520\n",
            "Epoch: [121/1000], Step: [100/301], Loss: 0.3414, Dev: 0.7340\n",
            "Epoch: [121/1000], Step: [200/301], Loss: 0.2636, Dev: 0.7320\n",
            "Epoch: [121/1000], Step: [300/301], Loss: 0.3495, Dev: 0.7580\n",
            "Epoch: [122/1000], Step: [100/301], Loss: 0.2149, Dev: 0.7510\n",
            "Epoch: [122/1000], Step: [200/301], Loss: 0.4402, Dev: 0.7550\n",
            "Epoch: [122/1000], Step: [300/301], Loss: 0.2319, Dev: 0.7450\n",
            "Epoch: [123/1000], Step: [100/301], Loss: 0.2019, Dev: 0.7670\n",
            "Epoch: [123/1000], Step: [200/301], Loss: 0.3567, Dev: 0.7350\n",
            "Epoch: [123/1000], Step: [300/301], Loss: 0.2308, Dev: 0.7550\n",
            "Epoch: [124/1000], Step: [100/301], Loss: 0.3844, Dev: 0.7410\n",
            "Epoch: [124/1000], Step: [200/301], Loss: 0.2627, Dev: 0.7550\n",
            "Epoch: [124/1000], Step: [300/301], Loss: 0.3331, Dev: 0.7560\n",
            "Epoch: [125/1000], Step: [100/301], Loss: 0.1672, Dev: 0.7350\n",
            "Epoch: [125/1000], Step: [200/301], Loss: 0.4731, Dev: 0.7570\n",
            "Epoch: [125/1000], Step: [300/301], Loss: 0.2708, Dev: 0.7480\n",
            "Epoch: [126/1000], Step: [100/301], Loss: 0.2830, Dev: 0.7410\n",
            "Epoch: [126/1000], Step: [200/301], Loss: 0.4327, Dev: 0.7630\n",
            "Epoch: [126/1000], Step: [300/301], Loss: 0.3947, Dev: 0.7730\n",
            "Epoch: [127/1000], Step: [100/301], Loss: 0.3650, Dev: 0.7190\n",
            "Epoch: [127/1000], Step: [200/301], Loss: 0.4024, Dev: 0.7400\n",
            "Epoch: [127/1000], Step: [300/301], Loss: 0.2226, Dev: 0.7590\n",
            "Epoch: [128/1000], Step: [100/301], Loss: 0.3530, Dev: 0.7420\n",
            "Epoch: [128/1000], Step: [200/301], Loss: 0.4323, Dev: 0.7680\n",
            "Epoch: [128/1000], Step: [300/301], Loss: 0.4351, Dev: 0.7460\n",
            "Epoch: [129/1000], Step: [100/301], Loss: 0.3848, Dev: 0.7710\n",
            "Epoch: [129/1000], Step: [200/301], Loss: 0.3093, Dev: 0.7520\n",
            "Epoch: [129/1000], Step: [300/301], Loss: 0.3494, Dev: 0.7490\n",
            "Epoch: [130/1000], Step: [100/301], Loss: 0.2014, Dev: 0.7390\n",
            "Epoch: [130/1000], Step: [200/301], Loss: 0.2506, Dev: 0.7200\n",
            "Epoch: [130/1000], Step: [300/301], Loss: 0.2069, Dev: 0.7420\n",
            "Epoch: [131/1000], Step: [100/301], Loss: 0.3263, Dev: 0.7780\n",
            "Epoch: [131/1000], Step: [200/301], Loss: 0.2284, Dev: 0.7590\n",
            "Epoch: [131/1000], Step: [300/301], Loss: 0.5219, Dev: 0.7420\n",
            "Epoch: [132/1000], Step: [100/301], Loss: 0.2568, Dev: 0.7530\n",
            "Epoch: [132/1000], Step: [200/301], Loss: 0.2341, Dev: 0.7260\n",
            "Epoch: [132/1000], Step: [300/301], Loss: 0.4127, Dev: 0.7420\n",
            "Epoch: [133/1000], Step: [100/301], Loss: 0.3472, Dev: 0.7480\n",
            "Epoch: [133/1000], Step: [200/301], Loss: 0.2710, Dev: 0.7750\n",
            "Epoch: [133/1000], Step: [300/301], Loss: 0.2953, Dev: 0.7440\n",
            "Epoch: [134/1000], Step: [100/301], Loss: 0.3896, Dev: 0.7750\n",
            "Epoch: [134/1000], Step: [200/301], Loss: 0.2764, Dev: 0.7590\n",
            "Epoch: [134/1000], Step: [300/301], Loss: 0.3971, Dev: 0.7360\n",
            "Epoch: [135/1000], Step: [100/301], Loss: 0.1867, Dev: 0.7550\n",
            "Epoch: [135/1000], Step: [200/301], Loss: 0.2735, Dev: 0.7470\n",
            "Epoch: [135/1000], Step: [300/301], Loss: 0.2289, Dev: 0.7400\n",
            "Epoch: [136/1000], Step: [100/301], Loss: 0.1913, Dev: 0.7300\n",
            "Epoch: [136/1000], Step: [200/301], Loss: 0.3210, Dev: 0.7730\n",
            "Epoch: [136/1000], Step: [300/301], Loss: 0.3876, Dev: 0.7440\n",
            "Epoch: [137/1000], Step: [100/301], Loss: 0.3643, Dev: 0.7410\n",
            "Epoch: [137/1000], Step: [200/301], Loss: 0.4171, Dev: 0.7180\n",
            "Epoch: [137/1000], Step: [300/301], Loss: 0.2614, Dev: 0.7320\n",
            "Epoch: [138/1000], Step: [100/301], Loss: 0.3798, Dev: 0.7600\n",
            "Epoch: [138/1000], Step: [200/301], Loss: 0.2053, Dev: 0.7550\n",
            "Epoch: [138/1000], Step: [300/301], Loss: 0.3474, Dev: 0.7500\n",
            "Epoch: [139/1000], Step: [100/301], Loss: 0.2764, Dev: 0.7370\n",
            "Epoch: [139/1000], Step: [200/301], Loss: 0.2247, Dev: 0.7310\n",
            "Epoch: [139/1000], Step: [300/301], Loss: 0.3530, Dev: 0.7590\n",
            "Epoch: [140/1000], Step: [100/301], Loss: 0.3373, Dev: 0.7340\n",
            "Epoch: [140/1000], Step: [200/301], Loss: 0.4390, Dev: 0.7480\n",
            "Epoch: [140/1000], Step: [300/301], Loss: 0.3913, Dev: 0.7600\n",
            "Epoch: [141/1000], Step: [100/301], Loss: 0.2340, Dev: 0.7490\n",
            "Epoch: [141/1000], Step: [200/301], Loss: 0.3237, Dev: 0.7230\n",
            "Epoch: [141/1000], Step: [300/301], Loss: 0.5019, Dev: 0.7580\n",
            "Epoch: [142/1000], Step: [100/301], Loss: 0.3019, Dev: 0.7420\n",
            "Epoch: [142/1000], Step: [200/301], Loss: 0.1785, Dev: 0.7220\n",
            "Epoch: [142/1000], Step: [300/301], Loss: 0.2705, Dev: 0.7310\n",
            "Epoch: [143/1000], Step: [100/301], Loss: 0.3505, Dev: 0.7540\n",
            "Epoch: [143/1000], Step: [200/301], Loss: 0.1509, Dev: 0.7500\n",
            "Epoch: [143/1000], Step: [300/301], Loss: 0.2417, Dev: 0.7450\n",
            "Epoch: [144/1000], Step: [100/301], Loss: 0.1195, Dev: 0.7530\n",
            "Epoch: [144/1000], Step: [200/301], Loss: 0.1855, Dev: 0.7320\n",
            "Epoch: [144/1000], Step: [300/301], Loss: 0.3641, Dev: 0.7380\n",
            "Epoch: [145/1000], Step: [100/301], Loss: 0.2729, Dev: 0.7360\n",
            "Epoch: [145/1000], Step: [200/301], Loss: 0.3938, Dev: 0.7350\n",
            "Epoch: [145/1000], Step: [300/301], Loss: 0.3230, Dev: 0.7580\n",
            "Epoch: [146/1000], Step: [100/301], Loss: 0.4215, Dev: 0.7520\n",
            "Epoch: [146/1000], Step: [200/301], Loss: 0.4500, Dev: 0.7580\n",
            "Epoch: [146/1000], Step: [300/301], Loss: 0.2699, Dev: 0.7490\n",
            "Epoch: [147/1000], Step: [100/301], Loss: 0.4890, Dev: 0.7290\n",
            "Epoch: [147/1000], Step: [200/301], Loss: 0.3423, Dev: 0.7630\n",
            "Epoch: [147/1000], Step: [300/301], Loss: 0.2161, Dev: 0.7420\n",
            "Epoch: [148/1000], Step: [100/301], Loss: 0.2925, Dev: 0.7690\n",
            "Epoch: [148/1000], Step: [200/301], Loss: 0.2672, Dev: 0.7400\n",
            "Epoch: [148/1000], Step: [300/301], Loss: 0.3492, Dev: 0.7380\n",
            "Epoch: [149/1000], Step: [100/301], Loss: 0.2262, Dev: 0.7560\n",
            "Epoch: [149/1000], Step: [200/301], Loss: 0.2556, Dev: 0.7550\n",
            "Epoch: [149/1000], Step: [300/301], Loss: 0.3226, Dev: 0.7620\n",
            "Epoch: [150/1000], Step: [100/301], Loss: 0.1533, Dev: 0.7580\n",
            "Epoch: [150/1000], Step: [200/301], Loss: 0.3426, Dev: 0.7570\n",
            "Epoch: [150/1000], Step: [300/301], Loss: 0.2510, Dev: 0.7570\n",
            "Epoch: [151/1000], Step: [100/301], Loss: 0.3810, Dev: 0.7620\n",
            "Epoch: [151/1000], Step: [200/301], Loss: 0.2646, Dev: 0.7500\n",
            "Epoch: [151/1000], Step: [300/301], Loss: 0.3854, Dev: 0.7410\n",
            "Epoch: [152/1000], Step: [100/301], Loss: 0.2189, Dev: 0.7660\n",
            "Epoch: [152/1000], Step: [200/301], Loss: 0.2593, Dev: 0.7300\n",
            "Epoch: [152/1000], Step: [300/301], Loss: 0.1999, Dev: 0.7360\n",
            "Epoch: [153/1000], Step: [100/301], Loss: 0.2907, Dev: 0.7260\n",
            "Epoch: [153/1000], Step: [200/301], Loss: 0.3370, Dev: 0.7400\n",
            "Epoch: [153/1000], Step: [300/301], Loss: 0.3386, Dev: 0.7640\n",
            "Epoch: [154/1000], Step: [100/301], Loss: 0.2171, Dev: 0.7410\n",
            "Epoch: [154/1000], Step: [200/301], Loss: 0.3139, Dev: 0.7410\n",
            "Epoch: [154/1000], Step: [300/301], Loss: 0.3330, Dev: 0.7410\n",
            "Epoch: [155/1000], Step: [100/301], Loss: 0.3143, Dev: 0.7420\n",
            "Epoch: [155/1000], Step: [200/301], Loss: 0.1822, Dev: 0.7530\n",
            "Epoch: [155/1000], Step: [300/301], Loss: 0.2537, Dev: 0.7400\n",
            "Epoch: [156/1000], Step: [100/301], Loss: 0.2832, Dev: 0.7280\n",
            "Epoch: [156/1000], Step: [200/301], Loss: 0.3108, Dev: 0.7470\n",
            "Epoch: [156/1000], Step: [300/301], Loss: 0.2781, Dev: 0.7670\n",
            "Epoch: [157/1000], Step: [100/301], Loss: 0.1912, Dev: 0.7440\n",
            "Epoch: [157/1000], Step: [200/301], Loss: 0.3252, Dev: 0.7370\n",
            "Epoch: [157/1000], Step: [300/301], Loss: 0.3629, Dev: 0.7280\n",
            "Epoch: [158/1000], Step: [100/301], Loss: 0.2643, Dev: 0.7520\n",
            "Epoch: [158/1000], Step: [200/301], Loss: 0.2554, Dev: 0.7540\n",
            "Epoch: [158/1000], Step: [300/301], Loss: 0.2781, Dev: 0.7590\n",
            "Epoch: [159/1000], Step: [100/301], Loss: 0.1419, Dev: 0.7530\n",
            "Epoch: [159/1000], Step: [200/301], Loss: 0.2624, Dev: 0.7430\n",
            "Epoch: [159/1000], Step: [300/301], Loss: 0.2675, Dev: 0.7590\n",
            "Epoch: [160/1000], Step: [100/301], Loss: 0.2031, Dev: 0.7550\n",
            "Epoch: [160/1000], Step: [200/301], Loss: 0.2272, Dev: 0.7650\n",
            "Epoch: [160/1000], Step: [300/301], Loss: 0.3595, Dev: 0.7470\n",
            "Epoch: [161/1000], Step: [100/301], Loss: 0.2175, Dev: 0.7670\n",
            "Epoch: [161/1000], Step: [200/301], Loss: 0.3313, Dev: 0.7350\n",
            "Epoch: [161/1000], Step: [300/301], Loss: 0.1560, Dev: 0.7560\n",
            "Epoch: [162/1000], Step: [100/301], Loss: 0.2120, Dev: 0.7650\n",
            "Epoch: [162/1000], Step: [200/301], Loss: 0.1826, Dev: 0.7340\n",
            "Epoch: [162/1000], Step: [300/301], Loss: 0.2988, Dev: 0.7640\n",
            "Epoch: [163/1000], Step: [100/301], Loss: 0.2939, Dev: 0.7490\n",
            "Epoch: [163/1000], Step: [200/301], Loss: 0.1919, Dev: 0.7570\n",
            "Epoch: [163/1000], Step: [300/301], Loss: 0.2587, Dev: 0.7250\n",
            "Epoch: [164/1000], Step: [100/301], Loss: 0.2579, Dev: 0.7440\n",
            "Epoch: [164/1000], Step: [200/301], Loss: 0.2811, Dev: 0.7450\n",
            "Epoch: [164/1000], Step: [300/301], Loss: 0.3445, Dev: 0.7580\n",
            "Epoch: [165/1000], Step: [100/301], Loss: 0.3418, Dev: 0.7630\n",
            "Epoch: [165/1000], Step: [200/301], Loss: 0.3891, Dev: 0.7460\n",
            "Epoch: [165/1000], Step: [300/301], Loss: 0.2612, Dev: 0.7510\n",
            "Epoch: [166/1000], Step: [100/301], Loss: 0.4421, Dev: 0.7490\n",
            "Epoch: [166/1000], Step: [200/301], Loss: 0.2125, Dev: 0.7420\n",
            "Epoch: [166/1000], Step: [300/301], Loss: 0.1813, Dev: 0.7470\n",
            "Epoch: [167/1000], Step: [100/301], Loss: 0.4780, Dev: 0.7570\n",
            "Epoch: [167/1000], Step: [200/301], Loss: 0.4066, Dev: 0.7470\n",
            "Epoch: [167/1000], Step: [300/301], Loss: 0.1749, Dev: 0.7510\n",
            "Epoch: [168/1000], Step: [100/301], Loss: 0.4426, Dev: 0.7620\n",
            "Epoch: [168/1000], Step: [200/301], Loss: 0.3417, Dev: 0.7730\n",
            "Epoch: [168/1000], Step: [300/301], Loss: 0.2148, Dev: 0.7520\n",
            "Epoch: [169/1000], Step: [100/301], Loss: 0.3454, Dev: 0.7520\n",
            "Epoch: [169/1000], Step: [200/301], Loss: 0.4148, Dev: 0.7520\n",
            "Epoch: [169/1000], Step: [300/301], Loss: 0.2580, Dev: 0.7390\n",
            "Epoch: [170/1000], Step: [100/301], Loss: 0.2213, Dev: 0.7450\n",
            "Epoch: [170/1000], Step: [200/301], Loss: 0.4094, Dev: 0.7510\n",
            "Epoch: [170/1000], Step: [300/301], Loss: 0.1082, Dev: 0.7730\n",
            "Epoch: [171/1000], Step: [100/301], Loss: 0.2187, Dev: 0.7560\n",
            "Epoch: [171/1000], Step: [200/301], Loss: 0.2696, Dev: 0.7660\n",
            "Epoch: [171/1000], Step: [300/301], Loss: 0.2438, Dev: 0.7420\n",
            "Epoch: [172/1000], Step: [100/301], Loss: 0.3145, Dev: 0.7620\n",
            "Epoch: [172/1000], Step: [200/301], Loss: 0.2650, Dev: 0.7660\n",
            "Epoch: [172/1000], Step: [300/301], Loss: 0.2452, Dev: 0.7500\n",
            "Epoch: [173/1000], Step: [100/301], Loss: 0.1256, Dev: 0.7300\n",
            "Epoch: [173/1000], Step: [200/301], Loss: 0.3517, Dev: 0.7710\n",
            "Epoch: [173/1000], Step: [300/301], Loss: 0.2124, Dev: 0.7580\n",
            "Epoch: [174/1000], Step: [100/301], Loss: 0.2680, Dev: 0.7690\n",
            "Epoch: [174/1000], Step: [200/301], Loss: 0.2155, Dev: 0.7730\n",
            "Epoch: [174/1000], Step: [300/301], Loss: 0.3569, Dev: 0.7700\n",
            "Epoch: [175/1000], Step: [100/301], Loss: 0.3955, Dev: 0.7680\n",
            "Epoch: [175/1000], Step: [200/301], Loss: 0.2808, Dev: 0.7600\n",
            "Epoch: [175/1000], Step: [300/301], Loss: 0.3561, Dev: 0.7710\n",
            "Epoch: [176/1000], Step: [100/301], Loss: 0.3113, Dev: 0.7510\n",
            "Epoch: [176/1000], Step: [200/301], Loss: 0.1956, Dev: 0.7710\n",
            "Epoch: [176/1000], Step: [300/301], Loss: 0.3305, Dev: 0.7300\n",
            "Epoch: [177/1000], Step: [100/301], Loss: 0.2316, Dev: 0.7620\n",
            "Epoch: [177/1000], Step: [200/301], Loss: 0.2234, Dev: 0.7330\n",
            "Epoch: [177/1000], Step: [300/301], Loss: 0.2902, Dev: 0.7670\n",
            "Epoch: [178/1000], Step: [100/301], Loss: 0.2382, Dev: 0.7580\n",
            "Epoch: [178/1000], Step: [200/301], Loss: 0.2815, Dev: 0.7510\n",
            "Epoch: [178/1000], Step: [300/301], Loss: 0.3006, Dev: 0.7750\n",
            "Epoch: [179/1000], Step: [100/301], Loss: 0.3105, Dev: 0.7490\n",
            "Epoch: [179/1000], Step: [200/301], Loss: 0.2685, Dev: 0.7730\n",
            "Epoch: [179/1000], Step: [300/301], Loss: 0.3524, Dev: 0.7560\n",
            "Epoch: [180/1000], Step: [100/301], Loss: 0.2329, Dev: 0.7540\n",
            "Epoch: [180/1000], Step: [200/301], Loss: 0.3040, Dev: 0.7640\n",
            "Epoch: [180/1000], Step: [300/301], Loss: 0.1952, Dev: 0.7380\n",
            "Epoch: [181/1000], Step: [100/301], Loss: 0.3610, Dev: 0.7590\n",
            "Epoch: [181/1000], Step: [200/301], Loss: 0.3036, Dev: 0.7370\n",
            "Epoch: [181/1000], Step: [300/301], Loss: 0.1485, Dev: 0.7570\n",
            "Epoch: [182/1000], Step: [100/301], Loss: 0.2207, Dev: 0.7740\n",
            "Epoch: [182/1000], Step: [200/301], Loss: 0.2713, Dev: 0.7670\n",
            "Epoch: [182/1000], Step: [300/301], Loss: 0.1488, Dev: 0.7600\n",
            "Epoch: [183/1000], Step: [100/301], Loss: 0.2156, Dev: 0.7660\n",
            "Epoch: [183/1000], Step: [200/301], Loss: 0.3097, Dev: 0.7510\n",
            "Epoch: [183/1000], Step: [300/301], Loss: 0.3270, Dev: 0.7550\n",
            "Epoch: [184/1000], Step: [100/301], Loss: 0.1129, Dev: 0.7670\n",
            "Epoch: [184/1000], Step: [200/301], Loss: 0.2189, Dev: 0.7630\n",
            "Epoch: [184/1000], Step: [300/301], Loss: 0.2104, Dev: 0.7500\n",
            "Epoch: [185/1000], Step: [100/301], Loss: 0.2392, Dev: 0.7730\n",
            "Epoch: [185/1000], Step: [200/301], Loss: 0.2377, Dev: 0.7600\n",
            "Epoch: [185/1000], Step: [300/301], Loss: 0.2322, Dev: 0.7640\n",
            "Epoch: [186/1000], Step: [100/301], Loss: 0.2699, Dev: 0.7780\n",
            "Epoch: [186/1000], Step: [200/301], Loss: 0.1627, Dev: 0.7610\n",
            "Epoch: [186/1000], Step: [300/301], Loss: 0.3181, Dev: 0.7680\n",
            "Epoch: [187/1000], Step: [100/301], Loss: 0.3141, Dev: 0.7580\n",
            "Epoch: [187/1000], Step: [200/301], Loss: 0.2673, Dev: 0.7430\n",
            "Epoch: [187/1000], Step: [300/301], Loss: 0.1228, Dev: 0.7470\n",
            "Epoch: [188/1000], Step: [100/301], Loss: 0.2629, Dev: 0.7310\n",
            "Epoch: [188/1000], Step: [200/301], Loss: 0.1490, Dev: 0.7750\n",
            "Epoch: [188/1000], Step: [300/301], Loss: 0.4227, Dev: 0.7860\n",
            "Epoch: [189/1000], Step: [100/301], Loss: 0.1516, Dev: 0.7500\n",
            "Epoch: [189/1000], Step: [200/301], Loss: 0.1559, Dev: 0.7610\n",
            "Epoch: [189/1000], Step: [300/301], Loss: 0.2531, Dev: 0.7630\n",
            "Epoch: [190/1000], Step: [100/301], Loss: 0.1964, Dev: 0.7660\n",
            "Epoch: [190/1000], Step: [200/301], Loss: 0.2603, Dev: 0.7570\n",
            "Epoch: [190/1000], Step: [300/301], Loss: 0.2001, Dev: 0.7720\n",
            "Epoch: [191/1000], Step: [100/301], Loss: 0.2461, Dev: 0.7670\n",
            "Epoch: [191/1000], Step: [200/301], Loss: 0.2955, Dev: 0.7760\n",
            "Epoch: [191/1000], Step: [300/301], Loss: 0.2641, Dev: 0.7760\n",
            "Epoch: [192/1000], Step: [100/301], Loss: 0.2393, Dev: 0.7700\n",
            "Epoch: [192/1000], Step: [200/301], Loss: 0.1543, Dev: 0.7540\n",
            "Epoch: [192/1000], Step: [300/301], Loss: 0.1975, Dev: 0.7820\n",
            "Epoch: [193/1000], Step: [100/301], Loss: 0.2710, Dev: 0.7570\n",
            "Epoch: [193/1000], Step: [200/301], Loss: 0.2626, Dev: 0.7750\n",
            "Epoch: [193/1000], Step: [300/301], Loss: 0.2384, Dev: 0.7580\n",
            "Epoch: [194/1000], Step: [100/301], Loss: 0.2875, Dev: 0.7580\n",
            "Epoch: [194/1000], Step: [200/301], Loss: 0.2474, Dev: 0.7660\n",
            "Epoch: [194/1000], Step: [300/301], Loss: 0.1701, Dev: 0.7480\n",
            "Epoch: [195/1000], Step: [100/301], Loss: 0.1656, Dev: 0.7530\n",
            "Epoch: [195/1000], Step: [200/301], Loss: 0.4667, Dev: 0.7570\n",
            "Epoch: [195/1000], Step: [300/301], Loss: 0.2756, Dev: 0.7770\n",
            "Epoch: [196/1000], Step: [100/301], Loss: 0.2146, Dev: 0.7660\n",
            "Epoch: [196/1000], Step: [200/301], Loss: 0.1631, Dev: 0.7480\n",
            "Epoch: [196/1000], Step: [300/301], Loss: 0.2066, Dev: 0.7360\n",
            "Epoch: [197/1000], Step: [100/301], Loss: 0.1759, Dev: 0.7530\n",
            "Epoch: [197/1000], Step: [200/301], Loss: 0.2489, Dev: 0.7830\n",
            "Epoch: [197/1000], Step: [300/301], Loss: 0.2320, Dev: 0.7770\n",
            "Epoch: [198/1000], Step: [100/301], Loss: 0.3014, Dev: 0.7580\n",
            "Epoch: [198/1000], Step: [200/301], Loss: 0.3497, Dev: 0.7790\n",
            "Epoch: [198/1000], Step: [300/301], Loss: 0.1774, Dev: 0.7450\n",
            "Epoch: [199/1000], Step: [100/301], Loss: 0.1967, Dev: 0.7820\n",
            "Epoch: [199/1000], Step: [200/301], Loss: 0.2398, Dev: 0.7650\n",
            "Epoch: [199/1000], Step: [300/301], Loss: 0.2982, Dev: 0.7770\n",
            "Epoch: [200/1000], Step: [100/301], Loss: 0.1744, Dev: 0.7580\n",
            "Epoch: [200/1000], Step: [200/301], Loss: 0.2707, Dev: 0.7600\n",
            "Epoch: [200/1000], Step: [300/301], Loss: 0.2223, Dev: 0.7620\n",
            "Epoch: [201/1000], Step: [100/301], Loss: 0.1655, Dev: 0.7610\n",
            "Epoch: [201/1000], Step: [200/301], Loss: 0.2151, Dev: 0.7650\n",
            "Epoch: [201/1000], Step: [300/301], Loss: 0.2372, Dev: 0.7640\n",
            "Epoch: [202/1000], Step: [100/301], Loss: 0.1847, Dev: 0.7760\n",
            "Epoch: [202/1000], Step: [200/301], Loss: 0.2007, Dev: 0.7610\n",
            "Epoch: [202/1000], Step: [300/301], Loss: 0.2288, Dev: 0.7470\n",
            "Epoch: [203/1000], Step: [100/301], Loss: 0.3523, Dev: 0.7740\n",
            "Epoch: [203/1000], Step: [200/301], Loss: 0.1267, Dev: 0.7250\n",
            "Epoch: [203/1000], Step: [300/301], Loss: 0.4375, Dev: 0.7740\n",
            "Epoch: [204/1000], Step: [100/301], Loss: 0.2453, Dev: 0.7280\n",
            "Epoch: [204/1000], Step: [200/301], Loss: 0.1935, Dev: 0.7660\n",
            "Epoch: [204/1000], Step: [300/301], Loss: 0.2629, Dev: 0.7660\n",
            "Epoch: [205/1000], Step: [100/301], Loss: 0.2272, Dev: 0.7750\n",
            "Epoch: [205/1000], Step: [200/301], Loss: 0.2564, Dev: 0.7620\n",
            "Epoch: [205/1000], Step: [300/301], Loss: 0.3300, Dev: 0.7640\n",
            "Epoch: [206/1000], Step: [100/301], Loss: 0.3323, Dev: 0.7620\n",
            "Epoch: [206/1000], Step: [200/301], Loss: 0.2737, Dev: 0.7760\n",
            "Epoch: [206/1000], Step: [300/301], Loss: 0.2195, Dev: 0.7610\n",
            "Epoch: [207/1000], Step: [100/301], Loss: 0.1994, Dev: 0.7580\n",
            "Epoch: [207/1000], Step: [200/301], Loss: 0.1564, Dev: 0.7670\n",
            "Epoch: [207/1000], Step: [300/301], Loss: 0.1861, Dev: 0.7520\n",
            "Epoch: [208/1000], Step: [100/301], Loss: 0.3431, Dev: 0.7660\n",
            "Epoch: [208/1000], Step: [200/301], Loss: 0.2642, Dev: 0.7770\n",
            "Epoch: [208/1000], Step: [300/301], Loss: 0.3500, Dev: 0.7750\n",
            "Epoch: [209/1000], Step: [100/301], Loss: 0.2765, Dev: 0.7670\n",
            "Epoch: [209/1000], Step: [200/301], Loss: 0.2898, Dev: 0.7680\n",
            "Epoch: [209/1000], Step: [300/301], Loss: 0.2341, Dev: 0.7750\n",
            "Epoch: [210/1000], Step: [100/301], Loss: 0.2749, Dev: 0.7750\n",
            "Epoch: [210/1000], Step: [200/301], Loss: 0.3179, Dev: 0.7760\n",
            "Epoch: [210/1000], Step: [300/301], Loss: 0.2440, Dev: 0.7620\n",
            "Epoch: [211/1000], Step: [100/301], Loss: 0.2381, Dev: 0.7540\n",
            "Epoch: [211/1000], Step: [200/301], Loss: 0.2997, Dev: 0.7580\n",
            "Epoch: [211/1000], Step: [300/301], Loss: 0.1843, Dev: 0.7650\n",
            "Epoch: [212/1000], Step: [100/301], Loss: 0.1871, Dev: 0.7620\n",
            "Epoch: [212/1000], Step: [200/301], Loss: 0.1905, Dev: 0.7660\n",
            "Epoch: [212/1000], Step: [300/301], Loss: 0.3889, Dev: 0.7840\n",
            "Epoch: [213/1000], Step: [100/301], Loss: 0.2545, Dev: 0.7730\n",
            "Epoch: [213/1000], Step: [200/301], Loss: 0.3075, Dev: 0.7530\n",
            "Epoch: [213/1000], Step: [300/301], Loss: 0.3197, Dev: 0.7740\n",
            "Epoch: [214/1000], Step: [100/301], Loss: 0.3423, Dev: 0.7680\n",
            "Epoch: [214/1000], Step: [200/301], Loss: 0.2183, Dev: 0.7820\n",
            "Epoch: [214/1000], Step: [300/301], Loss: 0.2826, Dev: 0.7560\n",
            "Epoch: [215/1000], Step: [100/301], Loss: 0.4038, Dev: 0.7580\n",
            "Epoch: [215/1000], Step: [200/301], Loss: 0.2726, Dev: 0.7570\n",
            "Epoch: [215/1000], Step: [300/301], Loss: 0.2510, Dev: 0.7710\n",
            "Epoch: [216/1000], Step: [100/301], Loss: 0.2667, Dev: 0.7470\n",
            "Epoch: [216/1000], Step: [200/301], Loss: 0.2792, Dev: 0.7460\n",
            "Epoch: [216/1000], Step: [300/301], Loss: 0.2805, Dev: 0.7630\n",
            "Epoch: [217/1000], Step: [100/301], Loss: 0.1907, Dev: 0.7780\n",
            "Epoch: [217/1000], Step: [200/301], Loss: 0.2229, Dev: 0.7690\n",
            "Epoch: [217/1000], Step: [300/301], Loss: 0.3088, Dev: 0.7710\n",
            "Epoch: [218/1000], Step: [100/301], Loss: 0.1394, Dev: 0.7870\n",
            "Epoch: [218/1000], Step: [200/301], Loss: 0.2955, Dev: 0.7630\n",
            "Epoch: [218/1000], Step: [300/301], Loss: 0.1701, Dev: 0.7690\n",
            "Epoch: [219/1000], Step: [100/301], Loss: 0.2913, Dev: 0.7740\n",
            "Epoch: [219/1000], Step: [200/301], Loss: 0.1256, Dev: 0.7480\n",
            "Epoch: [219/1000], Step: [300/301], Loss: 0.3535, Dev: 0.7780\n",
            "Epoch: [220/1000], Step: [100/301], Loss: 0.1367, Dev: 0.7600\n",
            "Epoch: [220/1000], Step: [200/301], Loss: 0.2023, Dev: 0.7600\n",
            "Epoch: [220/1000], Step: [300/301], Loss: 0.2056, Dev: 0.7790\n",
            "Epoch: [221/1000], Step: [100/301], Loss: 0.2485, Dev: 0.7790\n",
            "Epoch: [221/1000], Step: [200/301], Loss: 0.1779, Dev: 0.7700\n",
            "Epoch: [221/1000], Step: [300/301], Loss: 0.1729, Dev: 0.7470\n",
            "Epoch: [222/1000], Step: [100/301], Loss: 0.1105, Dev: 0.7570\n",
            "Epoch: [222/1000], Step: [200/301], Loss: 0.3044, Dev: 0.7640\n",
            "Epoch: [222/1000], Step: [300/301], Loss: 0.3243, Dev: 0.7790\n",
            "Epoch: [223/1000], Step: [100/301], Loss: 0.3915, Dev: 0.7680\n",
            "Epoch: [223/1000], Step: [200/301], Loss: 0.1788, Dev: 0.7720\n",
            "Epoch: [223/1000], Step: [300/301], Loss: 0.2437, Dev: 0.7560\n",
            "Epoch: [224/1000], Step: [100/301], Loss: 0.2082, Dev: 0.7580\n",
            "Epoch: [224/1000], Step: [200/301], Loss: 0.2150, Dev: 0.7700\n",
            "Epoch: [224/1000], Step: [300/301], Loss: 0.2311, Dev: 0.7650\n",
            "Epoch: [225/1000], Step: [100/301], Loss: 0.3391, Dev: 0.7730\n",
            "Epoch: [225/1000], Step: [200/301], Loss: 0.1723, Dev: 0.7740\n",
            "Epoch: [225/1000], Step: [300/301], Loss: 0.1717, Dev: 0.7650\n",
            "Epoch: [226/1000], Step: [100/301], Loss: 0.2870, Dev: 0.7470\n",
            "Epoch: [226/1000], Step: [200/301], Loss: 0.1919, Dev: 0.7790\n",
            "Epoch: [226/1000], Step: [300/301], Loss: 0.2349, Dev: 0.7650\n",
            "Epoch: [227/1000], Step: [100/301], Loss: 0.2000, Dev: 0.7720\n",
            "Epoch: [227/1000], Step: [200/301], Loss: 0.1143, Dev: 0.7590\n",
            "Epoch: [227/1000], Step: [300/301], Loss: 0.3361, Dev: 0.7700\n",
            "Epoch: [228/1000], Step: [100/301], Loss: 0.2471, Dev: 0.7640\n",
            "Epoch: [228/1000], Step: [200/301], Loss: 0.2652, Dev: 0.7680\n",
            "Epoch: [228/1000], Step: [300/301], Loss: 0.1862, Dev: 0.7600\n",
            "Epoch: [229/1000], Step: [100/301], Loss: 0.2499, Dev: 0.7570\n",
            "Epoch: [229/1000], Step: [200/301], Loss: 0.2154, Dev: 0.7630\n",
            "Epoch: [229/1000], Step: [300/301], Loss: 0.1886, Dev: 0.7720\n",
            "Epoch: [230/1000], Step: [100/301], Loss: 0.3781, Dev: 0.7680\n",
            "Epoch: [230/1000], Step: [200/301], Loss: 0.1237, Dev: 0.7740\n",
            "Epoch: [230/1000], Step: [300/301], Loss: 0.2920, Dev: 0.7520\n",
            "Epoch: [231/1000], Step: [100/301], Loss: 0.2653, Dev: 0.7790\n",
            "Epoch: [231/1000], Step: [200/301], Loss: 0.2310, Dev: 0.7720\n",
            "Epoch: [231/1000], Step: [300/301], Loss: 0.1561, Dev: 0.7640\n",
            "Epoch: [232/1000], Step: [100/301], Loss: 0.2736, Dev: 0.7740\n",
            "Epoch: [232/1000], Step: [200/301], Loss: 0.2147, Dev: 0.7760\n",
            "Epoch: [232/1000], Step: [300/301], Loss: 0.3204, Dev: 0.7650\n",
            "Epoch: [233/1000], Step: [100/301], Loss: 0.2162, Dev: 0.7500\n",
            "Epoch: [233/1000], Step: [200/301], Loss: 0.2718, Dev: 0.7620\n",
            "Epoch: [233/1000], Step: [300/301], Loss: 0.2033, Dev: 0.7690\n",
            "Epoch: [234/1000], Step: [100/301], Loss: 0.2148, Dev: 0.7540\n",
            "Epoch: [234/1000], Step: [200/301], Loss: 0.1927, Dev: 0.7640\n",
            "Epoch: [234/1000], Step: [300/301], Loss: 0.2558, Dev: 0.7890\n",
            "Epoch: [235/1000], Step: [100/301], Loss: 0.2116, Dev: 0.7850\n",
            "Epoch: [235/1000], Step: [200/301], Loss: 0.1925, Dev: 0.7630\n",
            "Epoch: [235/1000], Step: [300/301], Loss: 0.2047, Dev: 0.7320\n",
            "Epoch: [236/1000], Step: [100/301], Loss: 0.1615, Dev: 0.7730\n",
            "Epoch: [236/1000], Step: [200/301], Loss: 0.1970, Dev: 0.7650\n",
            "Epoch: [236/1000], Step: [300/301], Loss: 0.1579, Dev: 0.7530\n",
            "Epoch: [237/1000], Step: [100/301], Loss: 0.2565, Dev: 0.7630\n",
            "Epoch: [237/1000], Step: [200/301], Loss: 0.1946, Dev: 0.7510\n",
            "Epoch: [237/1000], Step: [300/301], Loss: 0.2714, Dev: 0.7710\n",
            "Epoch: [238/1000], Step: [100/301], Loss: 0.1439, Dev: 0.7760\n",
            "Epoch: [238/1000], Step: [200/301], Loss: 0.2231, Dev: 0.7620\n",
            "Epoch: [238/1000], Step: [300/301], Loss: 0.1907, Dev: 0.7830\n",
            "Epoch: [239/1000], Step: [100/301], Loss: 0.0890, Dev: 0.7550\n",
            "Epoch: [239/1000], Step: [200/301], Loss: 0.2749, Dev: 0.7570\n",
            "Epoch: [239/1000], Step: [300/301], Loss: 0.1845, Dev: 0.7780\n",
            "Epoch: [240/1000], Step: [100/301], Loss: 0.1513, Dev: 0.7770\n",
            "Epoch: [240/1000], Step: [200/301], Loss: 0.2828, Dev: 0.7610\n",
            "Epoch: [240/1000], Step: [300/301], Loss: 0.2118, Dev: 0.7690\n",
            "Epoch: [241/1000], Step: [100/301], Loss: 0.3292, Dev: 0.7520\n",
            "Epoch: [241/1000], Step: [200/301], Loss: 0.3609, Dev: 0.7700\n",
            "Epoch: [241/1000], Step: [300/301], Loss: 0.3389, Dev: 0.7570\n",
            "Epoch: [242/1000], Step: [100/301], Loss: 0.1187, Dev: 0.7710\n",
            "Epoch: [242/1000], Step: [200/301], Loss: 0.1730, Dev: 0.7570\n",
            "Epoch: [242/1000], Step: [300/301], Loss: 0.1779, Dev: 0.7790\n",
            "Epoch: [243/1000], Step: [100/301], Loss: 0.2346, Dev: 0.7670\n",
            "Epoch: [243/1000], Step: [200/301], Loss: 0.1722, Dev: 0.7460\n",
            "Epoch: [243/1000], Step: [300/301], Loss: 0.1865, Dev: 0.7770\n",
            "Epoch: [244/1000], Step: [100/301], Loss: 0.2755, Dev: 0.7550\n",
            "Epoch: [244/1000], Step: [200/301], Loss: 0.1872, Dev: 0.7680\n",
            "Epoch: [244/1000], Step: [300/301], Loss: 0.2506, Dev: 0.7590\n",
            "Epoch: [245/1000], Step: [100/301], Loss: 0.2765, Dev: 0.7590\n",
            "Epoch: [245/1000], Step: [200/301], Loss: 0.3286, Dev: 0.7710\n",
            "Epoch: [245/1000], Step: [300/301], Loss: 0.1976, Dev: 0.7610\n",
            "Epoch: [246/1000], Step: [100/301], Loss: 0.3022, Dev: 0.7590\n",
            "Epoch: [246/1000], Step: [200/301], Loss: 0.1778, Dev: 0.7750\n",
            "Epoch: [246/1000], Step: [300/301], Loss: 0.2296, Dev: 0.7830\n",
            "Epoch: [247/1000], Step: [100/301], Loss: 0.1622, Dev: 0.7450\n",
            "Epoch: [247/1000], Step: [200/301], Loss: 0.1566, Dev: 0.7770\n",
            "Epoch: [247/1000], Step: [300/301], Loss: 0.3494, Dev: 0.7810\n",
            "Epoch: [248/1000], Step: [100/301], Loss: 0.3064, Dev: 0.7700\n",
            "Epoch: [248/1000], Step: [200/301], Loss: 0.3111, Dev: 0.7720\n",
            "Epoch: [248/1000], Step: [300/301], Loss: 0.2238, Dev: 0.8020\n",
            "Epoch: [249/1000], Step: [100/301], Loss: 0.1705, Dev: 0.7490\n",
            "Epoch: [249/1000], Step: [200/301], Loss: 0.2700, Dev: 0.7430\n",
            "Epoch: [249/1000], Step: [300/301], Loss: 0.2073, Dev: 0.7770\n",
            "Epoch: [250/1000], Step: [100/301], Loss: 0.2005, Dev: 0.7790\n",
            "Epoch: [250/1000], Step: [200/301], Loss: 0.3513, Dev: 0.7430\n",
            "Epoch: [250/1000], Step: [300/301], Loss: 0.2364, Dev: 0.7710\n",
            "Epoch: [251/1000], Step: [100/301], Loss: 0.2634, Dev: 0.7530\n",
            "Epoch: [251/1000], Step: [200/301], Loss: 0.1857, Dev: 0.7480\n",
            "Epoch: [251/1000], Step: [300/301], Loss: 0.2333, Dev: 0.7770\n",
            "Epoch: [252/1000], Step: [100/301], Loss: 0.2113, Dev: 0.7780\n",
            "Epoch: [252/1000], Step: [200/301], Loss: 0.2299, Dev: 0.7940\n",
            "Epoch: [252/1000], Step: [300/301], Loss: 0.2504, Dev: 0.7710\n",
            "Epoch: [253/1000], Step: [100/301], Loss: 0.1929, Dev: 0.7460\n",
            "Epoch: [253/1000], Step: [200/301], Loss: 0.2043, Dev: 0.7530\n",
            "Epoch: [253/1000], Step: [300/301], Loss: 0.3089, Dev: 0.7820\n",
            "Epoch: [254/1000], Step: [100/301], Loss: 0.2914, Dev: 0.7720\n",
            "Epoch: [254/1000], Step: [200/301], Loss: 0.1451, Dev: 0.7230\n",
            "Epoch: [254/1000], Step: [300/301], Loss: 0.4189, Dev: 0.7600\n",
            "Epoch: [255/1000], Step: [100/301], Loss: 0.1958, Dev: 0.7610\n",
            "Epoch: [255/1000], Step: [200/301], Loss: 0.1797, Dev: 0.7630\n",
            "Epoch: [255/1000], Step: [300/301], Loss: 0.1921, Dev: 0.7510\n",
            "Epoch: [256/1000], Step: [100/301], Loss: 0.1447, Dev: 0.7270\n",
            "Epoch: [256/1000], Step: [200/301], Loss: 0.1682, Dev: 0.7690\n",
            "Epoch: [256/1000], Step: [300/301], Loss: 0.2718, Dev: 0.7700\n",
            "Epoch: [257/1000], Step: [100/301], Loss: 0.2175, Dev: 0.7520\n",
            "Epoch: [257/1000], Step: [200/301], Loss: 0.2182, Dev: 0.7670\n",
            "Epoch: [257/1000], Step: [300/301], Loss: 0.2035, Dev: 0.7740\n",
            "Epoch: [258/1000], Step: [100/301], Loss: 0.2103, Dev: 0.7780\n",
            "Epoch: [258/1000], Step: [200/301], Loss: 0.1524, Dev: 0.7750\n",
            "Epoch: [258/1000], Step: [300/301], Loss: 0.2046, Dev: 0.7510\n",
            "Epoch: [259/1000], Step: [100/301], Loss: 0.3472, Dev: 0.7460\n",
            "Epoch: [259/1000], Step: [200/301], Loss: 0.1668, Dev: 0.7450\n",
            "Epoch: [259/1000], Step: [300/301], Loss: 0.1598, Dev: 0.7820\n",
            "Epoch: [260/1000], Step: [100/301], Loss: 0.2040, Dev: 0.7440\n",
            "Epoch: [260/1000], Step: [200/301], Loss: 0.2117, Dev: 0.7590\n",
            "Epoch: [260/1000], Step: [300/301], Loss: 0.2518, Dev: 0.7870\n",
            "Epoch: [261/1000], Step: [100/301], Loss: 0.2061, Dev: 0.7570\n",
            "Epoch: [261/1000], Step: [200/301], Loss: 0.2944, Dev: 0.7490\n",
            "Epoch: [261/1000], Step: [300/301], Loss: 0.1727, Dev: 0.7790\n",
            "Epoch: [262/1000], Step: [100/301], Loss: 0.1761, Dev: 0.7820\n",
            "Epoch: [262/1000], Step: [200/301], Loss: 0.1410, Dev: 0.7720\n",
            "Epoch: [262/1000], Step: [300/301], Loss: 0.1579, Dev: 0.7580\n",
            "Epoch: [263/1000], Step: [100/301], Loss: 0.1923, Dev: 0.7580\n",
            "Epoch: [263/1000], Step: [200/301], Loss: 0.2366, Dev: 0.7870\n",
            "Epoch: [263/1000], Step: [300/301], Loss: 0.0859, Dev: 0.7830\n",
            "Epoch: [264/1000], Step: [100/301], Loss: 0.2577, Dev: 0.7540\n",
            "Epoch: [264/1000], Step: [200/301], Loss: 0.3357, Dev: 0.7890\n",
            "Epoch: [264/1000], Step: [300/301], Loss: 0.1508, Dev: 0.7510\n",
            "Epoch: [265/1000], Step: [100/301], Loss: 0.1369, Dev: 0.7630\n",
            "Epoch: [265/1000], Step: [200/301], Loss: 0.2236, Dev: 0.7620\n",
            "Epoch: [265/1000], Step: [300/301], Loss: 0.2720, Dev: 0.7440\n",
            "Epoch: [266/1000], Step: [100/301], Loss: 0.1016, Dev: 0.7850\n",
            "Epoch: [266/1000], Step: [200/301], Loss: 0.2158, Dev: 0.7470\n",
            "Epoch: [266/1000], Step: [300/301], Loss: 0.2286, Dev: 0.7650\n",
            "Epoch: [267/1000], Step: [100/301], Loss: 0.1914, Dev: 0.7470\n",
            "Epoch: [267/1000], Step: [200/301], Loss: 0.1696, Dev: 0.7470\n",
            "Epoch: [267/1000], Step: [300/301], Loss: 0.3139, Dev: 0.7610\n",
            "Epoch: [268/1000], Step: [100/301], Loss: 0.1645, Dev: 0.7570\n",
            "Epoch: [268/1000], Step: [200/301], Loss: 0.2323, Dev: 0.7530\n",
            "Epoch: [268/1000], Step: [300/301], Loss: 0.1738, Dev: 0.7730\n",
            "Epoch: [269/1000], Step: [100/301], Loss: 0.2626, Dev: 0.7530\n",
            "Epoch: [269/1000], Step: [200/301], Loss: 0.1824, Dev: 0.7700\n",
            "Epoch: [269/1000], Step: [300/301], Loss: 0.1580, Dev: 0.7640\n",
            "Epoch: [270/1000], Step: [100/301], Loss: 0.2960, Dev: 0.7750\n",
            "Epoch: [270/1000], Step: [200/301], Loss: 0.1514, Dev: 0.7650\n",
            "Epoch: [270/1000], Step: [300/301], Loss: 0.1943, Dev: 0.7470\n",
            "Epoch: [271/1000], Step: [100/301], Loss: 0.1307, Dev: 0.7620\n",
            "Epoch: [271/1000], Step: [200/301], Loss: 0.2095, Dev: 0.7810\n",
            "Epoch: [271/1000], Step: [300/301], Loss: 0.1528, Dev: 0.7600\n",
            "Epoch: [272/1000], Step: [100/301], Loss: 0.1913, Dev: 0.7610\n",
            "Epoch: [272/1000], Step: [200/301], Loss: 0.1838, Dev: 0.7470\n",
            "Epoch: [272/1000], Step: [300/301], Loss: 0.2741, Dev: 0.7650\n",
            "Epoch: [273/1000], Step: [100/301], Loss: 0.1461, Dev: 0.7600\n",
            "Epoch: [273/1000], Step: [200/301], Loss: 0.1422, Dev: 0.7560\n",
            "Epoch: [273/1000], Step: [300/301], Loss: 0.2104, Dev: 0.7770\n",
            "Epoch: [274/1000], Step: [100/301], Loss: 0.2744, Dev: 0.7620\n",
            "Epoch: [274/1000], Step: [200/301], Loss: 0.2088, Dev: 0.7610\n",
            "Epoch: [274/1000], Step: [300/301], Loss: 0.3625, Dev: 0.7760\n",
            "Epoch: [275/1000], Step: [100/301], Loss: 0.2706, Dev: 0.7820\n",
            "Epoch: [275/1000], Step: [200/301], Loss: 0.1711, Dev: 0.7650\n",
            "Epoch: [275/1000], Step: [300/301], Loss: 0.1600, Dev: 0.7690\n",
            "Epoch: [276/1000], Step: [100/301], Loss: 0.2262, Dev: 0.7660\n",
            "Epoch: [276/1000], Step: [200/301], Loss: 0.1715, Dev: 0.7680\n",
            "Epoch: [276/1000], Step: [300/301], Loss: 0.2444, Dev: 0.7570\n",
            "Epoch: [277/1000], Step: [100/301], Loss: 0.1248, Dev: 0.7650\n",
            "Epoch: [277/1000], Step: [200/301], Loss: 0.1283, Dev: 0.7530\n",
            "Epoch: [277/1000], Step: [300/301], Loss: 0.2353, Dev: 0.7590\n",
            "Epoch: [278/1000], Step: [100/301], Loss: 0.1210, Dev: 0.7660\n",
            "Epoch: [278/1000], Step: [200/301], Loss: 0.3266, Dev: 0.7490\n",
            "Epoch: [278/1000], Step: [300/301], Loss: 0.1479, Dev: 0.7620\n",
            "Epoch: [279/1000], Step: [100/301], Loss: 0.1396, Dev: 0.7580\n",
            "Epoch: [279/1000], Step: [200/301], Loss: 0.2423, Dev: 0.7700\n",
            "Epoch: [279/1000], Step: [300/301], Loss: 0.2451, Dev: 0.7670\n",
            "Epoch: [280/1000], Step: [100/301], Loss: 0.1421, Dev: 0.7590\n",
            "Epoch: [280/1000], Step: [200/301], Loss: 0.2422, Dev: 0.7770\n",
            "Epoch: [280/1000], Step: [300/301], Loss: 0.2837, Dev: 0.7800\n",
            "Epoch: [281/1000], Step: [100/301], Loss: 0.2355, Dev: 0.7620\n",
            "Epoch: [281/1000], Step: [200/301], Loss: 0.0948, Dev: 0.7570\n",
            "Epoch: [281/1000], Step: [300/301], Loss: 0.1994, Dev: 0.7510\n",
            "Epoch: [282/1000], Step: [100/301], Loss: 0.1702, Dev: 0.7710\n",
            "Epoch: [282/1000], Step: [200/301], Loss: 0.1874, Dev: 0.7580\n",
            "Epoch: [282/1000], Step: [300/301], Loss: 0.2540, Dev: 0.7710\n",
            "Epoch: [283/1000], Step: [100/301], Loss: 0.3248, Dev: 0.7740\n",
            "Epoch: [283/1000], Step: [200/301], Loss: 0.1561, Dev: 0.7740\n",
            "Epoch: [283/1000], Step: [300/301], Loss: 0.1786, Dev: 0.7560\n",
            "Epoch: [284/1000], Step: [100/301], Loss: 0.3070, Dev: 0.7680\n",
            "Epoch: [284/1000], Step: [200/301], Loss: 0.0880, Dev: 0.7800\n",
            "Epoch: [284/1000], Step: [300/301], Loss: 0.2682, Dev: 0.7530\n",
            "Epoch: [285/1000], Step: [100/301], Loss: 0.1329, Dev: 0.7820\n",
            "Epoch: [285/1000], Step: [200/301], Loss: 0.2815, Dev: 0.7600\n",
            "Epoch: [285/1000], Step: [300/301], Loss: 0.1700, Dev: 0.7880\n",
            "Epoch: [286/1000], Step: [100/301], Loss: 0.2437, Dev: 0.7680\n",
            "Epoch: [286/1000], Step: [200/301], Loss: 0.2045, Dev: 0.7480\n",
            "Epoch: [286/1000], Step: [300/301], Loss: 0.1770, Dev: 0.7710\n",
            "Epoch: [287/1000], Step: [100/301], Loss: 0.2467, Dev: 0.7710\n",
            "Epoch: [287/1000], Step: [200/301], Loss: 0.2590, Dev: 0.7560\n",
            "Epoch: [287/1000], Step: [300/301], Loss: 0.2954, Dev: 0.7690\n",
            "Epoch: [288/1000], Step: [100/301], Loss: 0.2207, Dev: 0.7670\n",
            "Epoch: [288/1000], Step: [200/301], Loss: 0.2707, Dev: 0.7650\n",
            "Epoch: [288/1000], Step: [300/301], Loss: 0.1704, Dev: 0.7700\n",
            "Epoch: [289/1000], Step: [100/301], Loss: 0.2739, Dev: 0.7700\n",
            "Epoch: [289/1000], Step: [200/301], Loss: 0.2253, Dev: 0.7600\n",
            "Epoch: [289/1000], Step: [300/301], Loss: 0.1749, Dev: 0.7780\n",
            "Epoch: [290/1000], Step: [100/301], Loss: 0.2899, Dev: 0.7720\n",
            "Epoch: [290/1000], Step: [200/301], Loss: 0.1332, Dev: 0.7680\n",
            "Epoch: [290/1000], Step: [300/301], Loss: 0.1137, Dev: 0.7650\n",
            "Epoch: [291/1000], Step: [100/301], Loss: 0.2952, Dev: 0.7680\n",
            "Epoch: [291/1000], Step: [200/301], Loss: 0.0874, Dev: 0.7650\n",
            "Epoch: [291/1000], Step: [300/301], Loss: 0.2479, Dev: 0.7800\n",
            "Epoch: [292/1000], Step: [100/301], Loss: 0.2777, Dev: 0.7630\n",
            "Epoch: [292/1000], Step: [200/301], Loss: 0.3242, Dev: 0.7770\n",
            "Epoch: [292/1000], Step: [300/301], Loss: 0.2359, Dev: 0.7480\n",
            "Epoch: [293/1000], Step: [100/301], Loss: 0.2006, Dev: 0.7750\n",
            "Epoch: [293/1000], Step: [200/301], Loss: 0.1620, Dev: 0.7670\n",
            "Epoch: [293/1000], Step: [300/301], Loss: 0.1877, Dev: 0.7610\n",
            "Epoch: [294/1000], Step: [100/301], Loss: 0.2788, Dev: 0.7560\n",
            "Epoch: [294/1000], Step: [200/301], Loss: 0.1447, Dev: 0.7980\n",
            "Epoch: [294/1000], Step: [300/301], Loss: 0.2176, Dev: 0.7440\n",
            "Epoch: [295/1000], Step: [100/301], Loss: 0.1653, Dev: 0.7660\n",
            "Epoch: [295/1000], Step: [200/301], Loss: 0.1474, Dev: 0.7510\n",
            "Epoch: [295/1000], Step: [300/301], Loss: 0.2428, Dev: 0.7720\n",
            "Epoch: [296/1000], Step: [100/301], Loss: 0.1910, Dev: 0.7690\n",
            "Epoch: [296/1000], Step: [200/301], Loss: 0.2616, Dev: 0.7530\n",
            "Epoch: [296/1000], Step: [300/301], Loss: 0.1965, Dev: 0.7750\n",
            "Epoch: [297/1000], Step: [100/301], Loss: 0.2925, Dev: 0.7700\n",
            "Epoch: [297/1000], Step: [200/301], Loss: 0.2143, Dev: 0.7500\n",
            "Epoch: [297/1000], Step: [300/301], Loss: 0.3129, Dev: 0.7610\n",
            "Epoch: [298/1000], Step: [100/301], Loss: 0.1501, Dev: 0.7600\n",
            "Epoch: [298/1000], Step: [200/301], Loss: 0.3051, Dev: 0.7750\n",
            "Epoch: [298/1000], Step: [300/301], Loss: 0.1489, Dev: 0.7700\n",
            "Epoch: [299/1000], Step: [100/301], Loss: 0.2144, Dev: 0.7720\n",
            "Epoch: [299/1000], Step: [200/301], Loss: 0.2247, Dev: 0.7670\n",
            "Epoch: [299/1000], Step: [300/301], Loss: 0.1501, Dev: 0.7630\n",
            "Epoch: [300/1000], Step: [100/301], Loss: 0.1966, Dev: 0.7620\n",
            "Epoch: [300/1000], Step: [200/301], Loss: 0.2527, Dev: 0.7740\n",
            "Epoch: [300/1000], Step: [300/301], Loss: 0.1460, Dev: 0.7660\n",
            "Epoch: [301/1000], Step: [100/301], Loss: 0.2113, Dev: 0.7540\n",
            "Epoch: [301/1000], Step: [200/301], Loss: 0.3420, Dev: 0.7610\n",
            "Epoch: [301/1000], Step: [300/301], Loss: 0.2657, Dev: 0.7570\n",
            "Epoch: [302/1000], Step: [100/301], Loss: 0.2832, Dev: 0.7650\n",
            "Epoch: [302/1000], Step: [200/301], Loss: 0.2102, Dev: 0.7660\n",
            "Epoch: [302/1000], Step: [300/301], Loss: 0.2959, Dev: 0.7660\n",
            "Epoch: [303/1000], Step: [100/301], Loss: 0.1931, Dev: 0.7760\n",
            "Epoch: [303/1000], Step: [200/301], Loss: 0.2674, Dev: 0.7700\n",
            "Epoch: [303/1000], Step: [300/301], Loss: 0.2457, Dev: 0.7680\n",
            "Epoch: [304/1000], Step: [100/301], Loss: 0.2406, Dev: 0.7650\n",
            "Epoch: [304/1000], Step: [200/301], Loss: 0.1983, Dev: 0.7870\n",
            "Epoch: [304/1000], Step: [300/301], Loss: 0.2802, Dev: 0.7670\n",
            "Epoch: [305/1000], Step: [100/301], Loss: 0.3480, Dev: 0.7510\n",
            "Epoch: [305/1000], Step: [200/301], Loss: 0.2095, Dev: 0.7670\n",
            "Epoch: [305/1000], Step: [300/301], Loss: 0.1070, Dev: 0.7710\n",
            "Epoch: [306/1000], Step: [100/301], Loss: 0.2823, Dev: 0.7800\n",
            "Epoch: [306/1000], Step: [200/301], Loss: 0.2198, Dev: 0.7600\n",
            "Epoch: [306/1000], Step: [300/301], Loss: 0.2727, Dev: 0.7840\n",
            "Epoch: [307/1000], Step: [100/301], Loss: 0.3108, Dev: 0.7590\n",
            "Epoch: [307/1000], Step: [200/301], Loss: 0.2849, Dev: 0.7590\n",
            "Epoch: [307/1000], Step: [300/301], Loss: 0.1770, Dev: 0.7720\n",
            "Epoch: [308/1000], Step: [100/301], Loss: 0.2266, Dev: 0.7680\n",
            "Epoch: [308/1000], Step: [200/301], Loss: 0.2546, Dev: 0.7560\n",
            "Epoch: [308/1000], Step: [300/301], Loss: 0.1112, Dev: 0.7700\n",
            "Epoch: [309/1000], Step: [100/301], Loss: 0.2181, Dev: 0.7670\n",
            "Epoch: [309/1000], Step: [200/301], Loss: 0.2841, Dev: 0.7450\n",
            "Epoch: [309/1000], Step: [300/301], Loss: 0.1058, Dev: 0.7670\n",
            "Epoch: [310/1000], Step: [100/301], Loss: 0.1854, Dev: 0.7560\n",
            "Epoch: [310/1000], Step: [200/301], Loss: 0.2809, Dev: 0.7570\n",
            "Epoch: [310/1000], Step: [300/301], Loss: 0.3120, Dev: 0.7650\n",
            "Epoch: [311/1000], Step: [100/301], Loss: 0.1843, Dev: 0.7630\n",
            "Epoch: [311/1000], Step: [200/301], Loss: 0.1704, Dev: 0.7590\n",
            "Epoch: [311/1000], Step: [300/301], Loss: 0.2136, Dev: 0.7630\n",
            "Epoch: [312/1000], Step: [100/301], Loss: 0.2335, Dev: 0.7750\n",
            "Epoch: [312/1000], Step: [200/301], Loss: 0.2768, Dev: 0.7800\n",
            "Epoch: [312/1000], Step: [300/301], Loss: 0.2459, Dev: 0.7730\n",
            "Epoch: [313/1000], Step: [100/301], Loss: 0.1449, Dev: 0.7670\n",
            "Epoch: [313/1000], Step: [200/301], Loss: 0.1825, Dev: 0.7670\n",
            "Epoch: [313/1000], Step: [300/301], Loss: 0.2394, Dev: 0.7660\n",
            "Epoch: [314/1000], Step: [100/301], Loss: 0.2497, Dev: 0.7440\n",
            "Epoch: [314/1000], Step: [200/301], Loss: 0.2501, Dev: 0.7590\n",
            "Epoch: [314/1000], Step: [300/301], Loss: 0.2622, Dev: 0.7640\n",
            "Epoch: [315/1000], Step: [100/301], Loss: 0.2057, Dev: 0.7460\n",
            "Epoch: [315/1000], Step: [200/301], Loss: 0.3309, Dev: 0.7580\n",
            "Epoch: [315/1000], Step: [300/301], Loss: 0.2896, Dev: 0.7720\n",
            "Epoch: [316/1000], Step: [100/301], Loss: 0.1673, Dev: 0.7740\n",
            "Epoch: [316/1000], Step: [200/301], Loss: 0.3492, Dev: 0.7680\n",
            "Epoch: [316/1000], Step: [300/301], Loss: 0.3123, Dev: 0.7650\n",
            "Epoch: [317/1000], Step: [100/301], Loss: 0.2497, Dev: 0.7750\n",
            "Epoch: [317/1000], Step: [200/301], Loss: 0.3390, Dev: 0.7550\n",
            "Epoch: [317/1000], Step: [300/301], Loss: 0.2725, Dev: 0.7520\n",
            "Epoch: [318/1000], Step: [100/301], Loss: 0.1227, Dev: 0.7570\n",
            "Epoch: [318/1000], Step: [200/301], Loss: 0.1604, Dev: 0.7690\n",
            "Epoch: [318/1000], Step: [300/301], Loss: 0.2137, Dev: 0.7660\n",
            "Epoch: [319/1000], Step: [100/301], Loss: 0.1615, Dev: 0.7490\n",
            "Epoch: [319/1000], Step: [200/301], Loss: 0.1726, Dev: 0.7520\n",
            "Epoch: [319/1000], Step: [300/301], Loss: 0.1554, Dev: 0.7580\n",
            "Epoch: [320/1000], Step: [100/301], Loss: 0.1689, Dev: 0.7660\n",
            "Epoch: [320/1000], Step: [200/301], Loss: 0.1966, Dev: 0.7590\n",
            "Epoch: [320/1000], Step: [300/301], Loss: 0.1430, Dev: 0.7590\n",
            "Epoch: [321/1000], Step: [100/301], Loss: 0.1752, Dev: 0.7620\n",
            "Epoch: [321/1000], Step: [200/301], Loss: 0.2290, Dev: 0.7770\n",
            "Epoch: [321/1000], Step: [300/301], Loss: 0.1293, Dev: 0.7610\n",
            "Epoch: [322/1000], Step: [100/301], Loss: 0.3221, Dev: 0.7550\n",
            "Epoch: [322/1000], Step: [200/301], Loss: 0.1688, Dev: 0.7610\n",
            "Epoch: [322/1000], Step: [300/301], Loss: 0.2472, Dev: 0.7490\n",
            "Epoch: [323/1000], Step: [100/301], Loss: 0.1957, Dev: 0.7580\n",
            "Epoch: [323/1000], Step: [200/301], Loss: 0.2210, Dev: 0.7690\n",
            "Epoch: [323/1000], Step: [300/301], Loss: 0.3284, Dev: 0.7570\n",
            "Epoch: [324/1000], Step: [100/301], Loss: 0.0979, Dev: 0.7460\n",
            "Epoch: [324/1000], Step: [200/301], Loss: 0.1498, Dev: 0.7460\n",
            "Epoch: [324/1000], Step: [300/301], Loss: 0.1595, Dev: 0.7640\n",
            "Epoch: [325/1000], Step: [100/301], Loss: 0.1759, Dev: 0.7690\n",
            "Epoch: [325/1000], Step: [200/301], Loss: 0.1534, Dev: 0.7610\n",
            "Epoch: [325/1000], Step: [300/301], Loss: 0.1742, Dev: 0.7500\n",
            "Epoch: [326/1000], Step: [100/301], Loss: 0.1766, Dev: 0.7660\n",
            "Epoch: [326/1000], Step: [200/301], Loss: 0.2353, Dev: 0.7530\n",
            "Epoch: [326/1000], Step: [300/301], Loss: 0.2669, Dev: 0.7630\n",
            "Epoch: [327/1000], Step: [100/301], Loss: 0.3824, Dev: 0.7620\n",
            "Epoch: [327/1000], Step: [200/301], Loss: 0.1580, Dev: 0.7620\n",
            "Epoch: [327/1000], Step: [300/301], Loss: 0.1852, Dev: 0.7550\n",
            "Epoch: [328/1000], Step: [100/301], Loss: 0.2577, Dev: 0.7820\n",
            "Epoch: [328/1000], Step: [200/301], Loss: 0.0800, Dev: 0.7570\n",
            "Epoch: [328/1000], Step: [300/301], Loss: 0.2183, Dev: 0.7600\n",
            "Epoch: [329/1000], Step: [100/301], Loss: 0.2095, Dev: 0.7590\n",
            "Epoch: [329/1000], Step: [200/301], Loss: 0.1629, Dev: 0.7680\n",
            "Epoch: [329/1000], Step: [300/301], Loss: 0.1811, Dev: 0.7660\n",
            "Epoch: [330/1000], Step: [100/301], Loss: 0.2016, Dev: 0.7710\n",
            "Epoch: [330/1000], Step: [200/301], Loss: 0.2677, Dev: 0.7630\n",
            "Epoch: [330/1000], Step: [300/301], Loss: 0.1911, Dev: 0.7460\n",
            "Epoch: [331/1000], Step: [100/301], Loss: 0.1509, Dev: 0.7670\n",
            "Epoch: [331/1000], Step: [200/301], Loss: 0.2295, Dev: 0.7560\n",
            "Epoch: [331/1000], Step: [300/301], Loss: 0.2365, Dev: 0.7670\n",
            "Epoch: [332/1000], Step: [100/301], Loss: 0.1383, Dev: 0.7500\n",
            "Epoch: [332/1000], Step: [200/301], Loss: 0.1353, Dev: 0.7700\n",
            "Epoch: [332/1000], Step: [300/301], Loss: 0.2550, Dev: 0.7820\n",
            "Epoch: [333/1000], Step: [100/301], Loss: 0.3000, Dev: 0.7520\n",
            "Epoch: [333/1000], Step: [200/301], Loss: 0.1635, Dev: 0.7670\n",
            "Epoch: [333/1000], Step: [300/301], Loss: 0.2449, Dev: 0.7530\n",
            "Epoch: [334/1000], Step: [100/301], Loss: 0.1567, Dev: 0.7560\n",
            "Epoch: [334/1000], Step: [200/301], Loss: 0.2738, Dev: 0.7550\n",
            "Epoch: [334/1000], Step: [300/301], Loss: 0.1352, Dev: 0.7530\n",
            "Epoch: [335/1000], Step: [100/301], Loss: 0.2144, Dev: 0.7730\n",
            "Epoch: [335/1000], Step: [200/301], Loss: 0.2946, Dev: 0.7550\n",
            "Epoch: [335/1000], Step: [300/301], Loss: 0.3194, Dev: 0.7670\n",
            "Epoch: [336/1000], Step: [100/301], Loss: 0.2553, Dev: 0.7480\n",
            "Epoch: [336/1000], Step: [200/301], Loss: 0.3263, Dev: 0.7580\n",
            "Epoch: [336/1000], Step: [300/301], Loss: 0.2395, Dev: 0.7710\n",
            "Epoch: [337/1000], Step: [100/301], Loss: 0.3032, Dev: 0.7600\n",
            "Epoch: [337/1000], Step: [200/301], Loss: 0.1959, Dev: 0.7630\n",
            "Epoch: [337/1000], Step: [300/301], Loss: 0.1846, Dev: 0.7680\n",
            "Epoch: [338/1000], Step: [100/301], Loss: 0.2448, Dev: 0.7540\n",
            "Epoch: [338/1000], Step: [200/301], Loss: 0.1481, Dev: 0.7510\n",
            "Epoch: [338/1000], Step: [300/301], Loss: 0.1386, Dev: 0.7520\n",
            "Epoch: [339/1000], Step: [100/301], Loss: 0.3990, Dev: 0.7570\n",
            "Epoch: [339/1000], Step: [200/301], Loss: 0.1656, Dev: 0.7500\n",
            "Epoch: [339/1000], Step: [300/301], Loss: 0.2171, Dev: 0.7630\n",
            "Epoch: [340/1000], Step: [100/301], Loss: 0.2555, Dev: 0.7570\n",
            "Epoch: [340/1000], Step: [200/301], Loss: 0.1688, Dev: 0.7600\n",
            "Epoch: [340/1000], Step: [300/301], Loss: 0.1963, Dev: 0.7610\n",
            "Epoch: [341/1000], Step: [100/301], Loss: 0.2107, Dev: 0.7470\n",
            "Epoch: [341/1000], Step: [200/301], Loss: 0.1710, Dev: 0.7530\n",
            "Epoch: [341/1000], Step: [300/301], Loss: 0.1639, Dev: 0.7480\n",
            "Epoch: [342/1000], Step: [100/301], Loss: 0.2206, Dev: 0.7680\n",
            "Epoch: [342/1000], Step: [200/301], Loss: 0.2192, Dev: 0.7660\n",
            "Epoch: [342/1000], Step: [300/301], Loss: 0.3747, Dev: 0.7660\n",
            "Epoch: [343/1000], Step: [100/301], Loss: 0.1806, Dev: 0.7610\n",
            "Epoch: [343/1000], Step: [200/301], Loss: 0.1617, Dev: 0.7640\n",
            "Epoch: [343/1000], Step: [300/301], Loss: 0.2477, Dev: 0.7670\n",
            "Epoch: [344/1000], Step: [100/301], Loss: 0.2238, Dev: 0.7730\n",
            "Epoch: [344/1000], Step: [200/301], Loss: 0.2840, Dev: 0.7510\n",
            "Epoch: [344/1000], Step: [300/301], Loss: 0.2159, Dev: 0.7660\n",
            "Epoch: [345/1000], Step: [100/301], Loss: 0.4064, Dev: 0.7540\n",
            "Epoch: [345/1000], Step: [200/301], Loss: 0.2748, Dev: 0.7730\n",
            "Epoch: [345/1000], Step: [300/301], Loss: 0.2480, Dev: 0.7660\n",
            "Epoch: [346/1000], Step: [100/301], Loss: 0.2105, Dev: 0.7590\n",
            "Epoch: [346/1000], Step: [200/301], Loss: 0.1465, Dev: 0.7660\n",
            "Epoch: [346/1000], Step: [300/301], Loss: 0.1765, Dev: 0.7540\n",
            "Epoch: [347/1000], Step: [100/301], Loss: 0.1119, Dev: 0.7520\n",
            "Epoch: [347/1000], Step: [200/301], Loss: 0.1662, Dev: 0.7670\n",
            "Epoch: [347/1000], Step: [300/301], Loss: 0.3067, Dev: 0.7640\n",
            "Epoch: [348/1000], Step: [100/301], Loss: 0.3630, Dev: 0.7750\n",
            "Epoch: [348/1000], Step: [200/301], Loss: 0.2319, Dev: 0.7540\n",
            "Epoch: [348/1000], Step: [300/301], Loss: 0.2032, Dev: 0.7600\n",
            "Epoch: [349/1000], Step: [100/301], Loss: 0.1799, Dev: 0.7620\n",
            "Epoch: [349/1000], Step: [200/301], Loss: 0.1581, Dev: 0.7580\n",
            "Epoch: [349/1000], Step: [300/301], Loss: 0.1767, Dev: 0.7640\n",
            "Epoch: [350/1000], Step: [100/301], Loss: 0.1376, Dev: 0.7590\n",
            "Epoch: [350/1000], Step: [200/301], Loss: 0.2655, Dev: 0.7580\n",
            "Epoch: [350/1000], Step: [300/301], Loss: 0.2174, Dev: 0.7480\n",
            "Epoch: [351/1000], Step: [100/301], Loss: 0.1600, Dev: 0.7590\n",
            "Epoch: [351/1000], Step: [200/301], Loss: 0.2424, Dev: 0.7550\n",
            "Epoch: [351/1000], Step: [300/301], Loss: 0.2451, Dev: 0.7550\n",
            "Epoch: [352/1000], Step: [100/301], Loss: 0.1149, Dev: 0.7620\n",
            "Epoch: [352/1000], Step: [200/301], Loss: 0.2418, Dev: 0.7740\n",
            "Epoch: [352/1000], Step: [300/301], Loss: 0.2137, Dev: 0.7540\n",
            "Epoch: [353/1000], Step: [100/301], Loss: 0.2533, Dev: 0.7680\n",
            "Epoch: [353/1000], Step: [200/301], Loss: 0.2284, Dev: 0.7470\n",
            "Epoch: [353/1000], Step: [300/301], Loss: 0.2643, Dev: 0.7620\n",
            "Epoch: [354/1000], Step: [100/301], Loss: 0.1535, Dev: 0.7600\n",
            "Epoch: [354/1000], Step: [200/301], Loss: 0.1434, Dev: 0.7510\n",
            "Epoch: [354/1000], Step: [300/301], Loss: 0.3363, Dev: 0.7660\n",
            "Epoch: [355/1000], Step: [100/301], Loss: 0.2627, Dev: 0.7550\n",
            "Epoch: [355/1000], Step: [200/301], Loss: 0.3469, Dev: 0.7650\n",
            "Epoch: [355/1000], Step: [300/301], Loss: 0.1344, Dev: 0.7740\n",
            "Epoch: [356/1000], Step: [100/301], Loss: 0.1533, Dev: 0.7430\n",
            "Epoch: [356/1000], Step: [200/301], Loss: 0.3774, Dev: 0.7470\n",
            "Epoch: [356/1000], Step: [300/301], Loss: 0.1921, Dev: 0.7540\n",
            "Epoch: [357/1000], Step: [100/301], Loss: 0.2112, Dev: 0.7740\n",
            "Epoch: [357/1000], Step: [200/301], Loss: 0.2970, Dev: 0.7630\n",
            "Epoch: [357/1000], Step: [300/301], Loss: 0.1545, Dev: 0.7560\n",
            "Epoch: [358/1000], Step: [100/301], Loss: 0.1106, Dev: 0.7450\n",
            "Epoch: [358/1000], Step: [200/301], Loss: 0.2307, Dev: 0.7910\n",
            "Epoch: [358/1000], Step: [300/301], Loss: 0.1783, Dev: 0.7600\n",
            "Epoch: [359/1000], Step: [100/301], Loss: 0.3196, Dev: 0.7530\n",
            "Epoch: [359/1000], Step: [200/301], Loss: 0.1517, Dev: 0.7660\n",
            "Epoch: [359/1000], Step: [300/301], Loss: 0.1685, Dev: 0.7650\n",
            "Epoch: [360/1000], Step: [100/301], Loss: 0.1308, Dev: 0.7590\n",
            "Epoch: [360/1000], Step: [200/301], Loss: 0.3509, Dev: 0.7700\n",
            "Epoch: [360/1000], Step: [300/301], Loss: 0.2112, Dev: 0.7460\n",
            "Epoch: [361/1000], Step: [100/301], Loss: 0.2242, Dev: 0.7470\n",
            "Epoch: [361/1000], Step: [200/301], Loss: 0.1660, Dev: 0.7730\n",
            "Epoch: [361/1000], Step: [300/301], Loss: 0.3071, Dev: 0.7620\n",
            "Epoch: [362/1000], Step: [100/301], Loss: 0.1171, Dev: 0.7510\n",
            "Epoch: [362/1000], Step: [200/301], Loss: 0.2922, Dev: 0.7650\n",
            "Epoch: [362/1000], Step: [300/301], Loss: 0.1675, Dev: 0.7460\n",
            "Epoch: [363/1000], Step: [100/301], Loss: 0.2253, Dev: 0.7550\n",
            "Epoch: [363/1000], Step: [200/301], Loss: 0.1242, Dev: 0.7590\n",
            "Epoch: [363/1000], Step: [300/301], Loss: 0.2678, Dev: 0.7490\n",
            "Epoch: [364/1000], Step: [100/301], Loss: 0.1537, Dev: 0.7670\n",
            "Epoch: [364/1000], Step: [200/301], Loss: 0.1613, Dev: 0.7580\n",
            "Epoch: [364/1000], Step: [300/301], Loss: 0.2894, Dev: 0.7630\n",
            "Epoch: [365/1000], Step: [100/301], Loss: 0.2239, Dev: 0.7650\n",
            "Epoch: [365/1000], Step: [200/301], Loss: 0.1973, Dev: 0.7560\n",
            "Epoch: [365/1000], Step: [300/301], Loss: 0.1676, Dev: 0.7660\n",
            "Epoch: [366/1000], Step: [100/301], Loss: 0.3151, Dev: 0.7500\n",
            "Epoch: [366/1000], Step: [200/301], Loss: 0.3799, Dev: 0.7460\n",
            "Epoch: [366/1000], Step: [300/301], Loss: 0.2713, Dev: 0.7310\n",
            "Epoch: [367/1000], Step: [100/301], Loss: 0.2189, Dev: 0.7500\n",
            "Epoch: [367/1000], Step: [200/301], Loss: 0.2470, Dev: 0.7590\n",
            "Epoch: [367/1000], Step: [300/301], Loss: 0.1284, Dev: 0.7560\n",
            "Epoch: [368/1000], Step: [100/301], Loss: 0.2118, Dev: 0.7650\n",
            "Epoch: [368/1000], Step: [200/301], Loss: 0.1524, Dev: 0.7450\n",
            "Epoch: [368/1000], Step: [300/301], Loss: 0.2208, Dev: 0.7590\n",
            "Epoch: [369/1000], Step: [100/301], Loss: 0.1628, Dev: 0.7670\n",
            "Epoch: [369/1000], Step: [200/301], Loss: 0.2129, Dev: 0.7600\n",
            "Epoch: [369/1000], Step: [300/301], Loss: 0.2269, Dev: 0.7660\n",
            "Epoch: [370/1000], Step: [100/301], Loss: 0.2622, Dev: 0.7530\n",
            "Epoch: [370/1000], Step: [200/301], Loss: 0.1359, Dev: 0.7450\n",
            "Epoch: [370/1000], Step: [300/301], Loss: 0.1816, Dev: 0.7500\n",
            "Epoch: [371/1000], Step: [100/301], Loss: 0.2344, Dev: 0.7690\n",
            "Epoch: [371/1000], Step: [200/301], Loss: 0.1784, Dev: 0.7580\n",
            "Epoch: [371/1000], Step: [300/301], Loss: 0.2210, Dev: 0.7530\n",
            "Epoch: [372/1000], Step: [100/301], Loss: 0.3203, Dev: 0.7550\n",
            "Epoch: [372/1000], Step: [200/301], Loss: 0.2265, Dev: 0.7640\n",
            "Epoch: [372/1000], Step: [300/301], Loss: 0.2883, Dev: 0.7540\n",
            "Epoch: [373/1000], Step: [100/301], Loss: 0.1361, Dev: 0.7560\n",
            "Epoch: [373/1000], Step: [200/301], Loss: 0.1867, Dev: 0.7810\n",
            "Epoch: [373/1000], Step: [300/301], Loss: 0.1134, Dev: 0.7420\n",
            "Epoch: [374/1000], Step: [100/301], Loss: 0.1525, Dev: 0.7660\n",
            "Epoch: [374/1000], Step: [200/301], Loss: 0.1912, Dev: 0.7460\n",
            "Epoch: [374/1000], Step: [300/301], Loss: 0.1394, Dev: 0.7650\n",
            "Epoch: [375/1000], Step: [100/301], Loss: 0.0885, Dev: 0.7540\n",
            "Epoch: [375/1000], Step: [200/301], Loss: 0.2524, Dev: 0.7420\n",
            "Epoch: [375/1000], Step: [300/301], Loss: 0.1282, Dev: 0.7510\n",
            "Epoch: [376/1000], Step: [100/301], Loss: 0.1352, Dev: 0.7500\n",
            "Epoch: [376/1000], Step: [200/301], Loss: 0.1459, Dev: 0.7620\n",
            "Epoch: [376/1000], Step: [300/301], Loss: 0.1974, Dev: 0.7530\n",
            "Epoch: [377/1000], Step: [100/301], Loss: 0.1383, Dev: 0.7460\n",
            "Epoch: [377/1000], Step: [200/301], Loss: 0.2549, Dev: 0.7580\n",
            "Epoch: [377/1000], Step: [300/301], Loss: 0.2724, Dev: 0.7600\n",
            "Epoch: [378/1000], Step: [100/301], Loss: 0.2011, Dev: 0.7670\n",
            "Epoch: [378/1000], Step: [200/301], Loss: 0.2556, Dev: 0.7600\n",
            "Epoch: [378/1000], Step: [300/301], Loss: 0.2108, Dev: 0.7550\n",
            "Epoch: [379/1000], Step: [100/301], Loss: 0.2460, Dev: 0.7630\n",
            "Epoch: [379/1000], Step: [200/301], Loss: 0.1838, Dev: 0.7480\n",
            "Epoch: [379/1000], Step: [300/301], Loss: 0.1399, Dev: 0.7460\n",
            "Epoch: [380/1000], Step: [100/301], Loss: 0.2393, Dev: 0.7710\n",
            "Epoch: [380/1000], Step: [200/301], Loss: 0.2137, Dev: 0.7610\n",
            "Epoch: [380/1000], Step: [300/301], Loss: 0.1407, Dev: 0.7630\n",
            "Epoch: [381/1000], Step: [100/301], Loss: 0.1872, Dev: 0.7670\n",
            "Epoch: [381/1000], Step: [200/301], Loss: 0.0950, Dev: 0.7470\n",
            "Epoch: [381/1000], Step: [300/301], Loss: 0.1197, Dev: 0.7580\n",
            "Epoch: [382/1000], Step: [100/301], Loss: 0.1713, Dev: 0.7630\n",
            "Epoch: [382/1000], Step: [200/301], Loss: 0.1627, Dev: 0.7400\n",
            "Epoch: [382/1000], Step: [300/301], Loss: 0.3060, Dev: 0.7560\n",
            "Epoch: [383/1000], Step: [100/301], Loss: 0.2013, Dev: 0.7730\n",
            "Epoch: [383/1000], Step: [200/301], Loss: 0.1968, Dev: 0.7640\n",
            "Epoch: [383/1000], Step: [300/301], Loss: 0.1812, Dev: 0.7570\n",
            "Epoch: [384/1000], Step: [100/301], Loss: 0.2832, Dev: 0.7540\n",
            "Epoch: [384/1000], Step: [200/301], Loss: 0.1243, Dev: 0.7590\n",
            "Epoch: [384/1000], Step: [300/301], Loss: 0.2675, Dev: 0.7480\n",
            "Epoch: [385/1000], Step: [100/301], Loss: 0.1689, Dev: 0.7410\n",
            "Epoch: [385/1000], Step: [200/301], Loss: 0.2203, Dev: 0.7670\n",
            "Epoch: [385/1000], Step: [300/301], Loss: 0.2087, Dev: 0.7690\n",
            "Epoch: [386/1000], Step: [100/301], Loss: 0.2018, Dev: 0.7600\n",
            "Epoch: [386/1000], Step: [200/301], Loss: 0.2277, Dev: 0.7680\n",
            "Epoch: [386/1000], Step: [300/301], Loss: 0.1446, Dev: 0.7630\n",
            "Epoch: [387/1000], Step: [100/301], Loss: 0.2361, Dev: 0.7430\n",
            "Epoch: [387/1000], Step: [200/301], Loss: 0.1246, Dev: 0.7460\n",
            "Epoch: [387/1000], Step: [300/301], Loss: 0.2739, Dev: 0.7420\n",
            "Epoch: [388/1000], Step: [100/301], Loss: 0.1662, Dev: 0.7690\n",
            "Epoch: [388/1000], Step: [200/301], Loss: 0.3125, Dev: 0.7540\n",
            "Epoch: [388/1000], Step: [300/301], Loss: 0.1605, Dev: 0.7370\n",
            "Epoch: [389/1000], Step: [100/301], Loss: 0.2728, Dev: 0.7620\n",
            "Epoch: [389/1000], Step: [200/301], Loss: 0.1259, Dev: 0.7480\n",
            "Epoch: [389/1000], Step: [300/301], Loss: 0.2371, Dev: 0.7500\n",
            "Epoch: [390/1000], Step: [100/301], Loss: 0.3087, Dev: 0.7380\n",
            "Epoch: [390/1000], Step: [200/301], Loss: 0.1618, Dev: 0.7520\n",
            "Epoch: [390/1000], Step: [300/301], Loss: 0.1770, Dev: 0.7610\n",
            "Epoch: [391/1000], Step: [100/301], Loss: 0.1386, Dev: 0.7450\n",
            "Epoch: [391/1000], Step: [200/301], Loss: 0.2602, Dev: 0.7670\n",
            "Epoch: [391/1000], Step: [300/301], Loss: 0.1123, Dev: 0.7720\n",
            "Epoch: [392/1000], Step: [100/301], Loss: 0.1654, Dev: 0.7450\n",
            "Epoch: [392/1000], Step: [200/301], Loss: 0.1650, Dev: 0.7610\n",
            "Epoch: [392/1000], Step: [300/301], Loss: 0.1722, Dev: 0.7580\n",
            "Epoch: [393/1000], Step: [100/301], Loss: 0.2257, Dev: 0.7710\n",
            "Epoch: [393/1000], Step: [200/301], Loss: 0.1745, Dev: 0.7560\n",
            "Epoch: [393/1000], Step: [300/301], Loss: 0.2577, Dev: 0.7560\n",
            "Epoch: [394/1000], Step: [100/301], Loss: 0.1445, Dev: 0.7740\n",
            "Epoch: [394/1000], Step: [200/301], Loss: 0.1967, Dev: 0.7390\n",
            "Epoch: [394/1000], Step: [300/301], Loss: 0.2484, Dev: 0.7530\n",
            "Epoch: [395/1000], Step: [100/301], Loss: 0.1757, Dev: 0.7710\n",
            "Epoch: [395/1000], Step: [200/301], Loss: 0.1897, Dev: 0.7710\n",
            "Epoch: [395/1000], Step: [300/301], Loss: 0.1550, Dev: 0.7600\n",
            "Epoch: [396/1000], Step: [100/301], Loss: 0.1609, Dev: 0.7490\n",
            "Epoch: [396/1000], Step: [200/301], Loss: 0.2304, Dev: 0.7510\n",
            "Epoch: [396/1000], Step: [300/301], Loss: 0.2315, Dev: 0.7640\n",
            "Epoch: [397/1000], Step: [100/301], Loss: 0.3156, Dev: 0.7460\n",
            "Epoch: [397/1000], Step: [200/301], Loss: 0.2784, Dev: 0.7480\n",
            "Epoch: [397/1000], Step: [300/301], Loss: 0.2901, Dev: 0.7580\n",
            "Epoch: [398/1000], Step: [100/301], Loss: 0.1222, Dev: 0.7500\n",
            "Epoch: [398/1000], Step: [200/301], Loss: 0.2000, Dev: 0.7480\n",
            "Epoch: [398/1000], Step: [300/301], Loss: 0.2061, Dev: 0.7530\n",
            "Epoch: [399/1000], Step: [100/301], Loss: 0.1641, Dev: 0.7640\n",
            "Epoch: [399/1000], Step: [200/301], Loss: 0.3367, Dev: 0.7640\n",
            "Epoch: [399/1000], Step: [300/301], Loss: 0.2039, Dev: 0.7530\n",
            "Epoch: [400/1000], Step: [100/301], Loss: 0.2772, Dev: 0.7500\n",
            "Epoch: [400/1000], Step: [200/301], Loss: 0.1829, Dev: 0.7530\n",
            "Epoch: [400/1000], Step: [300/301], Loss: 0.1825, Dev: 0.7480\n",
            "Epoch: [401/1000], Step: [100/301], Loss: 0.1993, Dev: 0.7540\n",
            "Epoch: [401/1000], Step: [200/301], Loss: 0.1599, Dev: 0.7470\n",
            "Epoch: [401/1000], Step: [300/301], Loss: 0.1397, Dev: 0.7740\n",
            "Epoch: [402/1000], Step: [100/301], Loss: 0.1144, Dev: 0.7590\n",
            "Epoch: [402/1000], Step: [200/301], Loss: 0.2953, Dev: 0.7570\n",
            "Epoch: [402/1000], Step: [300/301], Loss: 0.1806, Dev: 0.7570\n",
            "Epoch: [403/1000], Step: [100/301], Loss: 0.1423, Dev: 0.7570\n",
            "Epoch: [403/1000], Step: [200/301], Loss: 0.1728, Dev: 0.7450\n",
            "Epoch: [403/1000], Step: [300/301], Loss: 0.2088, Dev: 0.7630\n",
            "Epoch: [404/1000], Step: [100/301], Loss: 0.1469, Dev: 0.7450\n",
            "Epoch: [404/1000], Step: [200/301], Loss: 0.1434, Dev: 0.7550\n",
            "Epoch: [404/1000], Step: [300/301], Loss: 0.1545, Dev: 0.7540\n",
            "Epoch: [405/1000], Step: [100/301], Loss: 0.1548, Dev: 0.7490\n",
            "Epoch: [405/1000], Step: [200/301], Loss: 0.2496, Dev: 0.7570\n",
            "Epoch: [405/1000], Step: [300/301], Loss: 0.1373, Dev: 0.7630\n",
            "Epoch: [406/1000], Step: [100/301], Loss: 0.2660, Dev: 0.7640\n",
            "Epoch: [406/1000], Step: [200/301], Loss: 0.2622, Dev: 0.7580\n",
            "Epoch: [406/1000], Step: [300/301], Loss: 0.1260, Dev: 0.7680\n",
            "Epoch: [407/1000], Step: [100/301], Loss: 0.1769, Dev: 0.7400\n",
            "Epoch: [407/1000], Step: [200/301], Loss: 0.1888, Dev: 0.7510\n",
            "Epoch: [407/1000], Step: [300/301], Loss: 0.2147, Dev: 0.7380\n",
            "Epoch: [408/1000], Step: [100/301], Loss: 0.1634, Dev: 0.7630\n",
            "Epoch: [408/1000], Step: [200/301], Loss: 0.1799, Dev: 0.7730\n",
            "Epoch: [408/1000], Step: [300/301], Loss: 0.3183, Dev: 0.7670\n",
            "Epoch: [409/1000], Step: [100/301], Loss: 0.1725, Dev: 0.7560\n",
            "Epoch: [409/1000], Step: [200/301], Loss: 0.3284, Dev: 0.7560\n",
            "Epoch: [409/1000], Step: [300/301], Loss: 0.1774, Dev: 0.7500\n",
            "Epoch: [410/1000], Step: [100/301], Loss: 0.1923, Dev: 0.7480\n",
            "Epoch: [410/1000], Step: [200/301], Loss: 0.0895, Dev: 0.7400\n",
            "Epoch: [410/1000], Step: [300/301], Loss: 0.2763, Dev: 0.7530\n",
            "Epoch: [411/1000], Step: [100/301], Loss: 0.2582, Dev: 0.7540\n",
            "Epoch: [411/1000], Step: [200/301], Loss: 0.1516, Dev: 0.7580\n",
            "Epoch: [411/1000], Step: [300/301], Loss: 0.2342, Dev: 0.7410\n",
            "Epoch: [412/1000], Step: [100/301], Loss: 0.1512, Dev: 0.7630\n",
            "Epoch: [412/1000], Step: [200/301], Loss: 0.2447, Dev: 0.7750\n",
            "Epoch: [412/1000], Step: [300/301], Loss: 0.1408, Dev: 0.7480\n",
            "Epoch: [413/1000], Step: [100/301], Loss: 0.1264, Dev: 0.7520\n",
            "Epoch: [413/1000], Step: [200/301], Loss: 0.2071, Dev: 0.7640\n",
            "Epoch: [413/1000], Step: [300/301], Loss: 0.2909, Dev: 0.7530\n",
            "Epoch: [414/1000], Step: [100/301], Loss: 0.2843, Dev: 0.7630\n",
            "Epoch: [414/1000], Step: [200/301], Loss: 0.2010, Dev: 0.7690\n",
            "Epoch: [414/1000], Step: [300/301], Loss: 0.3132, Dev: 0.7430\n",
            "Epoch: [415/1000], Step: [100/301], Loss: 0.2043, Dev: 0.7670\n",
            "Epoch: [415/1000], Step: [200/301], Loss: 0.2226, Dev: 0.7430\n",
            "Epoch: [415/1000], Step: [300/301], Loss: 0.2424, Dev: 0.7580\n",
            "Epoch: [416/1000], Step: [100/301], Loss: 0.2190, Dev: 0.7570\n",
            "Epoch: [416/1000], Step: [200/301], Loss: 0.2652, Dev: 0.7480\n",
            "Epoch: [416/1000], Step: [300/301], Loss: 0.2114, Dev: 0.7660\n",
            "Epoch: [417/1000], Step: [100/301], Loss: 0.1325, Dev: 0.7590\n",
            "Epoch: [417/1000], Step: [200/301], Loss: 0.3155, Dev: 0.7580\n",
            "Epoch: [417/1000], Step: [300/301], Loss: 0.2406, Dev: 0.7600\n",
            "Epoch: [418/1000], Step: [100/301], Loss: 0.2401, Dev: 0.7550\n",
            "Epoch: [418/1000], Step: [200/301], Loss: 0.1290, Dev: 0.7570\n",
            "Epoch: [418/1000], Step: [300/301], Loss: 0.1468, Dev: 0.7620\n",
            "Epoch: [419/1000], Step: [100/301], Loss: 0.3535, Dev: 0.7500\n",
            "Epoch: [419/1000], Step: [200/301], Loss: 0.1961, Dev: 0.7600\n",
            "Epoch: [419/1000], Step: [300/301], Loss: 0.2719, Dev: 0.7560\n",
            "Epoch: [420/1000], Step: [100/301], Loss: 0.3362, Dev: 0.7390\n",
            "Epoch: [420/1000], Step: [200/301], Loss: 0.2331, Dev: 0.7560\n",
            "Epoch: [420/1000], Step: [300/301], Loss: 0.2036, Dev: 0.7500\n",
            "Epoch: [421/1000], Step: [100/301], Loss: 0.1632, Dev: 0.7530\n",
            "Epoch: [421/1000], Step: [200/301], Loss: 0.1583, Dev: 0.7630\n",
            "Epoch: [421/1000], Step: [300/301], Loss: 0.3145, Dev: 0.7490\n",
            "Epoch: [422/1000], Step: [100/301], Loss: 0.2287, Dev: 0.7560\n",
            "Epoch: [422/1000], Step: [200/301], Loss: 0.1823, Dev: 0.7570\n",
            "Epoch: [422/1000], Step: [300/301], Loss: 0.1050, Dev: 0.7690\n",
            "Epoch: [423/1000], Step: [100/301], Loss: 0.2420, Dev: 0.7500\n",
            "Epoch: [423/1000], Step: [200/301], Loss: 0.1319, Dev: 0.7470\n",
            "Epoch: [423/1000], Step: [300/301], Loss: 0.2639, Dev: 0.7590\n",
            "Epoch: [424/1000], Step: [100/301], Loss: 0.2177, Dev: 0.7470\n",
            "Epoch: [424/1000], Step: [200/301], Loss: 0.1799, Dev: 0.7440\n",
            "Epoch: [424/1000], Step: [300/301], Loss: 0.1922, Dev: 0.7520\n",
            "Epoch: [425/1000], Step: [100/301], Loss: 0.1571, Dev: 0.7510\n",
            "Epoch: [425/1000], Step: [200/301], Loss: 0.2196, Dev: 0.7460\n",
            "Epoch: [425/1000], Step: [300/301], Loss: 0.2485, Dev: 0.7600\n",
            "Epoch: [426/1000], Step: [100/301], Loss: 0.1965, Dev: 0.7490\n",
            "Epoch: [426/1000], Step: [200/301], Loss: 0.1107, Dev: 0.7530\n",
            "Epoch: [426/1000], Step: [300/301], Loss: 0.1800, Dev: 0.7580\n",
            "Epoch: [427/1000], Step: [100/301], Loss: 0.2123, Dev: 0.7450\n",
            "Epoch: [427/1000], Step: [200/301], Loss: 0.1484, Dev: 0.7760\n",
            "Epoch: [427/1000], Step: [300/301], Loss: 0.3495, Dev: 0.7650\n",
            "Epoch: [428/1000], Step: [100/301], Loss: 0.1747, Dev: 0.7450\n",
            "Epoch: [428/1000], Step: [200/301], Loss: 0.1857, Dev: 0.7690\n",
            "Epoch: [428/1000], Step: [300/301], Loss: 0.1933, Dev: 0.7630\n",
            "Epoch: [429/1000], Step: [100/301], Loss: 0.1426, Dev: 0.7560\n",
            "Epoch: [429/1000], Step: [200/301], Loss: 0.1677, Dev: 0.7350\n",
            "Epoch: [429/1000], Step: [300/301], Loss: 0.1130, Dev: 0.7430\n",
            "Epoch: [430/1000], Step: [100/301], Loss: 0.1805, Dev: 0.7670\n",
            "Epoch: [430/1000], Step: [200/301], Loss: 0.1337, Dev: 0.7600\n",
            "Epoch: [430/1000], Step: [300/301], Loss: 0.2482, Dev: 0.7560\n",
            "Epoch: [431/1000], Step: [100/301], Loss: 0.2169, Dev: 0.7580\n",
            "Epoch: [431/1000], Step: [200/301], Loss: 0.2243, Dev: 0.7540\n",
            "Epoch: [431/1000], Step: [300/301], Loss: 0.1895, Dev: 0.7590\n",
            "Epoch: [432/1000], Step: [100/301], Loss: 0.3251, Dev: 0.7510\n",
            "Epoch: [432/1000], Step: [200/301], Loss: 0.1746, Dev: 0.7610\n",
            "Epoch: [432/1000], Step: [300/301], Loss: 0.1596, Dev: 0.7540\n",
            "Epoch: [433/1000], Step: [100/301], Loss: 0.2024, Dev: 0.7610\n",
            "Epoch: [433/1000], Step: [200/301], Loss: 0.1420, Dev: 0.7530\n",
            "Epoch: [433/1000], Step: [300/301], Loss: 0.0981, Dev: 0.7590\n",
            "Epoch: [434/1000], Step: [100/301], Loss: 0.3457, Dev: 0.7450\n",
            "Epoch: [434/1000], Step: [200/301], Loss: 0.2013, Dev: 0.7430\n",
            "Epoch: [434/1000], Step: [300/301], Loss: 0.1767, Dev: 0.7750\n",
            "Epoch: [435/1000], Step: [100/301], Loss: 0.2375, Dev: 0.7640\n",
            "Epoch: [435/1000], Step: [200/301], Loss: 0.2233, Dev: 0.7550\n",
            "Epoch: [435/1000], Step: [300/301], Loss: 0.1434, Dev: 0.7550\n",
            "Epoch: [436/1000], Step: [100/301], Loss: 0.1177, Dev: 0.7510\n",
            "Epoch: [436/1000], Step: [200/301], Loss: 0.3307, Dev: 0.7790\n",
            "Epoch: [436/1000], Step: [300/301], Loss: 0.1580, Dev: 0.7510\n",
            "Epoch: [437/1000], Step: [100/301], Loss: 0.1028, Dev: 0.7590\n",
            "Epoch: [437/1000], Step: [200/301], Loss: 0.1729, Dev: 0.7520\n",
            "Epoch: [437/1000], Step: [300/301], Loss: 0.2398, Dev: 0.7520\n",
            "Epoch: [438/1000], Step: [100/301], Loss: 0.2023, Dev: 0.7560\n",
            "Epoch: [438/1000], Step: [200/301], Loss: 0.2056, Dev: 0.7550\n",
            "Epoch: [438/1000], Step: [300/301], Loss: 0.2505, Dev: 0.7520\n",
            "Epoch: [439/1000], Step: [100/301], Loss: 0.1309, Dev: 0.7700\n",
            "Epoch: [439/1000], Step: [200/301], Loss: 0.1677, Dev: 0.7490\n",
            "Epoch: [439/1000], Step: [300/301], Loss: 0.1517, Dev: 0.7480\n",
            "Epoch: [440/1000], Step: [100/301], Loss: 0.2706, Dev: 0.7550\n",
            "Epoch: [440/1000], Step: [200/301], Loss: 0.1250, Dev: 0.7530\n",
            "Epoch: [440/1000], Step: [300/301], Loss: 0.1456, Dev: 0.7620\n",
            "Epoch: [441/1000], Step: [100/301], Loss: 0.1793, Dev: 0.7450\n",
            "Epoch: [441/1000], Step: [200/301], Loss: 0.1885, Dev: 0.7570\n",
            "Epoch: [441/1000], Step: [300/301], Loss: 0.1440, Dev: 0.7540\n",
            "Epoch: [442/1000], Step: [100/301], Loss: 0.1984, Dev: 0.7530\n",
            "Epoch: [442/1000], Step: [200/301], Loss: 0.2211, Dev: 0.7630\n",
            "Epoch: [442/1000], Step: [300/301], Loss: 0.1503, Dev: 0.7420\n",
            "Epoch: [443/1000], Step: [100/301], Loss: 0.2187, Dev: 0.7670\n",
            "Epoch: [443/1000], Step: [200/301], Loss: 0.1830, Dev: 0.7680\n",
            "Epoch: [443/1000], Step: [300/301], Loss: 0.1587, Dev: 0.7440\n",
            "Epoch: [444/1000], Step: [100/301], Loss: 0.1499, Dev: 0.7520\n",
            "Epoch: [444/1000], Step: [200/301], Loss: 0.1770, Dev: 0.7690\n",
            "Epoch: [444/1000], Step: [300/301], Loss: 0.2592, Dev: 0.7660\n",
            "Epoch: [445/1000], Step: [100/301], Loss: 0.2757, Dev: 0.7690\n",
            "Epoch: [445/1000], Step: [200/301], Loss: 0.1458, Dev: 0.7520\n",
            "Epoch: [445/1000], Step: [300/301], Loss: 0.0805, Dev: 0.7490\n",
            "Epoch: [446/1000], Step: [100/301], Loss: 0.1188, Dev: 0.7650\n",
            "Epoch: [446/1000], Step: [200/301], Loss: 0.1534, Dev: 0.7580\n",
            "Epoch: [446/1000], Step: [300/301], Loss: 0.2915, Dev: 0.7480\n",
            "Epoch: [447/1000], Step: [100/301], Loss: 0.1120, Dev: 0.7550\n",
            "Epoch: [447/1000], Step: [200/301], Loss: 0.2523, Dev: 0.7410\n",
            "Epoch: [447/1000], Step: [300/301], Loss: 0.1911, Dev: 0.7550\n",
            "Epoch: [448/1000], Step: [100/301], Loss: 0.2088, Dev: 0.7560\n",
            "Epoch: [448/1000], Step: [200/301], Loss: 0.2164, Dev: 0.7430\n",
            "Epoch: [448/1000], Step: [300/301], Loss: 0.2753, Dev: 0.7630\n",
            "Epoch: [449/1000], Step: [100/301], Loss: 0.1296, Dev: 0.7520\n",
            "Epoch: [449/1000], Step: [200/301], Loss: 0.1970, Dev: 0.7540\n",
            "Epoch: [449/1000], Step: [300/301], Loss: 0.1791, Dev: 0.7560\n",
            "Epoch: [450/1000], Step: [100/301], Loss: 0.1260, Dev: 0.7520\n",
            "Epoch: [450/1000], Step: [200/301], Loss: 0.1857, Dev: 0.7410\n",
            "Epoch: [450/1000], Step: [300/301], Loss: 0.1765, Dev: 0.7610\n",
            "Epoch: [451/1000], Step: [100/301], Loss: 0.1798, Dev: 0.7540\n",
            "Epoch: [451/1000], Step: [200/301], Loss: 0.1257, Dev: 0.7500\n",
            "Epoch: [451/1000], Step: [300/301], Loss: 0.1443, Dev: 0.7420\n",
            "Epoch: [452/1000], Step: [100/301], Loss: 0.1998, Dev: 0.7640\n",
            "Epoch: [452/1000], Step: [200/301], Loss: 0.1519, Dev: 0.7590\n",
            "Epoch: [452/1000], Step: [300/301], Loss: 0.3353, Dev: 0.7850\n",
            "Epoch: [453/1000], Step: [100/301], Loss: 0.2222, Dev: 0.7600\n",
            "Epoch: [453/1000], Step: [200/301], Loss: 0.2080, Dev: 0.7590\n",
            "Epoch: [453/1000], Step: [300/301], Loss: 0.1409, Dev: 0.7390\n",
            "Epoch: [454/1000], Step: [100/301], Loss: 0.2384, Dev: 0.7290\n",
            "Epoch: [454/1000], Step: [200/301], Loss: 0.1901, Dev: 0.7640\n",
            "Epoch: [454/1000], Step: [300/301], Loss: 0.1213, Dev: 0.7560\n",
            "Epoch: [455/1000], Step: [100/301], Loss: 0.1108, Dev: 0.7580\n",
            "Epoch: [455/1000], Step: [200/301], Loss: 0.1919, Dev: 0.7410\n",
            "Epoch: [455/1000], Step: [300/301], Loss: 0.1731, Dev: 0.7670\n",
            "Epoch: [456/1000], Step: [100/301], Loss: 0.2702, Dev: 0.7730\n",
            "Epoch: [456/1000], Step: [200/301], Loss: 0.1602, Dev: 0.7520\n",
            "Epoch: [456/1000], Step: [300/301], Loss: 0.2379, Dev: 0.7550\n",
            "Epoch: [457/1000], Step: [100/301], Loss: 0.2188, Dev: 0.7500\n",
            "Epoch: [457/1000], Step: [200/301], Loss: 0.2711, Dev: 0.7450\n",
            "Epoch: [457/1000], Step: [300/301], Loss: 0.2365, Dev: 0.7570\n",
            "Epoch: [458/1000], Step: [100/301], Loss: 0.1634, Dev: 0.7700\n",
            "Epoch: [458/1000], Step: [200/301], Loss: 0.2097, Dev: 0.7740\n",
            "Epoch: [458/1000], Step: [300/301], Loss: 0.2439, Dev: 0.7530\n",
            "Epoch: [459/1000], Step: [100/301], Loss: 0.3454, Dev: 0.7600\n",
            "Epoch: [459/1000], Step: [200/301], Loss: 0.1988, Dev: 0.7590\n",
            "Epoch: [459/1000], Step: [300/301], Loss: 0.2572, Dev: 0.7550\n",
            "Epoch: [460/1000], Step: [100/301], Loss: 0.0965, Dev: 0.7520\n",
            "Epoch: [460/1000], Step: [200/301], Loss: 0.1860, Dev: 0.7650\n",
            "Epoch: [460/1000], Step: [300/301], Loss: 0.2299, Dev: 0.7550\n",
            "Epoch: [461/1000], Step: [100/301], Loss: 0.3387, Dev: 0.7650\n",
            "Epoch: [461/1000], Step: [200/301], Loss: 0.1394, Dev: 0.7480\n",
            "Epoch: [461/1000], Step: [300/301], Loss: 0.2344, Dev: 0.7580\n",
            "Epoch: [462/1000], Step: [100/301], Loss: 0.1305, Dev: 0.7580\n",
            "Epoch: [462/1000], Step: [200/301], Loss: 0.2231, Dev: 0.7400\n",
            "Epoch: [462/1000], Step: [300/301], Loss: 0.3669, Dev: 0.7530\n",
            "Epoch: [463/1000], Step: [100/301], Loss: 0.3796, Dev: 0.7400\n",
            "Epoch: [463/1000], Step: [200/301], Loss: 0.1531, Dev: 0.7730\n",
            "Epoch: [463/1000], Step: [300/301], Loss: 0.1869, Dev: 0.7510\n",
            "Epoch: [464/1000], Step: [100/301], Loss: 0.2859, Dev: 0.7470\n",
            "Epoch: [464/1000], Step: [200/301], Loss: 0.1719, Dev: 0.7580\n",
            "Epoch: [464/1000], Step: [300/301], Loss: 0.1724, Dev: 0.7700\n",
            "Epoch: [465/1000], Step: [100/301], Loss: 0.1482, Dev: 0.7610\n",
            "Epoch: [465/1000], Step: [200/301], Loss: 0.2812, Dev: 0.7580\n",
            "Epoch: [465/1000], Step: [300/301], Loss: 0.2763, Dev: 0.7590\n",
            "Epoch: [466/1000], Step: [100/301], Loss: 0.2327, Dev: 0.7660\n",
            "Epoch: [466/1000], Step: [200/301], Loss: 0.1928, Dev: 0.7640\n",
            "Epoch: [466/1000], Step: [300/301], Loss: 0.2831, Dev: 0.7430\n",
            "Epoch: [467/1000], Step: [100/301], Loss: 0.1759, Dev: 0.7360\n",
            "Epoch: [467/1000], Step: [200/301], Loss: 0.1168, Dev: 0.7590\n",
            "Epoch: [467/1000], Step: [300/301], Loss: 0.2042, Dev: 0.7750\n",
            "Epoch: [468/1000], Step: [100/301], Loss: 0.2116, Dev: 0.7600\n",
            "Epoch: [468/1000], Step: [200/301], Loss: 0.1074, Dev: 0.7660\n",
            "Epoch: [468/1000], Step: [300/301], Loss: 0.1912, Dev: 0.7470\n",
            "Epoch: [469/1000], Step: [100/301], Loss: 0.2532, Dev: 0.7530\n",
            "Epoch: [469/1000], Step: [200/301], Loss: 0.1851, Dev: 0.7560\n",
            "Epoch: [469/1000], Step: [300/301], Loss: 0.2715, Dev: 0.7450\n",
            "Epoch: [470/1000], Step: [100/301], Loss: 0.1446, Dev: 0.7510\n",
            "Epoch: [470/1000], Step: [200/301], Loss: 0.2210, Dev: 0.7430\n",
            "Epoch: [470/1000], Step: [300/301], Loss: 0.1308, Dev: 0.7560\n",
            "Epoch: [471/1000], Step: [100/301], Loss: 0.1974, Dev: 0.7500\n",
            "Epoch: [471/1000], Step: [200/301], Loss: 0.3059, Dev: 0.7640\n",
            "Epoch: [471/1000], Step: [300/301], Loss: 0.1096, Dev: 0.7440\n",
            "Epoch: [472/1000], Step: [100/301], Loss: 0.2248, Dev: 0.7450\n",
            "Epoch: [472/1000], Step: [200/301], Loss: 0.1991, Dev: 0.7560\n",
            "Epoch: [472/1000], Step: [300/301], Loss: 0.1783, Dev: 0.7700\n",
            "Epoch: [473/1000], Step: [100/301], Loss: 0.2918, Dev: 0.7550\n",
            "Epoch: [473/1000], Step: [200/301], Loss: 0.1710, Dev: 0.7490\n",
            "Epoch: [473/1000], Step: [300/301], Loss: 0.2681, Dev: 0.7820\n",
            "Epoch: [474/1000], Step: [100/301], Loss: 0.1532, Dev: 0.7480\n",
            "Epoch: [474/1000], Step: [200/301], Loss: 0.2293, Dev: 0.7420\n",
            "Epoch: [474/1000], Step: [300/301], Loss: 0.2395, Dev: 0.7760\n",
            "Epoch: [475/1000], Step: [100/301], Loss: 0.1475, Dev: 0.7660\n",
            "Epoch: [475/1000], Step: [200/301], Loss: 0.4186, Dev: 0.7540\n",
            "Epoch: [475/1000], Step: [300/301], Loss: 0.2932, Dev: 0.7650\n",
            "Epoch: [476/1000], Step: [100/301], Loss: 0.2257, Dev: 0.7660\n",
            "Epoch: [476/1000], Step: [200/301], Loss: 0.1196, Dev: 0.7550\n",
            "Epoch: [476/1000], Step: [300/301], Loss: 0.2135, Dev: 0.7210\n",
            "Epoch: [477/1000], Step: [100/301], Loss: 0.2749, Dev: 0.7580\n",
            "Epoch: [477/1000], Step: [200/301], Loss: 0.2225, Dev: 0.7510\n",
            "Epoch: [477/1000], Step: [300/301], Loss: 0.2719, Dev: 0.7570\n",
            "Epoch: [478/1000], Step: [100/301], Loss: 0.2086, Dev: 0.7860\n",
            "Epoch: [478/1000], Step: [200/301], Loss: 0.1533, Dev: 0.7500\n",
            "Epoch: [478/1000], Step: [300/301], Loss: 0.1323, Dev: 0.7600\n",
            "Epoch: [479/1000], Step: [100/301], Loss: 0.2352, Dev: 0.7430\n",
            "Epoch: [479/1000], Step: [200/301], Loss: 0.1373, Dev: 0.7510\n",
            "Epoch: [479/1000], Step: [300/301], Loss: 0.1979, Dev: 0.7550\n",
            "Epoch: [480/1000], Step: [100/301], Loss: 0.1780, Dev: 0.7670\n",
            "Epoch: [480/1000], Step: [200/301], Loss: 0.1739, Dev: 0.7580\n",
            "Epoch: [480/1000], Step: [300/301], Loss: 0.1975, Dev: 0.7350\n",
            "Epoch: [481/1000], Step: [100/301], Loss: 0.1423, Dev: 0.7410\n",
            "Epoch: [481/1000], Step: [200/301], Loss: 0.2587, Dev: 0.7560\n",
            "Epoch: [481/1000], Step: [300/301], Loss: 0.2413, Dev: 0.7690\n",
            "Epoch: [482/1000], Step: [100/301], Loss: 0.2513, Dev: 0.7590\n",
            "Epoch: [482/1000], Step: [200/301], Loss: 0.1989, Dev: 0.7570\n",
            "Epoch: [482/1000], Step: [300/301], Loss: 0.1955, Dev: 0.7450\n",
            "Epoch: [483/1000], Step: [100/301], Loss: 0.2283, Dev: 0.7710\n",
            "Epoch: [483/1000], Step: [200/301], Loss: 0.1433, Dev: 0.7660\n",
            "Epoch: [483/1000], Step: [300/301], Loss: 0.2278, Dev: 0.7710\n",
            "Epoch: [484/1000], Step: [100/301], Loss: 0.2593, Dev: 0.7500\n",
            "Epoch: [484/1000], Step: [200/301], Loss: 0.2091, Dev: 0.7560\n",
            "Epoch: [484/1000], Step: [300/301], Loss: 0.3670, Dev: 0.7520\n",
            "Epoch: [485/1000], Step: [100/301], Loss: 0.3407, Dev: 0.7460\n",
            "Epoch: [485/1000], Step: [200/301], Loss: 0.2224, Dev: 0.7580\n",
            "Epoch: [485/1000], Step: [300/301], Loss: 0.2295, Dev: 0.7660\n",
            "Epoch: [486/1000], Step: [100/301], Loss: 0.0884, Dev: 0.7690\n",
            "Epoch: [486/1000], Step: [200/301], Loss: 0.1438, Dev: 0.7420\n",
            "Epoch: [486/1000], Step: [300/301], Loss: 0.2231, Dev: 0.7440\n",
            "Epoch: [487/1000], Step: [100/301], Loss: 0.2769, Dev: 0.7550\n",
            "Epoch: [487/1000], Step: [200/301], Loss: 0.1880, Dev: 0.7630\n",
            "Epoch: [487/1000], Step: [300/301], Loss: 0.2513, Dev: 0.7600\n",
            "Epoch: [488/1000], Step: [100/301], Loss: 0.1876, Dev: 0.7600\n",
            "Epoch: [488/1000], Step: [200/301], Loss: 0.1369, Dev: 0.7490\n",
            "Epoch: [488/1000], Step: [300/301], Loss: 0.1747, Dev: 0.7710\n",
            "Epoch: [489/1000], Step: [100/301], Loss: 0.1116, Dev: 0.7470\n",
            "Epoch: [489/1000], Step: [200/301], Loss: 0.2419, Dev: 0.7410\n",
            "Epoch: [489/1000], Step: [300/301], Loss: 0.1558, Dev: 0.7450\n",
            "Epoch: [490/1000], Step: [100/301], Loss: 0.1893, Dev: 0.7530\n",
            "Epoch: [490/1000], Step: [200/301], Loss: 0.1058, Dev: 0.7480\n",
            "Epoch: [490/1000], Step: [300/301], Loss: 0.1802, Dev: 0.7770\n",
            "Epoch: [491/1000], Step: [100/301], Loss: 0.1528, Dev: 0.7570\n",
            "Epoch: [491/1000], Step: [200/301], Loss: 0.1287, Dev: 0.7470\n",
            "Epoch: [491/1000], Step: [300/301], Loss: 0.3309, Dev: 0.7540\n",
            "Epoch: [492/1000], Step: [100/301], Loss: 0.1687, Dev: 0.7410\n",
            "Epoch: [492/1000], Step: [200/301], Loss: 0.1030, Dev: 0.7570\n",
            "Epoch: [492/1000], Step: [300/301], Loss: 0.1505, Dev: 0.7460\n",
            "Epoch: [493/1000], Step: [100/301], Loss: 0.2900, Dev: 0.7640\n",
            "Epoch: [493/1000], Step: [200/301], Loss: 0.2477, Dev: 0.7490\n",
            "Epoch: [493/1000], Step: [300/301], Loss: 0.1663, Dev: 0.7540\n",
            "Epoch: [494/1000], Step: [100/301], Loss: 0.3089, Dev: 0.7780\n",
            "Epoch: [494/1000], Step: [200/301], Loss: 0.1549, Dev: 0.7440\n",
            "Epoch: [494/1000], Step: [300/301], Loss: 0.1961, Dev: 0.7610\n",
            "Epoch: [495/1000], Step: [100/301], Loss: 0.2242, Dev: 0.7500\n",
            "Epoch: [495/1000], Step: [200/301], Loss: 0.3336, Dev: 0.7570\n",
            "Epoch: [495/1000], Step: [300/301], Loss: 0.1828, Dev: 0.7500\n",
            "Epoch: [496/1000], Step: [100/301], Loss: 0.1668, Dev: 0.7460\n",
            "Epoch: [496/1000], Step: [200/301], Loss: 0.2608, Dev: 0.7440\n",
            "Epoch: [496/1000], Step: [300/301], Loss: 0.1661, Dev: 0.7460\n",
            "Epoch: [497/1000], Step: [100/301], Loss: 0.1807, Dev: 0.7440\n",
            "Epoch: [497/1000], Step: [200/301], Loss: 0.1943, Dev: 0.7550\n",
            "Epoch: [497/1000], Step: [300/301], Loss: 0.2310, Dev: 0.7430\n",
            "Epoch: [498/1000], Step: [100/301], Loss: 0.1434, Dev: 0.7560\n",
            "Epoch: [498/1000], Step: [200/301], Loss: 0.1791, Dev: 0.7590\n",
            "Epoch: [498/1000], Step: [300/301], Loss: 0.1747, Dev: 0.7510\n",
            "Epoch: [499/1000], Step: [100/301], Loss: 0.2193, Dev: 0.7380\n",
            "Epoch: [499/1000], Step: [200/301], Loss: 0.1744, Dev: 0.7490\n",
            "Epoch: [499/1000], Step: [300/301], Loss: 0.1448, Dev: 0.7560\n",
            "Epoch: [500/1000], Step: [100/301], Loss: 0.2642, Dev: 0.7530\n",
            "Epoch: [500/1000], Step: [200/301], Loss: 0.1636, Dev: 0.7510\n",
            "Epoch: [500/1000], Step: [300/301], Loss: 0.1071, Dev: 0.7540\n",
            "Epoch: [501/1000], Step: [100/301], Loss: 0.0911, Dev: 0.7470\n",
            "Epoch: [501/1000], Step: [200/301], Loss: 0.2509, Dev: 0.7410\n",
            "Epoch: [501/1000], Step: [300/301], Loss: 0.1356, Dev: 0.7340\n",
            "Epoch: [502/1000], Step: [100/301], Loss: 0.1212, Dev: 0.7600\n",
            "Epoch: [502/1000], Step: [200/301], Loss: 0.3450, Dev: 0.7570\n",
            "Epoch: [502/1000], Step: [300/301], Loss: 0.1347, Dev: 0.7600\n",
            "Epoch: [503/1000], Step: [100/301], Loss: 0.1487, Dev: 0.7460\n",
            "Epoch: [503/1000], Step: [200/301], Loss: 0.1935, Dev: 0.7670\n",
            "Epoch: [503/1000], Step: [300/301], Loss: 0.1921, Dev: 0.7500\n",
            "Epoch: [504/1000], Step: [100/301], Loss: 0.3194, Dev: 0.7580\n",
            "Epoch: [504/1000], Step: [200/301], Loss: 0.1750, Dev: 0.7380\n",
            "Epoch: [504/1000], Step: [300/301], Loss: 0.2380, Dev: 0.7380\n",
            "Epoch: [505/1000], Step: [100/301], Loss: 0.1366, Dev: 0.7710\n",
            "Epoch: [505/1000], Step: [200/301], Loss: 0.1440, Dev: 0.7410\n",
            "Epoch: [505/1000], Step: [300/301], Loss: 0.1661, Dev: 0.7380\n",
            "Epoch: [506/1000], Step: [100/301], Loss: 0.0984, Dev: 0.7550\n",
            "Epoch: [506/1000], Step: [200/301], Loss: 0.1347, Dev: 0.7500\n",
            "Epoch: [506/1000], Step: [300/301], Loss: 0.1732, Dev: 0.7550\n",
            "Epoch: [507/1000], Step: [100/301], Loss: 0.1616, Dev: 0.7420\n",
            "Epoch: [507/1000], Step: [200/301], Loss: 0.2209, Dev: 0.7440\n",
            "Epoch: [507/1000], Step: [300/301], Loss: 0.1795, Dev: 0.7400\n",
            "Epoch: [508/1000], Step: [100/301], Loss: 0.1348, Dev: 0.7550\n",
            "Epoch: [508/1000], Step: [200/301], Loss: 0.1206, Dev: 0.7640\n",
            "Epoch: [508/1000], Step: [300/301], Loss: 0.2316, Dev: 0.7520\n",
            "Epoch: [509/1000], Step: [100/301], Loss: 0.2009, Dev: 0.7430\n",
            "Epoch: [509/1000], Step: [200/301], Loss: 0.2059, Dev: 0.7700\n",
            "Epoch: [509/1000], Step: [300/301], Loss: 0.2099, Dev: 0.7520\n",
            "Epoch: [510/1000], Step: [100/301], Loss: 0.2143, Dev: 0.7590\n",
            "Epoch: [510/1000], Step: [200/301], Loss: 0.2044, Dev: 0.7460\n",
            "Epoch: [510/1000], Step: [300/301], Loss: 0.2409, Dev: 0.7450\n",
            "Epoch: [511/1000], Step: [100/301], Loss: 0.3963, Dev: 0.7440\n",
            "Epoch: [511/1000], Step: [200/301], Loss: 0.1954, Dev: 0.7600\n",
            "Epoch: [511/1000], Step: [300/301], Loss: 0.1434, Dev: 0.7420\n",
            "Epoch: [512/1000], Step: [100/301], Loss: 0.2130, Dev: 0.7500\n",
            "Epoch: [512/1000], Step: [200/301], Loss: 0.1462, Dev: 0.7460\n",
            "Epoch: [512/1000], Step: [300/301], Loss: 0.2047, Dev: 0.7650\n",
            "Epoch: [513/1000], Step: [100/301], Loss: 0.1895, Dev: 0.7550\n",
            "Epoch: [513/1000], Step: [200/301], Loss: 0.1763, Dev: 0.7530\n",
            "Epoch: [513/1000], Step: [300/301], Loss: 0.1912, Dev: 0.7550\n",
            "Epoch: [514/1000], Step: [100/301], Loss: 0.2097, Dev: 0.7560\n",
            "Epoch: [514/1000], Step: [200/301], Loss: 0.3741, Dev: 0.7520\n",
            "Epoch: [514/1000], Step: [300/301], Loss: 0.1490, Dev: 0.7470\n",
            "Epoch: [515/1000], Step: [100/301], Loss: 0.2374, Dev: 0.7340\n",
            "Epoch: [515/1000], Step: [200/301], Loss: 0.1886, Dev: 0.7420\n",
            "Epoch: [515/1000], Step: [300/301], Loss: 0.2458, Dev: 0.7400\n",
            "Epoch: [516/1000], Step: [100/301], Loss: 0.1501, Dev: 0.7430\n",
            "Epoch: [516/1000], Step: [200/301], Loss: 0.1143, Dev: 0.7550\n",
            "Epoch: [516/1000], Step: [300/301], Loss: 0.1663, Dev: 0.7430\n",
            "Epoch: [517/1000], Step: [100/301], Loss: 0.2561, Dev: 0.7440\n",
            "Epoch: [517/1000], Step: [200/301], Loss: 0.1678, Dev: 0.7690\n",
            "Epoch: [517/1000], Step: [300/301], Loss: 0.2027, Dev: 0.7400\n",
            "Epoch: [518/1000], Step: [100/301], Loss: 0.1191, Dev: 0.7380\n",
            "Epoch: [518/1000], Step: [200/301], Loss: 0.2209, Dev: 0.7500\n",
            "Epoch: [518/1000], Step: [300/301], Loss: 0.1897, Dev: 0.7600\n",
            "Epoch: [519/1000], Step: [100/301], Loss: 0.2242, Dev: 0.7670\n",
            "Epoch: [519/1000], Step: [200/301], Loss: 0.1889, Dev: 0.7450\n",
            "Epoch: [519/1000], Step: [300/301], Loss: 0.2021, Dev: 0.7460\n",
            "Epoch: [520/1000], Step: [100/301], Loss: 0.1245, Dev: 0.7440\n",
            "Epoch: [520/1000], Step: [200/301], Loss: 0.1414, Dev: 0.7510\n",
            "Epoch: [520/1000], Step: [300/301], Loss: 0.2143, Dev: 0.7500\n",
            "Epoch: [521/1000], Step: [100/301], Loss: 0.2136, Dev: 0.7660\n",
            "Epoch: [521/1000], Step: [200/301], Loss: 0.1754, Dev: 0.7570\n",
            "Epoch: [521/1000], Step: [300/301], Loss: 0.1810, Dev: 0.7500\n",
            "Epoch: [522/1000], Step: [100/301], Loss: 0.2195, Dev: 0.7640\n",
            "Epoch: [522/1000], Step: [200/301], Loss: 0.1969, Dev: 0.7430\n",
            "Epoch: [522/1000], Step: [300/301], Loss: 0.1628, Dev: 0.7380\n",
            "Epoch: [523/1000], Step: [100/301], Loss: 0.1481, Dev: 0.7430\n",
            "Epoch: [523/1000], Step: [200/301], Loss: 0.2085, Dev: 0.7460\n",
            "Epoch: [523/1000], Step: [300/301], Loss: 0.1136, Dev: 0.7420\n",
            "Epoch: [524/1000], Step: [100/301], Loss: 0.2066, Dev: 0.7560\n",
            "Epoch: [524/1000], Step: [200/301], Loss: 0.1548, Dev: 0.7520\n",
            "Epoch: [524/1000], Step: [300/301], Loss: 0.1358, Dev: 0.7490\n",
            "Epoch: [525/1000], Step: [100/301], Loss: 0.3062, Dev: 0.7570\n",
            "Epoch: [525/1000], Step: [200/301], Loss: 0.2648, Dev: 0.7540\n",
            "Epoch: [525/1000], Step: [300/301], Loss: 0.1511, Dev: 0.7410\n",
            "Epoch: [526/1000], Step: [100/301], Loss: 0.2010, Dev: 0.7450\n",
            "Epoch: [526/1000], Step: [200/301], Loss: 0.2355, Dev: 0.7370\n",
            "Epoch: [526/1000], Step: [300/301], Loss: 0.2254, Dev: 0.7610\n",
            "Epoch: [527/1000], Step: [100/301], Loss: 0.1860, Dev: 0.7470\n",
            "Epoch: [527/1000], Step: [200/301], Loss: 0.2167, Dev: 0.7330\n",
            "Epoch: [527/1000], Step: [300/301], Loss: 0.1815, Dev: 0.7610\n",
            "Epoch: [528/1000], Step: [100/301], Loss: 0.1905, Dev: 0.7250\n",
            "Epoch: [528/1000], Step: [200/301], Loss: 0.3144, Dev: 0.7530\n",
            "Epoch: [528/1000], Step: [300/301], Loss: 0.1443, Dev: 0.7560\n",
            "Epoch: [529/1000], Step: [100/301], Loss: 0.2386, Dev: 0.7440\n",
            "Epoch: [529/1000], Step: [200/301], Loss: 0.2302, Dev: 0.7680\n",
            "Epoch: [529/1000], Step: [300/301], Loss: 0.2374, Dev: 0.7460\n",
            "Epoch: [530/1000], Step: [100/301], Loss: 0.3042, Dev: 0.7390\n",
            "Epoch: [530/1000], Step: [200/301], Loss: 0.2139, Dev: 0.7730\n",
            "Epoch: [530/1000], Step: [300/301], Loss: 0.2558, Dev: 0.7500\n",
            "Epoch: [531/1000], Step: [100/301], Loss: 0.1695, Dev: 0.7430\n",
            "Epoch: [531/1000], Step: [200/301], Loss: 0.2321, Dev: 0.7600\n",
            "Epoch: [531/1000], Step: [300/301], Loss: 0.2835, Dev: 0.7430\n",
            "Epoch: [532/1000], Step: [100/301], Loss: 0.1912, Dev: 0.7390\n",
            "Epoch: [532/1000], Step: [200/301], Loss: 0.3416, Dev: 0.7470\n",
            "Epoch: [532/1000], Step: [300/301], Loss: 0.1980, Dev: 0.7580\n",
            "Epoch: [533/1000], Step: [100/301], Loss: 0.2095, Dev: 0.7580\n",
            "Epoch: [533/1000], Step: [200/301], Loss: 0.2097, Dev: 0.7430\n",
            "Epoch: [533/1000], Step: [300/301], Loss: 0.2111, Dev: 0.7300\n",
            "Epoch: [534/1000], Step: [100/301], Loss: 0.1322, Dev: 0.7490\n",
            "Epoch: [534/1000], Step: [200/301], Loss: 0.1700, Dev: 0.7430\n",
            "Epoch: [534/1000], Step: [300/301], Loss: 0.2518, Dev: 0.7500\n",
            "Epoch: [535/1000], Step: [100/301], Loss: 0.1780, Dev: 0.7530\n",
            "Epoch: [535/1000], Step: [200/301], Loss: 0.2237, Dev: 0.7450\n",
            "Epoch: [535/1000], Step: [300/301], Loss: 0.1829, Dev: 0.7350\n",
            "Epoch: [536/1000], Step: [100/301], Loss: 0.3874, Dev: 0.7290\n",
            "Epoch: [536/1000], Step: [200/301], Loss: 0.2251, Dev: 0.7680\n",
            "Epoch: [536/1000], Step: [300/301], Loss: 0.1477, Dev: 0.7280\n",
            "Epoch: [537/1000], Step: [100/301], Loss: 0.1817, Dev: 0.7430\n",
            "Epoch: [537/1000], Step: [200/301], Loss: 0.1376, Dev: 0.7440\n",
            "Epoch: [537/1000], Step: [300/301], Loss: 0.1562, Dev: 0.7450\n",
            "Epoch: [538/1000], Step: [100/301], Loss: 0.1602, Dev: 0.7480\n",
            "Epoch: [538/1000], Step: [200/301], Loss: 0.2186, Dev: 0.7490\n",
            "Epoch: [538/1000], Step: [300/301], Loss: 0.1311, Dev: 0.7490\n",
            "Epoch: [539/1000], Step: [100/301], Loss: 0.1615, Dev: 0.7510\n",
            "Epoch: [539/1000], Step: [200/301], Loss: 0.2426, Dev: 0.7510\n",
            "Epoch: [539/1000], Step: [300/301], Loss: 0.1714, Dev: 0.7490\n",
            "Epoch: [540/1000], Step: [100/301], Loss: 0.2293, Dev: 0.7410\n",
            "Epoch: [540/1000], Step: [200/301], Loss: 0.1806, Dev: 0.7600\n",
            "Epoch: [540/1000], Step: [300/301], Loss: 0.1939, Dev: 0.7570\n",
            "Epoch: [541/1000], Step: [100/301], Loss: 0.2062, Dev: 0.7430\n",
            "Epoch: [541/1000], Step: [200/301], Loss: 0.1364, Dev: 0.7620\n",
            "Epoch: [541/1000], Step: [300/301], Loss: 0.2302, Dev: 0.7430\n",
            "Epoch: [542/1000], Step: [100/301], Loss: 0.1397, Dev: 0.7530\n",
            "Epoch: [542/1000], Step: [200/301], Loss: 0.1806, Dev: 0.7390\n",
            "Epoch: [542/1000], Step: [300/301], Loss: 0.1898, Dev: 0.7440\n",
            "Epoch: [543/1000], Step: [100/301], Loss: 0.1381, Dev: 0.7470\n",
            "Epoch: [543/1000], Step: [200/301], Loss: 0.3322, Dev: 0.7350\n",
            "Epoch: [543/1000], Step: [300/301], Loss: 0.1072, Dev: 0.7400\n",
            "Epoch: [544/1000], Step: [100/301], Loss: 0.2243, Dev: 0.7390\n",
            "Epoch: [544/1000], Step: [200/301], Loss: 0.1394, Dev: 0.7500\n",
            "Epoch: [544/1000], Step: [300/301], Loss: 0.1738, Dev: 0.7470\n",
            "Epoch: [545/1000], Step: [100/301], Loss: 0.1564, Dev: 0.7450\n",
            "Epoch: [545/1000], Step: [200/301], Loss: 0.1687, Dev: 0.7600\n",
            "Epoch: [545/1000], Step: [300/301], Loss: 0.1920, Dev: 0.7470\n",
            "Epoch: [546/1000], Step: [100/301], Loss: 0.2000, Dev: 0.7410\n",
            "Epoch: [546/1000], Step: [200/301], Loss: 0.1611, Dev: 0.7400\n",
            "Epoch: [546/1000], Step: [300/301], Loss: 0.1402, Dev: 0.7530\n",
            "Epoch: [547/1000], Step: [100/301], Loss: 0.1463, Dev: 0.7560\n",
            "Epoch: [547/1000], Step: [200/301], Loss: 0.1598, Dev: 0.7350\n",
            "Epoch: [547/1000], Step: [300/301], Loss: 0.1557, Dev: 0.7410\n",
            "Epoch: [548/1000], Step: [100/301], Loss: 0.1574, Dev: 0.7550\n",
            "Epoch: [548/1000], Step: [200/301], Loss: 0.2314, Dev: 0.7330\n",
            "Epoch: [548/1000], Step: [300/301], Loss: 0.2233, Dev: 0.7490\n",
            "Epoch: [549/1000], Step: [100/301], Loss: 0.1816, Dev: 0.7390\n",
            "Epoch: [549/1000], Step: [200/301], Loss: 0.1682, Dev: 0.7680\n",
            "Epoch: [549/1000], Step: [300/301], Loss: 0.1007, Dev: 0.7640\n",
            "Epoch: [550/1000], Step: [100/301], Loss: 0.1904, Dev: 0.7540\n",
            "Epoch: [550/1000], Step: [200/301], Loss: 0.1653, Dev: 0.7480\n",
            "Epoch: [550/1000], Step: [300/301], Loss: 0.1936, Dev: 0.7430\n",
            "Epoch: [551/1000], Step: [100/301], Loss: 0.1420, Dev: 0.7540\n",
            "Epoch: [551/1000], Step: [200/301], Loss: 0.2068, Dev: 0.7480\n",
            "Epoch: [551/1000], Step: [300/301], Loss: 0.1860, Dev: 0.7580\n",
            "Epoch: [552/1000], Step: [100/301], Loss: 0.2749, Dev: 0.7520\n",
            "Epoch: [552/1000], Step: [200/301], Loss: 0.2949, Dev: 0.7600\n",
            "Epoch: [552/1000], Step: [300/301], Loss: 0.1830, Dev: 0.7560\n",
            "Epoch: [553/1000], Step: [100/301], Loss: 0.2150, Dev: 0.7520\n",
            "Epoch: [553/1000], Step: [200/301], Loss: 0.2184, Dev: 0.7240\n",
            "Epoch: [553/1000], Step: [300/301], Loss: 0.2556, Dev: 0.7370\n",
            "Epoch: [554/1000], Step: [100/301], Loss: 0.2243, Dev: 0.7570\n",
            "Epoch: [554/1000], Step: [200/301], Loss: 0.1741, Dev: 0.7410\n",
            "Epoch: [554/1000], Step: [300/301], Loss: 0.1216, Dev: 0.7510\n",
            "Epoch: [555/1000], Step: [100/301], Loss: 0.1937, Dev: 0.7550\n",
            "Epoch: [555/1000], Step: [200/301], Loss: 0.2293, Dev: 0.7440\n",
            "Epoch: [555/1000], Step: [300/301], Loss: 0.2659, Dev: 0.7330\n",
            "Epoch: [556/1000], Step: [100/301], Loss: 0.1107, Dev: 0.7470\n",
            "Epoch: [556/1000], Step: [200/301], Loss: 0.1041, Dev: 0.7090\n",
            "Epoch: [556/1000], Step: [300/301], Loss: 0.1342, Dev: 0.7320\n",
            "Epoch: [557/1000], Step: [100/301], Loss: 0.2385, Dev: 0.7490\n",
            "Epoch: [557/1000], Step: [200/301], Loss: 0.1757, Dev: 0.7270\n",
            "Epoch: [557/1000], Step: [300/301], Loss: 0.3337, Dev: 0.7450\n",
            "Epoch: [558/1000], Step: [100/301], Loss: 0.0965, Dev: 0.7360\n",
            "Epoch: [558/1000], Step: [200/301], Loss: 0.1787, Dev: 0.7520\n",
            "Epoch: [558/1000], Step: [300/301], Loss: 0.1361, Dev: 0.7480\n",
            "Epoch: [559/1000], Step: [100/301], Loss: 0.2354, Dev: 0.7420\n",
            "Epoch: [559/1000], Step: [200/301], Loss: 0.1688, Dev: 0.7500\n",
            "Epoch: [559/1000], Step: [300/301], Loss: 0.1289, Dev: 0.7690\n",
            "Epoch: [560/1000], Step: [100/301], Loss: 0.2087, Dev: 0.7620\n",
            "Epoch: [560/1000], Step: [200/301], Loss: 0.2388, Dev: 0.7580\n",
            "Epoch: [560/1000], Step: [300/301], Loss: 0.0646, Dev: 0.7480\n",
            "Epoch: [561/1000], Step: [100/301], Loss: 0.1324, Dev: 0.7340\n",
            "Epoch: [561/1000], Step: [200/301], Loss: 0.1471, Dev: 0.7380\n",
            "Epoch: [561/1000], Step: [300/301], Loss: 0.1398, Dev: 0.7640\n",
            "Epoch: [562/1000], Step: [100/301], Loss: 0.2176, Dev: 0.7630\n",
            "Epoch: [562/1000], Step: [200/301], Loss: 0.2564, Dev: 0.7370\n",
            "Epoch: [562/1000], Step: [300/301], Loss: 0.1787, Dev: 0.7580\n",
            "Epoch: [563/1000], Step: [100/301], Loss: 0.1917, Dev: 0.7720\n",
            "Epoch: [563/1000], Step: [200/301], Loss: 0.2864, Dev: 0.7530\n",
            "Epoch: [563/1000], Step: [300/301], Loss: 0.1530, Dev: 0.7510\n",
            "Epoch: [564/1000], Step: [100/301], Loss: 0.2663, Dev: 0.7640\n",
            "Epoch: [564/1000], Step: [200/301], Loss: 0.1228, Dev: 0.7380\n",
            "Epoch: [564/1000], Step: [300/301], Loss: 0.1528, Dev: 0.7300\n",
            "Epoch: [565/1000], Step: [100/301], Loss: 0.1793, Dev: 0.7330\n",
            "Epoch: [565/1000], Step: [200/301], Loss: 0.1362, Dev: 0.7510\n",
            "Epoch: [565/1000], Step: [300/301], Loss: 0.2285, Dev: 0.7450\n",
            "Epoch: [566/1000], Step: [100/301], Loss: 0.1721, Dev: 0.7470\n",
            "Epoch: [566/1000], Step: [200/301], Loss: 0.1273, Dev: 0.7390\n",
            "Epoch: [566/1000], Step: [300/301], Loss: 0.1826, Dev: 0.7240\n",
            "Epoch: [567/1000], Step: [100/301], Loss: 0.2045, Dev: 0.7600\n",
            "Epoch: [567/1000], Step: [200/301], Loss: 0.1784, Dev: 0.7460\n",
            "Epoch: [567/1000], Step: [300/301], Loss: 0.2450, Dev: 0.7470\n",
            "Epoch: [568/1000], Step: [100/301], Loss: 0.2480, Dev: 0.7530\n",
            "Epoch: [568/1000], Step: [200/301], Loss: 0.2764, Dev: 0.7650\n",
            "Epoch: [568/1000], Step: [300/301], Loss: 0.1109, Dev: 0.7510\n",
            "Epoch: [569/1000], Step: [100/301], Loss: 0.1710, Dev: 0.7540\n",
            "Epoch: [569/1000], Step: [200/301], Loss: 0.1304, Dev: 0.7750\n",
            "Epoch: [569/1000], Step: [300/301], Loss: 0.3163, Dev: 0.7560\n",
            "Epoch: [570/1000], Step: [100/301], Loss: 0.1571, Dev: 0.7380\n",
            "Epoch: [570/1000], Step: [200/301], Loss: 0.1435, Dev: 0.7530\n",
            "Epoch: [570/1000], Step: [300/301], Loss: 0.1201, Dev: 0.7310\n",
            "Epoch: [571/1000], Step: [100/301], Loss: 0.2482, Dev: 0.7550\n",
            "Epoch: [571/1000], Step: [200/301], Loss: 0.1000, Dev: 0.7380\n",
            "Epoch: [571/1000], Step: [300/301], Loss: 0.1988, Dev: 0.7450\n",
            "Epoch: [572/1000], Step: [100/301], Loss: 0.2982, Dev: 0.7330\n",
            "Epoch: [572/1000], Step: [200/301], Loss: 0.2779, Dev: 0.7510\n",
            "Epoch: [572/1000], Step: [300/301], Loss: 0.1426, Dev: 0.7280\n",
            "Epoch: [573/1000], Step: [100/301], Loss: 0.1890, Dev: 0.7340\n",
            "Epoch: [573/1000], Step: [200/301], Loss: 0.2783, Dev: 0.7440\n",
            "Epoch: [573/1000], Step: [300/301], Loss: 0.1667, Dev: 0.7650\n",
            "Epoch: [574/1000], Step: [100/301], Loss: 0.1688, Dev: 0.7410\n",
            "Epoch: [574/1000], Step: [200/301], Loss: 0.2299, Dev: 0.7480\n",
            "Epoch: [574/1000], Step: [300/301], Loss: 0.2859, Dev: 0.7500\n",
            "Epoch: [575/1000], Step: [100/301], Loss: 0.1442, Dev: 0.7500\n",
            "Epoch: [575/1000], Step: [200/301], Loss: 0.1288, Dev: 0.7350\n",
            "Epoch: [575/1000], Step: [300/301], Loss: 0.1431, Dev: 0.7340\n",
            "Epoch: [576/1000], Step: [100/301], Loss: 0.1200, Dev: 0.7380\n",
            "Epoch: [576/1000], Step: [200/301], Loss: 0.2809, Dev: 0.7630\n",
            "Epoch: [576/1000], Step: [300/301], Loss: 0.1723, Dev: 0.7400\n",
            "Epoch: [577/1000], Step: [100/301], Loss: 0.1911, Dev: 0.7350\n",
            "Epoch: [577/1000], Step: [200/301], Loss: 0.1311, Dev: 0.7440\n",
            "Epoch: [577/1000], Step: [300/301], Loss: 0.2656, Dev: 0.7620\n",
            "Epoch: [578/1000], Step: [100/301], Loss: 0.3891, Dev: 0.7400\n",
            "Epoch: [578/1000], Step: [200/301], Loss: 0.1365, Dev: 0.7390\n",
            "Epoch: [578/1000], Step: [300/301], Loss: 0.2286, Dev: 0.7430\n",
            "Epoch: [579/1000], Step: [100/301], Loss: 0.1470, Dev: 0.7340\n",
            "Epoch: [579/1000], Step: [200/301], Loss: 0.1549, Dev: 0.7660\n",
            "Epoch: [579/1000], Step: [300/301], Loss: 0.1004, Dev: 0.7460\n",
            "Epoch: [580/1000], Step: [100/301], Loss: 0.3132, Dev: 0.7430\n",
            "Epoch: [580/1000], Step: [200/301], Loss: 0.1158, Dev: 0.7330\n",
            "Epoch: [580/1000], Step: [300/301], Loss: 0.1610, Dev: 0.7520\n",
            "Epoch: [581/1000], Step: [100/301], Loss: 0.1711, Dev: 0.7450\n",
            "Epoch: [581/1000], Step: [200/301], Loss: 0.1558, Dev: 0.7350\n",
            "Epoch: [581/1000], Step: [300/301], Loss: 0.3115, Dev: 0.7500\n",
            "Epoch: [582/1000], Step: [100/301], Loss: 0.1863, Dev: 0.7210\n",
            "Epoch: [582/1000], Step: [200/301], Loss: 0.2486, Dev: 0.7510\n",
            "Epoch: [582/1000], Step: [300/301], Loss: 0.2450, Dev: 0.7530\n",
            "Epoch: [583/1000], Step: [100/301], Loss: 0.1388, Dev: 0.7380\n",
            "Epoch: [583/1000], Step: [200/301], Loss: 0.1581, Dev: 0.7400\n",
            "Epoch: [583/1000], Step: [300/301], Loss: 0.1832, Dev: 0.7320\n",
            "Epoch: [584/1000], Step: [100/301], Loss: 0.1705, Dev: 0.7220\n",
            "Epoch: [584/1000], Step: [200/301], Loss: 0.1856, Dev: 0.7360\n",
            "Epoch: [584/1000], Step: [300/301], Loss: 0.1084, Dev: 0.7430\n",
            "Epoch: [585/1000], Step: [100/301], Loss: 0.1378, Dev: 0.7300\n",
            "Epoch: [585/1000], Step: [200/301], Loss: 0.3357, Dev: 0.7260\n",
            "Epoch: [585/1000], Step: [300/301], Loss: 0.2165, Dev: 0.7490\n",
            "Epoch: [586/1000], Step: [100/301], Loss: 0.1360, Dev: 0.7250\n",
            "Epoch: [586/1000], Step: [200/301], Loss: 0.2731, Dev: 0.7540\n",
            "Epoch: [586/1000], Step: [300/301], Loss: 0.2932, Dev: 0.7330\n",
            "Epoch: [587/1000], Step: [100/301], Loss: 0.2429, Dev: 0.7370\n",
            "Epoch: [587/1000], Step: [200/301], Loss: 0.1507, Dev: 0.7710\n",
            "Epoch: [587/1000], Step: [300/301], Loss: 0.1937, Dev: 0.7440\n",
            "Epoch: [588/1000], Step: [100/301], Loss: 0.1454, Dev: 0.7480\n",
            "Epoch: [588/1000], Step: [200/301], Loss: 0.3124, Dev: 0.7370\n",
            "Epoch: [588/1000], Step: [300/301], Loss: 0.3965, Dev: 0.7480\n",
            "Epoch: [589/1000], Step: [100/301], Loss: 0.1792, Dev: 0.7600\n",
            "Epoch: [589/1000], Step: [200/301], Loss: 0.1942, Dev: 0.7680\n",
            "Epoch: [589/1000], Step: [300/301], Loss: 0.2266, Dev: 0.7670\n",
            "Epoch: [590/1000], Step: [100/301], Loss: 0.1304, Dev: 0.7210\n",
            "Epoch: [590/1000], Step: [200/301], Loss: 0.1601, Dev: 0.7540\n",
            "Epoch: [590/1000], Step: [300/301], Loss: 0.2696, Dev: 0.7390\n",
            "Epoch: [591/1000], Step: [100/301], Loss: 0.2263, Dev: 0.7400\n",
            "Epoch: [591/1000], Step: [200/301], Loss: 0.1637, Dev: 0.7210\n",
            "Epoch: [591/1000], Step: [300/301], Loss: 0.2552, Dev: 0.7350\n",
            "Epoch: [592/1000], Step: [100/301], Loss: 0.2081, Dev: 0.7420\n",
            "Epoch: [592/1000], Step: [200/301], Loss: 0.2165, Dev: 0.7280\n",
            "Epoch: [592/1000], Step: [300/301], Loss: 0.1215, Dev: 0.7450\n",
            "Epoch: [593/1000], Step: [100/301], Loss: 0.3571, Dev: 0.7510\n",
            "Epoch: [593/1000], Step: [200/301], Loss: 0.3195, Dev: 0.7390\n",
            "Epoch: [593/1000], Step: [300/301], Loss: 0.2494, Dev: 0.7410\n",
            "Epoch: [594/1000], Step: [100/301], Loss: 0.1597, Dev: 0.7460\n",
            "Epoch: [594/1000], Step: [200/301], Loss: 0.1818, Dev: 0.7630\n",
            "Epoch: [594/1000], Step: [300/301], Loss: 0.2092, Dev: 0.7470\n",
            "Epoch: [595/1000], Step: [100/301], Loss: 0.1851, Dev: 0.7420\n",
            "Epoch: [595/1000], Step: [200/301], Loss: 0.2561, Dev: 0.7390\n",
            "Epoch: [595/1000], Step: [300/301], Loss: 0.1525, Dev: 0.7320\n",
            "Epoch: [596/1000], Step: [100/301], Loss: 0.3200, Dev: 0.7400\n",
            "Epoch: [596/1000], Step: [200/301], Loss: 0.1737, Dev: 0.7400\n",
            "Epoch: [596/1000], Step: [300/301], Loss: 0.3407, Dev: 0.7590\n",
            "Epoch: [597/1000], Step: [100/301], Loss: 0.1998, Dev: 0.7550\n",
            "Epoch: [597/1000], Step: [200/301], Loss: 0.1339, Dev: 0.7300\n",
            "Epoch: [597/1000], Step: [300/301], Loss: 0.1504, Dev: 0.7320\n",
            "Epoch: [598/1000], Step: [100/301], Loss: 0.1792, Dev: 0.7400\n",
            "Epoch: [598/1000], Step: [200/301], Loss: 0.1626, Dev: 0.7480\n",
            "Epoch: [598/1000], Step: [300/301], Loss: 0.1791, Dev: 0.7470\n",
            "Epoch: [599/1000], Step: [100/301], Loss: 0.1519, Dev: 0.7410\n",
            "Epoch: [599/1000], Step: [200/301], Loss: 0.2381, Dev: 0.7210\n",
            "Epoch: [599/1000], Step: [300/301], Loss: 0.1474, Dev: 0.7410\n",
            "Epoch: [600/1000], Step: [100/301], Loss: 0.1803, Dev: 0.7370\n",
            "Epoch: [600/1000], Step: [200/301], Loss: 0.1340, Dev: 0.7550\n",
            "Epoch: [600/1000], Step: [300/301], Loss: 0.2509, Dev: 0.7430\n",
            "Epoch: [601/1000], Step: [100/301], Loss: 0.1439, Dev: 0.7500\n",
            "Epoch: [601/1000], Step: [200/301], Loss: 0.2210, Dev: 0.7580\n",
            "Epoch: [601/1000], Step: [300/301], Loss: 0.2606, Dev: 0.7330\n",
            "Epoch: [602/1000], Step: [100/301], Loss: 0.3764, Dev: 0.7420\n",
            "Epoch: [602/1000], Step: [200/301], Loss: 0.1547, Dev: 0.7530\n",
            "Epoch: [602/1000], Step: [300/301], Loss: 0.2920, Dev: 0.7450\n",
            "Epoch: [603/1000], Step: [100/301], Loss: 0.2113, Dev: 0.7380\n",
            "Epoch: [603/1000], Step: [200/301], Loss: 0.2096, Dev: 0.7240\n",
            "Epoch: [603/1000], Step: [300/301], Loss: 0.1609, Dev: 0.7480\n",
            "Epoch: [604/1000], Step: [100/301], Loss: 0.1413, Dev: 0.7310\n",
            "Epoch: [604/1000], Step: [200/301], Loss: 0.1570, Dev: 0.7630\n",
            "Epoch: [604/1000], Step: [300/301], Loss: 0.1424, Dev: 0.7350\n",
            "Epoch: [605/1000], Step: [100/301], Loss: 0.1969, Dev: 0.7250\n",
            "Epoch: [605/1000], Step: [200/301], Loss: 0.2197, Dev: 0.7360\n",
            "Epoch: [605/1000], Step: [300/301], Loss: 0.1599, Dev: 0.7290\n",
            "Epoch: [606/1000], Step: [100/301], Loss: 0.1772, Dev: 0.7480\n",
            "Epoch: [606/1000], Step: [200/301], Loss: 0.2231, Dev: 0.7570\n",
            "Epoch: [606/1000], Step: [300/301], Loss: 0.2240, Dev: 0.7600\n",
            "Epoch: [607/1000], Step: [100/301], Loss: 0.2465, Dev: 0.7480\n",
            "Epoch: [607/1000], Step: [200/301], Loss: 0.1129, Dev: 0.7540\n",
            "Epoch: [607/1000], Step: [300/301], Loss: 0.2306, Dev: 0.7370\n",
            "Epoch: [608/1000], Step: [100/301], Loss: 0.1668, Dev: 0.7490\n",
            "Epoch: [608/1000], Step: [200/301], Loss: 0.2378, Dev: 0.7460\n",
            "Epoch: [608/1000], Step: [300/301], Loss: 0.1252, Dev: 0.7440\n",
            "Epoch: [609/1000], Step: [100/301], Loss: 0.2007, Dev: 0.7530\n",
            "Epoch: [609/1000], Step: [200/301], Loss: 0.2587, Dev: 0.7510\n",
            "Epoch: [609/1000], Step: [300/301], Loss: 0.1268, Dev: 0.7100\n",
            "Epoch: [610/1000], Step: [100/301], Loss: 0.2464, Dev: 0.7290\n",
            "Epoch: [610/1000], Step: [200/301], Loss: 0.1732, Dev: 0.7570\n",
            "Epoch: [610/1000], Step: [300/301], Loss: 0.2050, Dev: 0.7660\n",
            "Epoch: [611/1000], Step: [100/301], Loss: 0.1506, Dev: 0.7470\n",
            "Epoch: [611/1000], Step: [200/301], Loss: 0.1766, Dev: 0.7270\n",
            "Epoch: [611/1000], Step: [300/301], Loss: 0.2450, Dev: 0.7430\n",
            "Epoch: [612/1000], Step: [100/301], Loss: 0.2822, Dev: 0.7470\n",
            "Epoch: [612/1000], Step: [200/301], Loss: 0.1916, Dev: 0.7340\n",
            "Epoch: [612/1000], Step: [300/301], Loss: 0.1646, Dev: 0.7440\n",
            "Epoch: [613/1000], Step: [100/301], Loss: 0.2019, Dev: 0.7510\n",
            "Epoch: [613/1000], Step: [200/301], Loss: 0.1806, Dev: 0.7450\n",
            "Epoch: [613/1000], Step: [300/301], Loss: 0.1744, Dev: 0.7450\n",
            "Epoch: [614/1000], Step: [100/301], Loss: 0.1690, Dev: 0.7240\n",
            "Epoch: [614/1000], Step: [200/301], Loss: 0.1362, Dev: 0.7810\n",
            "Epoch: [614/1000], Step: [300/301], Loss: 0.1263, Dev: 0.7420\n",
            "Epoch: [615/1000], Step: [100/301], Loss: 0.2712, Dev: 0.7240\n",
            "Epoch: [615/1000], Step: [200/301], Loss: 0.2049, Dev: 0.7380\n",
            "Epoch: [615/1000], Step: [300/301], Loss: 0.1923, Dev: 0.7630\n",
            "Epoch: [616/1000], Step: [100/301], Loss: 0.1994, Dev: 0.7300\n",
            "Epoch: [616/1000], Step: [200/301], Loss: 0.2903, Dev: 0.7600\n",
            "Epoch: [616/1000], Step: [300/301], Loss: 0.1685, Dev: 0.7600\n",
            "Epoch: [617/1000], Step: [100/301], Loss: 0.1849, Dev: 0.7480\n",
            "Epoch: [617/1000], Step: [200/301], Loss: 0.2381, Dev: 0.7570\n",
            "Epoch: [617/1000], Step: [300/301], Loss: 0.1661, Dev: 0.7520\n",
            "Epoch: [618/1000], Step: [100/301], Loss: 0.2002, Dev: 0.7410\n",
            "Epoch: [618/1000], Step: [200/301], Loss: 0.2989, Dev: 0.7430\n",
            "Epoch: [618/1000], Step: [300/301], Loss: 0.1446, Dev: 0.7400\n",
            "Epoch: [619/1000], Step: [100/301], Loss: 0.2359, Dev: 0.7310\n",
            "Epoch: [619/1000], Step: [200/301], Loss: 0.1364, Dev: 0.7450\n",
            "Epoch: [619/1000], Step: [300/301], Loss: 0.2030, Dev: 0.7550\n",
            "Epoch: [620/1000], Step: [100/301], Loss: 0.2049, Dev: 0.7220\n",
            "Epoch: [620/1000], Step: [200/301], Loss: 0.1871, Dev: 0.7520\n",
            "Epoch: [620/1000], Step: [300/301], Loss: 0.2911, Dev: 0.7530\n",
            "Epoch: [621/1000], Step: [100/301], Loss: 0.1180, Dev: 0.7490\n",
            "Epoch: [621/1000], Step: [200/301], Loss: 0.1311, Dev: 0.7230\n",
            "Epoch: [621/1000], Step: [300/301], Loss: 0.2208, Dev: 0.7380\n",
            "Epoch: [622/1000], Step: [100/301], Loss: 0.1559, Dev: 0.7550\n",
            "Epoch: [622/1000], Step: [200/301], Loss: 0.1952, Dev: 0.7650\n",
            "Epoch: [622/1000], Step: [300/301], Loss: 0.1808, Dev: 0.7320\n",
            "Epoch: [623/1000], Step: [100/301], Loss: 0.2907, Dev: 0.7310\n",
            "Epoch: [623/1000], Step: [200/301], Loss: 0.3132, Dev: 0.7300\n",
            "Epoch: [623/1000], Step: [300/301], Loss: 0.2891, Dev: 0.7540\n",
            "Epoch: [624/1000], Step: [100/301], Loss: 0.1738, Dev: 0.7420\n",
            "Epoch: [624/1000], Step: [200/301], Loss: 0.1362, Dev: 0.7530\n",
            "Epoch: [624/1000], Step: [300/301], Loss: 0.1290, Dev: 0.7330\n",
            "Epoch: [625/1000], Step: [100/301], Loss: 0.1097, Dev: 0.7270\n",
            "Epoch: [625/1000], Step: [200/301], Loss: 0.1659, Dev: 0.7330\n",
            "Epoch: [625/1000], Step: [300/301], Loss: 0.1964, Dev: 0.7450\n",
            "Epoch: [626/1000], Step: [100/301], Loss: 0.1864, Dev: 0.7550\n",
            "Epoch: [626/1000], Step: [200/301], Loss: 0.1963, Dev: 0.7550\n",
            "Epoch: [626/1000], Step: [300/301], Loss: 0.2815, Dev: 0.7310\n",
            "Epoch: [627/1000], Step: [100/301], Loss: 0.1573, Dev: 0.7520\n",
            "Epoch: [627/1000], Step: [200/301], Loss: 0.1770, Dev: 0.7340\n",
            "Epoch: [627/1000], Step: [300/301], Loss: 0.2311, Dev: 0.7420\n",
            "Epoch: [628/1000], Step: [100/301], Loss: 0.2491, Dev: 0.7360\n",
            "Epoch: [628/1000], Step: [200/301], Loss: 0.1649, Dev: 0.7660\n",
            "Epoch: [628/1000], Step: [300/301], Loss: 0.2233, Dev: 0.7460\n",
            "Epoch: [629/1000], Step: [100/301], Loss: 0.1230, Dev: 0.7450\n",
            "Epoch: [629/1000], Step: [200/301], Loss: 0.1789, Dev: 0.7500\n",
            "Epoch: [629/1000], Step: [300/301], Loss: 0.2230, Dev: 0.7350\n",
            "Epoch: [630/1000], Step: [100/301], Loss: 0.1645, Dev: 0.7400\n",
            "Epoch: [630/1000], Step: [200/301], Loss: 0.3296, Dev: 0.7350\n",
            "Epoch: [630/1000], Step: [300/301], Loss: 0.3086, Dev: 0.7320\n",
            "Epoch: [631/1000], Step: [100/301], Loss: 0.1547, Dev: 0.7350\n",
            "Epoch: [631/1000], Step: [200/301], Loss: 0.1857, Dev: 0.7300\n",
            "Epoch: [631/1000], Step: [300/301], Loss: 0.1551, Dev: 0.7560\n",
            "Epoch: [632/1000], Step: [100/301], Loss: 0.1424, Dev: 0.7380\n",
            "Epoch: [632/1000], Step: [200/301], Loss: 0.1516, Dev: 0.7080\n",
            "Epoch: [632/1000], Step: [300/301], Loss: 0.2391, Dev: 0.7470\n",
            "Epoch: [633/1000], Step: [100/301], Loss: 0.2341, Dev: 0.7520\n",
            "Epoch: [633/1000], Step: [200/301], Loss: 0.2467, Dev: 0.7530\n",
            "Epoch: [633/1000], Step: [300/301], Loss: 0.2461, Dev: 0.7280\n",
            "Epoch: [634/1000], Step: [100/301], Loss: 0.1662, Dev: 0.7310\n",
            "Epoch: [634/1000], Step: [200/301], Loss: 0.0964, Dev: 0.7420\n",
            "Epoch: [634/1000], Step: [300/301], Loss: 0.1623, Dev: 0.7550\n",
            "Epoch: [635/1000], Step: [100/301], Loss: 0.1299, Dev: 0.7370\n",
            "Epoch: [635/1000], Step: [200/301], Loss: 0.1720, Dev: 0.7460\n",
            "Epoch: [635/1000], Step: [300/301], Loss: 0.1639, Dev: 0.7470\n",
            "Epoch: [636/1000], Step: [100/301], Loss: 0.2730, Dev: 0.7370\n",
            "Epoch: [636/1000], Step: [200/301], Loss: 0.1469, Dev: 0.7580\n",
            "Epoch: [636/1000], Step: [300/301], Loss: 0.1858, Dev: 0.7340\n",
            "Epoch: [637/1000], Step: [100/301], Loss: 0.2806, Dev: 0.7280\n",
            "Epoch: [637/1000], Step: [200/301], Loss: 0.2859, Dev: 0.7410\n",
            "Epoch: [637/1000], Step: [300/301], Loss: 0.1668, Dev: 0.7580\n",
            "Epoch: [638/1000], Step: [100/301], Loss: 0.2153, Dev: 0.7480\n",
            "Epoch: [638/1000], Step: [200/301], Loss: 0.1716, Dev: 0.7330\n",
            "Epoch: [638/1000], Step: [300/301], Loss: 0.1483, Dev: 0.7360\n",
            "Epoch: [639/1000], Step: [100/301], Loss: 0.1660, Dev: 0.7380\n",
            "Epoch: [639/1000], Step: [200/301], Loss: 0.2872, Dev: 0.7560\n",
            "Epoch: [639/1000], Step: [300/301], Loss: 0.0974, Dev: 0.7270\n",
            "Epoch: [640/1000], Step: [100/301], Loss: 0.1368, Dev: 0.7250\n",
            "Epoch: [640/1000], Step: [200/301], Loss: 0.1617, Dev: 0.7580\n",
            "Epoch: [640/1000], Step: [300/301], Loss: 0.1402, Dev: 0.7490\n",
            "Epoch: [641/1000], Step: [100/301], Loss: 0.1602, Dev: 0.7600\n",
            "Epoch: [641/1000], Step: [200/301], Loss: 0.1211, Dev: 0.7350\n",
            "Epoch: [641/1000], Step: [300/301], Loss: 0.1277, Dev: 0.7240\n",
            "Epoch: [642/1000], Step: [100/301], Loss: 0.1552, Dev: 0.7360\n",
            "Epoch: [642/1000], Step: [200/301], Loss: 0.2504, Dev: 0.7440\n",
            "Epoch: [642/1000], Step: [300/301], Loss: 0.1470, Dev: 0.7400\n",
            "Epoch: [643/1000], Step: [100/301], Loss: 0.1188, Dev: 0.7400\n",
            "Epoch: [643/1000], Step: [200/301], Loss: 0.2681, Dev: 0.7580\n",
            "Epoch: [643/1000], Step: [300/301], Loss: 0.3086, Dev: 0.7370\n",
            "Epoch: [644/1000], Step: [100/301], Loss: 0.2109, Dev: 0.7290\n",
            "Epoch: [644/1000], Step: [200/301], Loss: 0.1863, Dev: 0.7350\n",
            "Epoch: [644/1000], Step: [300/301], Loss: 0.2454, Dev: 0.7430\n",
            "Epoch: [645/1000], Step: [100/301], Loss: 0.2094, Dev: 0.7250\n",
            "Epoch: [645/1000], Step: [200/301], Loss: 0.2749, Dev: 0.7580\n",
            "Epoch: [645/1000], Step: [300/301], Loss: 0.1253, Dev: 0.7360\n",
            "Epoch: [646/1000], Step: [100/301], Loss: 0.1408, Dev: 0.7210\n",
            "Epoch: [646/1000], Step: [200/301], Loss: 0.3635, Dev: 0.7550\n",
            "Epoch: [646/1000], Step: [300/301], Loss: 0.1084, Dev: 0.7360\n",
            "Epoch: [647/1000], Step: [100/301], Loss: 0.2564, Dev: 0.7280\n",
            "Epoch: [647/1000], Step: [200/301], Loss: 0.1450, Dev: 0.7460\n",
            "Epoch: [647/1000], Step: [300/301], Loss: 0.1829, Dev: 0.7430\n",
            "Epoch: [648/1000], Step: [100/301], Loss: 0.1249, Dev: 0.7520\n",
            "Epoch: [648/1000], Step: [200/301], Loss: 0.1695, Dev: 0.7450\n",
            "Epoch: [648/1000], Step: [300/301], Loss: 0.1368, Dev: 0.7440\n",
            "Epoch: [649/1000], Step: [100/301], Loss: 0.1304, Dev: 0.7540\n",
            "Epoch: [649/1000], Step: [200/301], Loss: 0.1183, Dev: 0.7560\n",
            "Epoch: [649/1000], Step: [300/301], Loss: 0.1629, Dev: 0.7450\n",
            "Epoch: [650/1000], Step: [100/301], Loss: 0.1437, Dev: 0.7270\n",
            "Epoch: [650/1000], Step: [200/301], Loss: 0.2005, Dev: 0.7290\n",
            "Epoch: [650/1000], Step: [300/301], Loss: 0.1329, Dev: 0.7490\n",
            "Epoch: [651/1000], Step: [100/301], Loss: 0.2317, Dev: 0.7380\n",
            "Epoch: [651/1000], Step: [200/301], Loss: 0.1271, Dev: 0.7390\n",
            "Epoch: [651/1000], Step: [300/301], Loss: 0.1792, Dev: 0.7450\n",
            "Epoch: [652/1000], Step: [100/301], Loss: 0.1647, Dev: 0.7450\n",
            "Epoch: [652/1000], Step: [200/301], Loss: 0.1569, Dev: 0.7370\n",
            "Epoch: [652/1000], Step: [300/301], Loss: 0.1594, Dev: 0.7400\n",
            "Epoch: [653/1000], Step: [100/301], Loss: 0.2876, Dev: 0.7560\n",
            "Epoch: [653/1000], Step: [200/301], Loss: 0.1597, Dev: 0.7580\n",
            "Epoch: [653/1000], Step: [300/301], Loss: 0.2562, Dev: 0.7300\n",
            "Epoch: [654/1000], Step: [100/301], Loss: 0.1481, Dev: 0.7420\n",
            "Epoch: [654/1000], Step: [200/301], Loss: 0.1768, Dev: 0.7430\n",
            "Epoch: [654/1000], Step: [300/301], Loss: 0.1532, Dev: 0.7490\n",
            "Epoch: [655/1000], Step: [100/301], Loss: 0.2101, Dev: 0.7580\n",
            "Epoch: [655/1000], Step: [200/301], Loss: 0.1506, Dev: 0.7400\n",
            "Epoch: [655/1000], Step: [300/301], Loss: 0.3084, Dev: 0.7300\n",
            "Epoch: [656/1000], Step: [100/301], Loss: 0.2229, Dev: 0.7550\n",
            "Epoch: [656/1000], Step: [200/301], Loss: 0.1500, Dev: 0.7390\n",
            "Epoch: [656/1000], Step: [300/301], Loss: 0.2286, Dev: 0.7510\n",
            "Epoch: [657/1000], Step: [100/301], Loss: 0.0903, Dev: 0.7540\n",
            "Epoch: [657/1000], Step: [200/301], Loss: 0.1795, Dev: 0.7330\n",
            "Epoch: [657/1000], Step: [300/301], Loss: 0.1041, Dev: 0.7320\n",
            "Epoch: [658/1000], Step: [100/301], Loss: 0.1888, Dev: 0.7490\n",
            "Epoch: [658/1000], Step: [200/301], Loss: 0.1867, Dev: 0.7350\n",
            "Epoch: [658/1000], Step: [300/301], Loss: 0.1282, Dev: 0.7590\n",
            "Epoch: [659/1000], Step: [100/301], Loss: 0.2090, Dev: 0.7270\n",
            "Epoch: [659/1000], Step: [200/301], Loss: 0.2711, Dev: 0.7380\n",
            "Epoch: [659/1000], Step: [300/301], Loss: 0.2221, Dev: 0.7550\n",
            "Epoch: [660/1000], Step: [100/301], Loss: 0.1952, Dev: 0.7580\n",
            "Epoch: [660/1000], Step: [200/301], Loss: 0.2831, Dev: 0.7470\n",
            "Epoch: [660/1000], Step: [300/301], Loss: 0.3127, Dev: 0.7410\n",
            "Epoch: [661/1000], Step: [100/301], Loss: 0.2137, Dev: 0.7510\n",
            "Epoch: [661/1000], Step: [200/301], Loss: 0.1934, Dev: 0.7360\n",
            "Epoch: [661/1000], Step: [300/301], Loss: 0.2202, Dev: 0.7330\n",
            "Epoch: [662/1000], Step: [100/301], Loss: 0.1907, Dev: 0.7510\n",
            "Epoch: [662/1000], Step: [200/301], Loss: 0.1245, Dev: 0.7530\n",
            "Epoch: [662/1000], Step: [300/301], Loss: 0.1670, Dev: 0.7200\n",
            "Epoch: [663/1000], Step: [100/301], Loss: 0.3310, Dev: 0.7280\n",
            "Epoch: [663/1000], Step: [200/301], Loss: 0.1022, Dev: 0.7500\n",
            "Epoch: [663/1000], Step: [300/301], Loss: 0.2226, Dev: 0.7470\n",
            "Epoch: [664/1000], Step: [100/301], Loss: 0.1637, Dev: 0.7340\n",
            "Epoch: [664/1000], Step: [200/301], Loss: 0.1691, Dev: 0.7590\n",
            "Epoch: [664/1000], Step: [300/301], Loss: 0.1392, Dev: 0.7400\n",
            "Epoch: [665/1000], Step: [100/301], Loss: 0.1035, Dev: 0.7300\n",
            "Epoch: [665/1000], Step: [200/301], Loss: 0.1613, Dev: 0.7500\n",
            "Epoch: [665/1000], Step: [300/301], Loss: 0.2264, Dev: 0.7320\n",
            "Epoch: [666/1000], Step: [100/301], Loss: 0.2088, Dev: 0.7210\n",
            "Epoch: [666/1000], Step: [200/301], Loss: 0.2396, Dev: 0.7440\n",
            "Epoch: [666/1000], Step: [300/301], Loss: 0.2136, Dev: 0.7260\n",
            "Epoch: [667/1000], Step: [100/301], Loss: 0.1947, Dev: 0.7360\n",
            "Epoch: [667/1000], Step: [200/301], Loss: 0.1355, Dev: 0.7610\n",
            "Epoch: [667/1000], Step: [300/301], Loss: 0.1608, Dev: 0.7540\n",
            "Epoch: [668/1000], Step: [100/301], Loss: 0.2482, Dev: 0.7420\n",
            "Epoch: [668/1000], Step: [200/301], Loss: 0.1818, Dev: 0.7580\n",
            "Epoch: [668/1000], Step: [300/301], Loss: 0.1837, Dev: 0.7550\n",
            "Epoch: [669/1000], Step: [100/301], Loss: 0.1954, Dev: 0.7280\n",
            "Epoch: [669/1000], Step: [200/301], Loss: 0.1567, Dev: 0.7420\n",
            "Epoch: [669/1000], Step: [300/301], Loss: 0.1126, Dev: 0.7550\n",
            "Epoch: [670/1000], Step: [100/301], Loss: 0.1580, Dev: 0.7230\n",
            "Epoch: [670/1000], Step: [200/301], Loss: 0.1439, Dev: 0.7310\n",
            "Epoch: [670/1000], Step: [300/301], Loss: 0.2638, Dev: 0.7450\n",
            "Epoch: [671/1000], Step: [100/301], Loss: 0.1220, Dev: 0.7590\n",
            "Epoch: [671/1000], Step: [200/301], Loss: 0.1213, Dev: 0.7400\n",
            "Epoch: [671/1000], Step: [300/301], Loss: 0.1494, Dev: 0.7340\n",
            "Epoch: [672/1000], Step: [100/301], Loss: 0.1579, Dev: 0.7410\n",
            "Epoch: [672/1000], Step: [200/301], Loss: 0.1770, Dev: 0.7470\n",
            "Epoch: [672/1000], Step: [300/301], Loss: 0.2593, Dev: 0.7140\n",
            "Epoch: [673/1000], Step: [100/301], Loss: 0.1097, Dev: 0.7400\n",
            "Epoch: [673/1000], Step: [200/301], Loss: 0.1204, Dev: 0.7290\n",
            "Epoch: [673/1000], Step: [300/301], Loss: 0.2483, Dev: 0.7460\n",
            "Epoch: [674/1000], Step: [100/301], Loss: 0.1777, Dev: 0.7370\n",
            "Epoch: [674/1000], Step: [200/301], Loss: 0.1994, Dev: 0.7370\n",
            "Epoch: [674/1000], Step: [300/301], Loss: 0.1726, Dev: 0.7550\n",
            "Epoch: [675/1000], Step: [100/301], Loss: 0.1465, Dev: 0.7480\n",
            "Epoch: [675/1000], Step: [200/301], Loss: 0.1520, Dev: 0.7310\n",
            "Epoch: [675/1000], Step: [300/301], Loss: 0.2135, Dev: 0.7410\n",
            "Epoch: [676/1000], Step: [100/301], Loss: 0.1129, Dev: 0.7130\n",
            "Epoch: [676/1000], Step: [200/301], Loss: 0.1558, Dev: 0.7320\n",
            "Epoch: [676/1000], Step: [300/301], Loss: 0.2235, Dev: 0.7490\n",
            "Epoch: [677/1000], Step: [100/301], Loss: 0.3192, Dev: 0.7200\n",
            "Epoch: [677/1000], Step: [200/301], Loss: 0.2263, Dev: 0.7520\n",
            "Epoch: [677/1000], Step: [300/301], Loss: 0.1080, Dev: 0.7380\n",
            "Epoch: [678/1000], Step: [100/301], Loss: 0.1273, Dev: 0.7350\n",
            "Epoch: [678/1000], Step: [200/301], Loss: 0.2220, Dev: 0.7310\n",
            "Epoch: [678/1000], Step: [300/301], Loss: 0.1491, Dev: 0.7500\n",
            "Epoch: [679/1000], Step: [100/301], Loss: 0.2428, Dev: 0.7380\n",
            "Epoch: [679/1000], Step: [200/301], Loss: 0.3232, Dev: 0.7370\n",
            "Epoch: [679/1000], Step: [300/301], Loss: 0.2459, Dev: 0.7440\n",
            "Epoch: [680/1000], Step: [100/301], Loss: 0.1929, Dev: 0.7500\n",
            "Epoch: [680/1000], Step: [200/301], Loss: 0.1759, Dev: 0.7360\n",
            "Epoch: [680/1000], Step: [300/301], Loss: 0.2575, Dev: 0.7280\n",
            "Epoch: [681/1000], Step: [100/301], Loss: 0.1916, Dev: 0.7640\n",
            "Epoch: [681/1000], Step: [200/301], Loss: 0.3254, Dev: 0.7270\n",
            "Epoch: [681/1000], Step: [300/301], Loss: 0.1885, Dev: 0.7410\n",
            "Epoch: [682/1000], Step: [100/301], Loss: 0.1670, Dev: 0.7200\n",
            "Epoch: [682/1000], Step: [200/301], Loss: 0.2155, Dev: 0.7560\n",
            "Epoch: [682/1000], Step: [300/301], Loss: 0.1148, Dev: 0.7500\n",
            "Epoch: [683/1000], Step: [100/301], Loss: 0.1028, Dev: 0.7400\n",
            "Epoch: [683/1000], Step: [200/301], Loss: 0.1870, Dev: 0.7410\n",
            "Epoch: [683/1000], Step: [300/301], Loss: 0.1297, Dev: 0.7200\n",
            "Epoch: [684/1000], Step: [100/301], Loss: 0.1861, Dev: 0.7590\n",
            "Epoch: [684/1000], Step: [200/301], Loss: 0.1676, Dev: 0.7440\n",
            "Epoch: [684/1000], Step: [300/301], Loss: 0.1908, Dev: 0.7210\n",
            "Epoch: [685/1000], Step: [100/301], Loss: 0.1957, Dev: 0.7470\n",
            "Epoch: [685/1000], Step: [200/301], Loss: 0.1811, Dev: 0.7330\n",
            "Epoch: [685/1000], Step: [300/301], Loss: 0.1478, Dev: 0.7620\n",
            "Epoch: [686/1000], Step: [100/301], Loss: 0.2529, Dev: 0.7160\n",
            "Epoch: [686/1000], Step: [200/301], Loss: 0.3179, Dev: 0.7560\n",
            "Epoch: [686/1000], Step: [300/301], Loss: 0.1266, Dev: 0.7190\n",
            "Epoch: [687/1000], Step: [100/301], Loss: 0.2266, Dev: 0.7490\n",
            "Epoch: [687/1000], Step: [200/301], Loss: 0.2598, Dev: 0.7550\n",
            "Epoch: [687/1000], Step: [300/301], Loss: 0.1255, Dev: 0.7280\n",
            "Epoch: [688/1000], Step: [100/301], Loss: 0.1849, Dev: 0.7310\n",
            "Epoch: [688/1000], Step: [200/301], Loss: 0.1757, Dev: 0.7620\n",
            "Epoch: [688/1000], Step: [300/301], Loss: 0.1730, Dev: 0.7250\n",
            "Epoch: [689/1000], Step: [100/301], Loss: 0.1282, Dev: 0.7420\n",
            "Epoch: [689/1000], Step: [200/301], Loss: 0.1688, Dev: 0.7340\n",
            "Epoch: [689/1000], Step: [300/301], Loss: 0.1447, Dev: 0.7170\n",
            "Epoch: [690/1000], Step: [100/301], Loss: 0.2435, Dev: 0.7470\n",
            "Epoch: [690/1000], Step: [200/301], Loss: 0.2627, Dev: 0.7240\n",
            "Epoch: [690/1000], Step: [300/301], Loss: 0.1358, Dev: 0.7600\n",
            "Epoch: [691/1000], Step: [100/301], Loss: 0.1586, Dev: 0.7380\n",
            "Epoch: [691/1000], Step: [200/301], Loss: 0.1592, Dev: 0.7370\n",
            "Epoch: [691/1000], Step: [300/301], Loss: 0.2414, Dev: 0.7410\n",
            "Epoch: [692/1000], Step: [100/301], Loss: 0.1181, Dev: 0.7090\n",
            "Epoch: [692/1000], Step: [200/301], Loss: 0.1538, Dev: 0.7270\n",
            "Epoch: [692/1000], Step: [300/301], Loss: 0.2376, Dev: 0.7390\n",
            "Epoch: [693/1000], Step: [100/301], Loss: 0.1273, Dev: 0.7400\n",
            "Epoch: [693/1000], Step: [200/301], Loss: 0.1755, Dev: 0.7310\n",
            "Epoch: [693/1000], Step: [300/301], Loss: 0.1786, Dev: 0.7290\n",
            "Epoch: [694/1000], Step: [100/301], Loss: 0.1180, Dev: 0.7360\n",
            "Epoch: [694/1000], Step: [200/301], Loss: 0.1492, Dev: 0.7540\n",
            "Epoch: [694/1000], Step: [300/301], Loss: 0.1362, Dev: 0.7460\n",
            "Epoch: [695/1000], Step: [100/301], Loss: 0.2284, Dev: 0.7420\n",
            "Epoch: [695/1000], Step: [200/301], Loss: 0.2320, Dev: 0.7240\n",
            "Epoch: [695/1000], Step: [300/301], Loss: 0.2130, Dev: 0.7410\n",
            "Epoch: [696/1000], Step: [100/301], Loss: 0.1907, Dev: 0.7020\n",
            "Epoch: [696/1000], Step: [200/301], Loss: 0.1286, Dev: 0.7540\n",
            "Epoch: [696/1000], Step: [300/301], Loss: 0.1366, Dev: 0.7490\n",
            "Epoch: [697/1000], Step: [100/301], Loss: 0.2187, Dev: 0.7400\n",
            "Epoch: [697/1000], Step: [200/301], Loss: 0.1284, Dev: 0.7340\n",
            "Epoch: [697/1000], Step: [300/301], Loss: 0.1776, Dev: 0.7580\n",
            "Epoch: [698/1000], Step: [100/301], Loss: 0.2414, Dev: 0.7690\n",
            "Epoch: [698/1000], Step: [200/301], Loss: 0.1138, Dev: 0.7170\n",
            "Epoch: [698/1000], Step: [300/301], Loss: 0.2577, Dev: 0.7520\n",
            "Epoch: [699/1000], Step: [100/301], Loss: 0.1537, Dev: 0.7150\n",
            "Epoch: [699/1000], Step: [200/301], Loss: 0.2188, Dev: 0.7400\n",
            "Epoch: [699/1000], Step: [300/301], Loss: 0.1923, Dev: 0.7550\n",
            "Epoch: [700/1000], Step: [100/301], Loss: 0.3247, Dev: 0.7390\n",
            "Epoch: [700/1000], Step: [200/301], Loss: 0.1937, Dev: 0.7190\n",
            "Epoch: [700/1000], Step: [300/301], Loss: 0.1980, Dev: 0.7450\n",
            "Epoch: [701/1000], Step: [100/301], Loss: 0.1672, Dev: 0.7370\n",
            "Epoch: [701/1000], Step: [200/301], Loss: 0.2451, Dev: 0.7220\n",
            "Epoch: [701/1000], Step: [300/301], Loss: 0.1919, Dev: 0.7490\n",
            "Epoch: [702/1000], Step: [100/301], Loss: 0.1597, Dev: 0.7410\n",
            "Epoch: [702/1000], Step: [200/301], Loss: 0.2444, Dev: 0.7350\n",
            "Epoch: [702/1000], Step: [300/301], Loss: 0.1052, Dev: 0.7480\n",
            "Epoch: [703/1000], Step: [100/301], Loss: 0.1423, Dev: 0.7450\n",
            "Epoch: [703/1000], Step: [200/301], Loss: 0.1085, Dev: 0.7410\n",
            "Epoch: [703/1000], Step: [300/301], Loss: 0.1810, Dev: 0.7390\n",
            "Epoch: [704/1000], Step: [100/301], Loss: 0.2946, Dev: 0.7270\n",
            "Epoch: [704/1000], Step: [200/301], Loss: 0.2322, Dev: 0.7400\n",
            "Epoch: [704/1000], Step: [300/301], Loss: 0.1847, Dev: 0.7420\n",
            "Epoch: [705/1000], Step: [100/301], Loss: 0.2911, Dev: 0.7490\n",
            "Epoch: [705/1000], Step: [200/301], Loss: 0.2706, Dev: 0.7320\n",
            "Epoch: [705/1000], Step: [300/301], Loss: 0.1852, Dev: 0.7340\n",
            "Epoch: [706/1000], Step: [100/301], Loss: 0.1974, Dev: 0.7370\n",
            "Epoch: [706/1000], Step: [200/301], Loss: 0.1449, Dev: 0.7310\n",
            "Epoch: [706/1000], Step: [300/301], Loss: 0.1502, Dev: 0.7550\n",
            "Epoch: [707/1000], Step: [100/301], Loss: 0.1299, Dev: 0.7380\n",
            "Epoch: [707/1000], Step: [200/301], Loss: 0.1347, Dev: 0.7350\n",
            "Epoch: [707/1000], Step: [300/301], Loss: 0.2989, Dev: 0.7400\n",
            "Epoch: [708/1000], Step: [100/301], Loss: 0.1730, Dev: 0.7330\n",
            "Epoch: [708/1000], Step: [200/301], Loss: 0.1741, Dev: 0.7510\n",
            "Epoch: [708/1000], Step: [300/301], Loss: 0.2302, Dev: 0.7250\n",
            "Epoch: [709/1000], Step: [100/301], Loss: 0.1912, Dev: 0.7490\n",
            "Epoch: [709/1000], Step: [200/301], Loss: 0.2891, Dev: 0.7360\n",
            "Epoch: [709/1000], Step: [300/301], Loss: 0.1507, Dev: 0.7480\n",
            "Epoch: [710/1000], Step: [100/301], Loss: 0.3809, Dev: 0.7690\n",
            "Epoch: [710/1000], Step: [200/301], Loss: 0.2098, Dev: 0.7200\n",
            "Epoch: [710/1000], Step: [300/301], Loss: 0.2358, Dev: 0.7430\n",
            "Epoch: [711/1000], Step: [100/301], Loss: 0.2246, Dev: 0.7070\n",
            "Epoch: [711/1000], Step: [200/301], Loss: 0.1433, Dev: 0.7520\n",
            "Epoch: [711/1000], Step: [300/301], Loss: 0.2197, Dev: 0.7420\n",
            "Epoch: [712/1000], Step: [100/301], Loss: 0.2973, Dev: 0.7500\n",
            "Epoch: [712/1000], Step: [200/301], Loss: 0.1972, Dev: 0.7360\n",
            "Epoch: [712/1000], Step: [300/301], Loss: 0.2358, Dev: 0.7370\n",
            "Epoch: [713/1000], Step: [100/301], Loss: 0.1510, Dev: 0.7190\n",
            "Epoch: [713/1000], Step: [200/301], Loss: 0.1829, Dev: 0.7430\n",
            "Epoch: [713/1000], Step: [300/301], Loss: 0.1782, Dev: 0.7240\n",
            "Epoch: [714/1000], Step: [100/301], Loss: 0.2769, Dev: 0.7320\n",
            "Epoch: [714/1000], Step: [200/301], Loss: 0.2003, Dev: 0.7550\n",
            "Epoch: [714/1000], Step: [300/301], Loss: 0.1742, Dev: 0.7540\n",
            "Epoch: [715/1000], Step: [100/301], Loss: 0.2267, Dev: 0.7270\n",
            "Epoch: [715/1000], Step: [200/301], Loss: 0.2395, Dev: 0.7410\n",
            "Epoch: [715/1000], Step: [300/301], Loss: 0.2079, Dev: 0.7260\n",
            "Epoch: [716/1000], Step: [100/301], Loss: 0.1282, Dev: 0.7370\n",
            "Epoch: [716/1000], Step: [200/301], Loss: 0.2529, Dev: 0.7380\n",
            "Epoch: [716/1000], Step: [300/301], Loss: 0.2020, Dev: 0.7260\n",
            "Epoch: [717/1000], Step: [100/301], Loss: 0.1941, Dev: 0.7440\n",
            "Epoch: [717/1000], Step: [200/301], Loss: 0.1796, Dev: 0.7380\n",
            "Epoch: [717/1000], Step: [300/301], Loss: 0.1958, Dev: 0.7350\n",
            "Epoch: [718/1000], Step: [100/301], Loss: 0.2280, Dev: 0.7460\n",
            "Epoch: [718/1000], Step: [200/301], Loss: 0.0890, Dev: 0.7130\n",
            "Epoch: [718/1000], Step: [300/301], Loss: 0.1204, Dev: 0.7450\n",
            "Epoch: [719/1000], Step: [100/301], Loss: 0.1639, Dev: 0.7250\n",
            "Epoch: [719/1000], Step: [200/301], Loss: 0.1548, Dev: 0.7260\n",
            "Epoch: [719/1000], Step: [300/301], Loss: 0.1270, Dev: 0.7650\n",
            "Epoch: [720/1000], Step: [100/301], Loss: 0.1077, Dev: 0.7240\n",
            "Epoch: [720/1000], Step: [200/301], Loss: 0.1598, Dev: 0.7500\n",
            "Epoch: [720/1000], Step: [300/301], Loss: 0.1417, Dev: 0.7460\n",
            "Epoch: [721/1000], Step: [100/301], Loss: 0.2543, Dev: 0.7380\n",
            "Epoch: [721/1000], Step: [200/301], Loss: 0.2214, Dev: 0.7410\n",
            "Epoch: [721/1000], Step: [300/301], Loss: 0.2244, Dev: 0.7400\n",
            "Epoch: [722/1000], Step: [100/301], Loss: 0.1837, Dev: 0.7370\n",
            "Epoch: [722/1000], Step: [200/301], Loss: 0.1470, Dev: 0.7430\n",
            "Epoch: [722/1000], Step: [300/301], Loss: 0.1702, Dev: 0.7280\n",
            "Epoch: [723/1000], Step: [100/301], Loss: 0.2013, Dev: 0.7470\n",
            "Epoch: [723/1000], Step: [200/301], Loss: 0.1029, Dev: 0.7540\n",
            "Epoch: [723/1000], Step: [300/301], Loss: 0.2947, Dev: 0.7130\n",
            "Epoch: [724/1000], Step: [100/301], Loss: 0.2006, Dev: 0.7320\n",
            "Epoch: [724/1000], Step: [200/301], Loss: 0.1746, Dev: 0.7250\n",
            "Epoch: [724/1000], Step: [300/301], Loss: 0.1854, Dev: 0.7150\n",
            "Epoch: [725/1000], Step: [100/301], Loss: 0.1879, Dev: 0.7460\n",
            "Epoch: [725/1000], Step: [200/301], Loss: 0.2260, Dev: 0.7400\n",
            "Epoch: [725/1000], Step: [300/301], Loss: 0.0937, Dev: 0.7250\n",
            "Epoch: [726/1000], Step: [100/301], Loss: 0.2340, Dev: 0.7160\n",
            "Epoch: [726/1000], Step: [200/301], Loss: 0.2015, Dev: 0.7420\n",
            "Epoch: [726/1000], Step: [300/301], Loss: 0.2590, Dev: 0.7420\n",
            "Epoch: [727/1000], Step: [100/301], Loss: 0.2613, Dev: 0.7440\n",
            "Epoch: [727/1000], Step: [200/301], Loss: 0.2164, Dev: 0.7250\n",
            "Epoch: [727/1000], Step: [300/301], Loss: 0.2351, Dev: 0.7600\n",
            "Epoch: [728/1000], Step: [100/301], Loss: 0.0890, Dev: 0.7350\n",
            "Epoch: [728/1000], Step: [200/301], Loss: 0.1492, Dev: 0.7280\n",
            "Epoch: [728/1000], Step: [300/301], Loss: 0.1423, Dev: 0.7270\n",
            "Epoch: [729/1000], Step: [100/301], Loss: 0.1074, Dev: 0.7390\n",
            "Epoch: [729/1000], Step: [200/301], Loss: 0.1375, Dev: 0.7200\n",
            "Epoch: [729/1000], Step: [300/301], Loss: 0.2811, Dev: 0.7170\n",
            "Epoch: [730/1000], Step: [100/301], Loss: 0.3378, Dev: 0.7380\n",
            "Epoch: [730/1000], Step: [200/301], Loss: 0.1338, Dev: 0.7320\n",
            "Epoch: [730/1000], Step: [300/301], Loss: 0.1626, Dev: 0.7450\n",
            "Epoch: [731/1000], Step: [100/301], Loss: 0.1581, Dev: 0.7280\n",
            "Epoch: [731/1000], Step: [200/301], Loss: 0.2102, Dev: 0.7290\n",
            "Epoch: [731/1000], Step: [300/301], Loss: 0.1665, Dev: 0.7450\n",
            "Epoch: [732/1000], Step: [100/301], Loss: 0.2423, Dev: 0.7300\n",
            "Epoch: [732/1000], Step: [200/301], Loss: 0.2262, Dev: 0.7580\n",
            "Epoch: [732/1000], Step: [300/301], Loss: 0.2241, Dev: 0.7250\n",
            "Epoch: [733/1000], Step: [100/301], Loss: 0.1926, Dev: 0.7370\n",
            "Epoch: [733/1000], Step: [200/301], Loss: 0.1684, Dev: 0.7400\n",
            "Epoch: [733/1000], Step: [300/301], Loss: 0.1793, Dev: 0.7230\n",
            "Epoch: [734/1000], Step: [100/301], Loss: 0.2367, Dev: 0.7170\n",
            "Epoch: [734/1000], Step: [200/301], Loss: 0.2176, Dev: 0.7240\n",
            "Epoch: [734/1000], Step: [300/301], Loss: 0.2173, Dev: 0.7590\n",
            "Epoch: [735/1000], Step: [100/301], Loss: 0.1973, Dev: 0.7470\n",
            "Epoch: [735/1000], Step: [200/301], Loss: 0.3075, Dev: 0.7350\n",
            "Epoch: [735/1000], Step: [300/301], Loss: 0.0961, Dev: 0.7360\n",
            "Epoch: [736/1000], Step: [100/301], Loss: 0.2511, Dev: 0.7400\n",
            "Epoch: [736/1000], Step: [200/301], Loss: 0.1311, Dev: 0.7520\n",
            "Epoch: [736/1000], Step: [300/301], Loss: 0.1869, Dev: 0.7260\n",
            "Epoch: [737/1000], Step: [100/301], Loss: 0.1837, Dev: 0.7490\n",
            "Epoch: [737/1000], Step: [200/301], Loss: 0.2473, Dev: 0.7260\n",
            "Epoch: [737/1000], Step: [300/301], Loss: 0.1423, Dev: 0.7340\n",
            "Epoch: [738/1000], Step: [100/301], Loss: 0.1532, Dev: 0.7470\n",
            "Epoch: [738/1000], Step: [200/301], Loss: 0.1705, Dev: 0.7510\n",
            "Epoch: [738/1000], Step: [300/301], Loss: 0.1556, Dev: 0.7270\n",
            "Epoch: [739/1000], Step: [100/301], Loss: 0.1257, Dev: 0.7460\n",
            "Epoch: [739/1000], Step: [200/301], Loss: 0.1344, Dev: 0.7220\n",
            "Epoch: [739/1000], Step: [300/301], Loss: 0.1716, Dev: 0.7220\n",
            "Epoch: [740/1000], Step: [100/301], Loss: 0.1065, Dev: 0.7450\n",
            "Epoch: [740/1000], Step: [200/301], Loss: 0.1705, Dev: 0.7410\n",
            "Epoch: [740/1000], Step: [300/301], Loss: 0.1883, Dev: 0.7370\n",
            "Epoch: [741/1000], Step: [100/301], Loss: 0.1164, Dev: 0.7510\n",
            "Epoch: [741/1000], Step: [200/301], Loss: 0.1725, Dev: 0.7310\n",
            "Epoch: [741/1000], Step: [300/301], Loss: 0.3455, Dev: 0.7170\n",
            "Epoch: [742/1000], Step: [100/301], Loss: 0.1392, Dev: 0.7430\n",
            "Epoch: [742/1000], Step: [200/301], Loss: 0.2184, Dev: 0.7350\n",
            "Epoch: [742/1000], Step: [300/301], Loss: 0.1129, Dev: 0.7280\n",
            "Epoch: [743/1000], Step: [100/301], Loss: 0.1876, Dev: 0.7340\n",
            "Epoch: [743/1000], Step: [200/301], Loss: 0.1807, Dev: 0.7280\n",
            "Epoch: [743/1000], Step: [300/301], Loss: 0.1455, Dev: 0.7550\n",
            "Epoch: [744/1000], Step: [100/301], Loss: 0.1305, Dev: 0.7310\n",
            "Epoch: [744/1000], Step: [200/301], Loss: 0.1797, Dev: 0.7390\n",
            "Epoch: [744/1000], Step: [300/301], Loss: 0.1942, Dev: 0.7470\n",
            "Epoch: [745/1000], Step: [100/301], Loss: 0.1765, Dev: 0.7160\n",
            "Epoch: [745/1000], Step: [200/301], Loss: 0.1943, Dev: 0.7260\n",
            "Epoch: [745/1000], Step: [300/301], Loss: 0.1922, Dev: 0.7330\n",
            "Epoch: [746/1000], Step: [100/301], Loss: 0.1880, Dev: 0.7220\n",
            "Epoch: [746/1000], Step: [200/301], Loss: 0.2801, Dev: 0.7350\n",
            "Epoch: [746/1000], Step: [300/301], Loss: 0.3307, Dev: 0.7200\n",
            "Epoch: [747/1000], Step: [100/301], Loss: 0.3018, Dev: 0.7220\n",
            "Epoch: [747/1000], Step: [200/301], Loss: 0.1360, Dev: 0.7430\n",
            "Epoch: [747/1000], Step: [300/301], Loss: 0.2077, Dev: 0.7150\n",
            "Epoch: [748/1000], Step: [100/301], Loss: 0.2322, Dev: 0.7350\n",
            "Epoch: [748/1000], Step: [200/301], Loss: 0.2891, Dev: 0.7480\n",
            "Epoch: [748/1000], Step: [300/301], Loss: 0.2020, Dev: 0.7280\n",
            "Epoch: [749/1000], Step: [100/301], Loss: 0.1498, Dev: 0.7240\n",
            "Epoch: [749/1000], Step: [200/301], Loss: 0.1603, Dev: 0.7330\n",
            "Epoch: [749/1000], Step: [300/301], Loss: 0.2461, Dev: 0.7470\n",
            "Epoch: [750/1000], Step: [100/301], Loss: 0.1765, Dev: 0.7460\n",
            "Epoch: [750/1000], Step: [200/301], Loss: 0.2929, Dev: 0.7430\n",
            "Epoch: [750/1000], Step: [300/301], Loss: 0.3525, Dev: 0.7180\n",
            "Epoch: [751/1000], Step: [100/301], Loss: 0.1162, Dev: 0.7300\n",
            "Epoch: [751/1000], Step: [200/301], Loss: 0.1683, Dev: 0.7400\n",
            "Epoch: [751/1000], Step: [300/301], Loss: 0.2722, Dev: 0.7270\n",
            "Epoch: [752/1000], Step: [100/301], Loss: 0.2370, Dev: 0.7370\n",
            "Epoch: [752/1000], Step: [200/301], Loss: 0.1178, Dev: 0.7120\n",
            "Epoch: [752/1000], Step: [300/301], Loss: 0.1702, Dev: 0.7450\n",
            "Epoch: [753/1000], Step: [100/301], Loss: 0.1621, Dev: 0.7090\n",
            "Epoch: [753/1000], Step: [200/301], Loss: 0.1995, Dev: 0.7370\n",
            "Epoch: [753/1000], Step: [300/301], Loss: 0.1527, Dev: 0.7310\n",
            "Epoch: [754/1000], Step: [100/301], Loss: 0.1705, Dev: 0.7410\n",
            "Epoch: [754/1000], Step: [200/301], Loss: 0.1185, Dev: 0.7270\n",
            "Epoch: [754/1000], Step: [300/301], Loss: 0.1416, Dev: 0.7250\n",
            "Epoch: [755/1000], Step: [100/301], Loss: 0.2103, Dev: 0.7140\n",
            "Epoch: [755/1000], Step: [200/301], Loss: 0.1832, Dev: 0.7270\n",
            "Epoch: [755/1000], Step: [300/301], Loss: 0.2399, Dev: 0.7270\n",
            "Epoch: [756/1000], Step: [100/301], Loss: 0.2648, Dev: 0.7510\n",
            "Epoch: [756/1000], Step: [200/301], Loss: 0.1765, Dev: 0.7340\n",
            "Epoch: [756/1000], Step: [300/301], Loss: 0.1894, Dev: 0.7090\n",
            "Epoch: [757/1000], Step: [100/301], Loss: 0.1375, Dev: 0.7400\n",
            "Epoch: [757/1000], Step: [200/301], Loss: 0.1555, Dev: 0.7220\n",
            "Epoch: [757/1000], Step: [300/301], Loss: 0.3828, Dev: 0.7470\n",
            "Epoch: [758/1000], Step: [100/301], Loss: 0.1820, Dev: 0.7320\n",
            "Epoch: [758/1000], Step: [200/301], Loss: 0.1999, Dev: 0.7340\n",
            "Epoch: [758/1000], Step: [300/301], Loss: 0.2050, Dev: 0.7380\n",
            "Epoch: [759/1000], Step: [100/301], Loss: 0.1483, Dev: 0.7480\n",
            "Epoch: [759/1000], Step: [200/301], Loss: 0.1754, Dev: 0.7410\n",
            "Epoch: [759/1000], Step: [300/301], Loss: 0.2335, Dev: 0.7150\n",
            "Epoch: [760/1000], Step: [100/301], Loss: 0.1010, Dev: 0.7330\n",
            "Epoch: [760/1000], Step: [200/301], Loss: 0.2453, Dev: 0.7250\n",
            "Epoch: [760/1000], Step: [300/301], Loss: 0.2472, Dev: 0.7380\n",
            "Epoch: [761/1000], Step: [100/301], Loss: 0.2256, Dev: 0.7380\n",
            "Epoch: [761/1000], Step: [200/301], Loss: 0.2040, Dev: 0.7440\n",
            "Epoch: [761/1000], Step: [300/301], Loss: 0.1068, Dev: 0.7370\n",
            "Epoch: [762/1000], Step: [100/301], Loss: 0.1331, Dev: 0.7460\n",
            "Epoch: [762/1000], Step: [200/301], Loss: 0.1715, Dev: 0.7220\n",
            "Epoch: [762/1000], Step: [300/301], Loss: 0.1101, Dev: 0.7300\n",
            "Epoch: [763/1000], Step: [100/301], Loss: 0.1953, Dev: 0.7300\n",
            "Epoch: [763/1000], Step: [200/301], Loss: 0.1128, Dev: 0.7350\n",
            "Epoch: [763/1000], Step: [300/301], Loss: 0.1779, Dev: 0.7370\n",
            "Epoch: [764/1000], Step: [100/301], Loss: 0.1558, Dev: 0.7490\n",
            "Epoch: [764/1000], Step: [200/301], Loss: 0.1777, Dev: 0.7570\n",
            "Epoch: [764/1000], Step: [300/301], Loss: 0.1604, Dev: 0.7110\n",
            "Epoch: [765/1000], Step: [100/301], Loss: 0.2225, Dev: 0.7470\n",
            "Epoch: [765/1000], Step: [200/301], Loss: 0.2801, Dev: 0.7680\n",
            "Epoch: [765/1000], Step: [300/301], Loss: 0.1829, Dev: 0.7470\n",
            "Epoch: [766/1000], Step: [100/301], Loss: 0.1648, Dev: 0.7240\n",
            "Epoch: [766/1000], Step: [200/301], Loss: 0.1194, Dev: 0.7620\n",
            "Epoch: [766/1000], Step: [300/301], Loss: 0.1325, Dev: 0.7110\n",
            "Epoch: [767/1000], Step: [100/301], Loss: 0.2186, Dev: 0.7350\n",
            "Epoch: [767/1000], Step: [200/301], Loss: 0.2511, Dev: 0.7560\n",
            "Epoch: [767/1000], Step: [300/301], Loss: 0.1180, Dev: 0.7550\n",
            "Epoch: [768/1000], Step: [100/301], Loss: 0.2319, Dev: 0.7460\n",
            "Epoch: [768/1000], Step: [200/301], Loss: 0.2726, Dev: 0.7550\n",
            "Epoch: [768/1000], Step: [300/301], Loss: 0.2598, Dev: 0.7360\n",
            "Epoch: [769/1000], Step: [100/301], Loss: 0.1962, Dev: 0.7330\n",
            "Epoch: [769/1000], Step: [200/301], Loss: 0.2412, Dev: 0.7280\n",
            "Epoch: [769/1000], Step: [300/301], Loss: 0.1368, Dev: 0.7330\n",
            "Epoch: [770/1000], Step: [100/301], Loss: 0.1445, Dev: 0.7420\n",
            "Epoch: [770/1000], Step: [200/301], Loss: 0.2615, Dev: 0.7280\n",
            "Epoch: [770/1000], Step: [300/301], Loss: 0.2039, Dev: 0.7540\n",
            "Epoch: [771/1000], Step: [100/301], Loss: 0.2819, Dev: 0.7460\n",
            "Epoch: [771/1000], Step: [200/301], Loss: 0.2425, Dev: 0.7460\n",
            "Epoch: [771/1000], Step: [300/301], Loss: 0.1807, Dev: 0.7400\n",
            "Epoch: [772/1000], Step: [100/301], Loss: 0.1812, Dev: 0.7360\n",
            "Epoch: [772/1000], Step: [200/301], Loss: 0.1773, Dev: 0.7410\n",
            "Epoch: [772/1000], Step: [300/301], Loss: 0.1509, Dev: 0.7520\n",
            "Epoch: [773/1000], Step: [100/301], Loss: 0.2102, Dev: 0.7470\n",
            "Epoch: [773/1000], Step: [200/301], Loss: 0.1675, Dev: 0.7250\n",
            "Epoch: [773/1000], Step: [300/301], Loss: 0.1378, Dev: 0.7320\n",
            "Epoch: [774/1000], Step: [100/301], Loss: 0.2276, Dev: 0.7440\n",
            "Epoch: [774/1000], Step: [200/301], Loss: 0.1840, Dev: 0.7450\n",
            "Epoch: [774/1000], Step: [300/301], Loss: 0.3276, Dev: 0.7470\n",
            "Epoch: [775/1000], Step: [100/301], Loss: 0.2896, Dev: 0.7500\n",
            "Epoch: [775/1000], Step: [200/301], Loss: 0.1529, Dev: 0.7180\n",
            "Epoch: [775/1000], Step: [300/301], Loss: 0.2546, Dev: 0.7510\n",
            "Epoch: [776/1000], Step: [100/301], Loss: 0.2368, Dev: 0.7290\n",
            "Epoch: [776/1000], Step: [200/301], Loss: 0.1999, Dev: 0.7320\n",
            "Epoch: [776/1000], Step: [300/301], Loss: 0.2265, Dev: 0.7370\n",
            "Epoch: [777/1000], Step: [100/301], Loss: 0.2830, Dev: 0.7530\n",
            "Epoch: [777/1000], Step: [200/301], Loss: 0.1957, Dev: 0.7380\n",
            "Epoch: [777/1000], Step: [300/301], Loss: 0.1509, Dev: 0.7250\n",
            "Epoch: [778/1000], Step: [100/301], Loss: 0.1193, Dev: 0.7110\n",
            "Epoch: [778/1000], Step: [200/301], Loss: 0.1527, Dev: 0.7280\n",
            "Epoch: [778/1000], Step: [300/301], Loss: 0.2038, Dev: 0.7210\n",
            "Epoch: [779/1000], Step: [100/301], Loss: 0.2159, Dev: 0.7300\n",
            "Epoch: [779/1000], Step: [200/301], Loss: 0.1347, Dev: 0.7380\n",
            "Epoch: [779/1000], Step: [300/301], Loss: 0.1391, Dev: 0.7590\n",
            "Epoch: [780/1000], Step: [100/301], Loss: 0.1519, Dev: 0.7410\n",
            "Epoch: [780/1000], Step: [200/301], Loss: 0.1600, Dev: 0.7450\n",
            "Epoch: [780/1000], Step: [300/301], Loss: 0.2748, Dev: 0.7490\n",
            "Epoch: [781/1000], Step: [100/301], Loss: 0.3232, Dev: 0.7360\n",
            "Epoch: [781/1000], Step: [200/301], Loss: 0.3400, Dev: 0.7190\n",
            "Epoch: [781/1000], Step: [300/301], Loss: 0.2705, Dev: 0.7380\n",
            "Epoch: [782/1000], Step: [100/301], Loss: 0.2098, Dev: 0.7470\n",
            "Epoch: [782/1000], Step: [200/301], Loss: 0.1199, Dev: 0.7020\n",
            "Epoch: [782/1000], Step: [300/301], Loss: 0.1806, Dev: 0.7460\n",
            "Epoch: [783/1000], Step: [100/301], Loss: 0.2161, Dev: 0.7340\n",
            "Epoch: [783/1000], Step: [200/301], Loss: 0.1264, Dev: 0.7530\n",
            "Epoch: [783/1000], Step: [300/301], Loss: 0.1586, Dev: 0.7220\n",
            "Epoch: [784/1000], Step: [100/301], Loss: 0.2832, Dev: 0.7260\n",
            "Epoch: [784/1000], Step: [200/301], Loss: 0.1258, Dev: 0.7300\n",
            "Epoch: [784/1000], Step: [300/301], Loss: 0.1954, Dev: 0.7460\n",
            "Epoch: [785/1000], Step: [100/301], Loss: 0.1165, Dev: 0.7240\n",
            "Epoch: [785/1000], Step: [200/301], Loss: 0.2965, Dev: 0.7450\n",
            "Epoch: [785/1000], Step: [300/301], Loss: 0.2502, Dev: 0.7300\n",
            "Epoch: [786/1000], Step: [100/301], Loss: 0.2151, Dev: 0.7630\n",
            "Epoch: [786/1000], Step: [200/301], Loss: 0.1550, Dev: 0.7100\n",
            "Epoch: [786/1000], Step: [300/301], Loss: 0.1303, Dev: 0.7120\n",
            "Epoch: [787/1000], Step: [100/301], Loss: 0.1798, Dev: 0.7280\n",
            "Epoch: [787/1000], Step: [200/301], Loss: 0.2390, Dev: 0.7490\n",
            "Epoch: [787/1000], Step: [300/301], Loss: 0.2072, Dev: 0.7490\n",
            "Epoch: [788/1000], Step: [100/301], Loss: 0.1249, Dev: 0.7300\n",
            "Epoch: [788/1000], Step: [200/301], Loss: 0.1292, Dev: 0.7590\n",
            "Epoch: [788/1000], Step: [300/301], Loss: 0.2129, Dev: 0.7240\n",
            "Epoch: [789/1000], Step: [100/301], Loss: 0.3087, Dev: 0.7070\n",
            "Epoch: [789/1000], Step: [200/301], Loss: 0.1196, Dev: 0.7390\n",
            "Epoch: [789/1000], Step: [300/301], Loss: 0.3096, Dev: 0.7100\n",
            "Epoch: [790/1000], Step: [100/301], Loss: 0.2032, Dev: 0.7220\n",
            "Epoch: [790/1000], Step: [200/301], Loss: 0.1653, Dev: 0.7340\n",
            "Epoch: [790/1000], Step: [300/301], Loss: 0.1917, Dev: 0.7360\n",
            "Epoch: [791/1000], Step: [100/301], Loss: 0.1298, Dev: 0.7150\n",
            "Epoch: [791/1000], Step: [200/301], Loss: 0.2423, Dev: 0.7520\n",
            "Epoch: [791/1000], Step: [300/301], Loss: 0.1607, Dev: 0.7190\n",
            "Epoch: [792/1000], Step: [100/301], Loss: 0.2314, Dev: 0.7480\n",
            "Epoch: [792/1000], Step: [200/301], Loss: 0.1485, Dev: 0.7270\n",
            "Epoch: [792/1000], Step: [300/301], Loss: 0.2469, Dev: 0.7270\n",
            "Epoch: [793/1000], Step: [100/301], Loss: 0.1828, Dev: 0.7460\n",
            "Epoch: [793/1000], Step: [200/301], Loss: 0.2178, Dev: 0.7430\n",
            "Epoch: [793/1000], Step: [300/301], Loss: 0.1511, Dev: 0.7400\n",
            "Epoch: [794/1000], Step: [100/301], Loss: 0.2110, Dev: 0.7470\n",
            "Epoch: [794/1000], Step: [200/301], Loss: 0.1695, Dev: 0.7180\n",
            "Epoch: [794/1000], Step: [300/301], Loss: 0.1605, Dev: 0.7250\n",
            "Epoch: [795/1000], Step: [100/301], Loss: 0.1975, Dev: 0.7460\n",
            "Epoch: [795/1000], Step: [200/301], Loss: 0.1913, Dev: 0.7420\n",
            "Epoch: [795/1000], Step: [300/301], Loss: 0.1484, Dev: 0.7160\n",
            "Epoch: [796/1000], Step: [100/301], Loss: 0.2358, Dev: 0.6990\n",
            "Epoch: [796/1000], Step: [200/301], Loss: 0.1784, Dev: 0.7440\n",
            "Epoch: [796/1000], Step: [300/301], Loss: 0.1185, Dev: 0.7350\n",
            "Epoch: [797/1000], Step: [100/301], Loss: 0.1469, Dev: 0.7360\n",
            "Epoch: [797/1000], Step: [200/301], Loss: 0.2933, Dev: 0.7140\n",
            "Epoch: [797/1000], Step: [300/301], Loss: 0.2151, Dev: 0.7180\n",
            "Epoch: [798/1000], Step: [100/301], Loss: 0.1596, Dev: 0.7380\n",
            "Epoch: [798/1000], Step: [200/301], Loss: 0.1768, Dev: 0.7290\n",
            "Epoch: [798/1000], Step: [300/301], Loss: 0.1794, Dev: 0.7370\n",
            "Epoch: [799/1000], Step: [100/301], Loss: 0.1823, Dev: 0.7170\n",
            "Epoch: [799/1000], Step: [200/301], Loss: 0.0774, Dev: 0.7490\n",
            "Epoch: [799/1000], Step: [300/301], Loss: 0.1960, Dev: 0.7050\n",
            "Epoch: [800/1000], Step: [100/301], Loss: 0.1308, Dev: 0.7260\n",
            "Epoch: [800/1000], Step: [200/301], Loss: 0.1555, Dev: 0.7390\n",
            "Epoch: [800/1000], Step: [300/301], Loss: 0.1251, Dev: 0.7180\n",
            "Epoch: [801/1000], Step: [100/301], Loss: 0.1075, Dev: 0.7360\n",
            "Epoch: [801/1000], Step: [200/301], Loss: 0.2322, Dev: 0.7200\n",
            "Epoch: [801/1000], Step: [300/301], Loss: 0.1267, Dev: 0.7490\n",
            "Epoch: [802/1000], Step: [100/301], Loss: 0.2152, Dev: 0.7180\n",
            "Epoch: [802/1000], Step: [200/301], Loss: 0.1077, Dev: 0.7400\n",
            "Epoch: [802/1000], Step: [300/301], Loss: 0.0963, Dev: 0.7470\n",
            "Epoch: [803/1000], Step: [100/301], Loss: 0.1540, Dev: 0.7260\n",
            "Epoch: [803/1000], Step: [200/301], Loss: 0.1640, Dev: 0.7510\n",
            "Epoch: [803/1000], Step: [300/301], Loss: 0.2366, Dev: 0.7450\n",
            "Epoch: [804/1000], Step: [100/301], Loss: 0.2240, Dev: 0.7470\n",
            "Epoch: [804/1000], Step: [200/301], Loss: 0.1921, Dev: 0.7230\n",
            "Epoch: [804/1000], Step: [300/301], Loss: 0.1328, Dev: 0.7340\n",
            "Epoch: [805/1000], Step: [100/301], Loss: 0.1425, Dev: 0.7270\n",
            "Epoch: [805/1000], Step: [200/301], Loss: 0.3024, Dev: 0.7450\n",
            "Epoch: [805/1000], Step: [300/301], Loss: 0.1603, Dev: 0.7230\n",
            "Epoch: [806/1000], Step: [100/301], Loss: 0.2789, Dev: 0.7130\n",
            "Epoch: [806/1000], Step: [200/301], Loss: 0.1465, Dev: 0.7420\n",
            "Epoch: [806/1000], Step: [300/301], Loss: 0.1379, Dev: 0.7550\n",
            "Epoch: [807/1000], Step: [100/301], Loss: 0.1894, Dev: 0.7530\n",
            "Epoch: [807/1000], Step: [200/301], Loss: 0.1447, Dev: 0.7380\n",
            "Epoch: [807/1000], Step: [300/301], Loss: 0.2113, Dev: 0.7290\n",
            "Epoch: [808/1000], Step: [100/301], Loss: 0.2760, Dev: 0.7120\n",
            "Epoch: [808/1000], Step: [200/301], Loss: 0.2342, Dev: 0.7450\n",
            "Epoch: [808/1000], Step: [300/301], Loss: 0.2947, Dev: 0.7580\n",
            "Epoch: [809/1000], Step: [100/301], Loss: 0.2407, Dev: 0.7300\n",
            "Epoch: [809/1000], Step: [200/301], Loss: 0.2589, Dev: 0.7170\n",
            "Epoch: [809/1000], Step: [300/301], Loss: 0.1268, Dev: 0.7210\n",
            "Epoch: [810/1000], Step: [100/301], Loss: 0.1939, Dev: 0.7590\n",
            "Epoch: [810/1000], Step: [200/301], Loss: 0.2208, Dev: 0.7280\n",
            "Epoch: [810/1000], Step: [300/301], Loss: 0.1393, Dev: 0.7080\n",
            "Epoch: [811/1000], Step: [100/301], Loss: 0.2093, Dev: 0.7350\n",
            "Epoch: [811/1000], Step: [200/301], Loss: 0.2104, Dev: 0.7200\n",
            "Epoch: [811/1000], Step: [300/301], Loss: 0.2725, Dev: 0.7620\n",
            "Epoch: [812/1000], Step: [100/301], Loss: 0.1466, Dev: 0.7340\n",
            "Epoch: [812/1000], Step: [200/301], Loss: 0.1476, Dev: 0.7180\n",
            "Epoch: [812/1000], Step: [300/301], Loss: 0.1960, Dev: 0.7270\n",
            "Epoch: [813/1000], Step: [100/301], Loss: 0.1549, Dev: 0.7300\n",
            "Epoch: [813/1000], Step: [200/301], Loss: 0.1618, Dev: 0.7070\n",
            "Epoch: [813/1000], Step: [300/301], Loss: 0.1899, Dev: 0.7350\n",
            "Epoch: [814/1000], Step: [100/301], Loss: 0.2337, Dev: 0.7550\n",
            "Epoch: [814/1000], Step: [200/301], Loss: 0.1735, Dev: 0.7290\n",
            "Epoch: [814/1000], Step: [300/301], Loss: 0.1812, Dev: 0.7410\n",
            "Epoch: [815/1000], Step: [100/301], Loss: 0.1600, Dev: 0.7730\n",
            "Epoch: [815/1000], Step: [200/301], Loss: 0.1727, Dev: 0.7180\n",
            "Epoch: [815/1000], Step: [300/301], Loss: 0.1168, Dev: 0.7320\n",
            "Epoch: [816/1000], Step: [100/301], Loss: 0.2190, Dev: 0.7200\n",
            "Epoch: [816/1000], Step: [200/301], Loss: 0.2738, Dev: 0.7300\n",
            "Epoch: [816/1000], Step: [300/301], Loss: 0.1912, Dev: 0.7280\n",
            "Epoch: [817/1000], Step: [100/301], Loss: 0.1912, Dev: 0.7290\n",
            "Epoch: [817/1000], Step: [200/301], Loss: 0.1928, Dev: 0.7440\n",
            "Epoch: [817/1000], Step: [300/301], Loss: 0.1849, Dev: 0.7210\n",
            "Epoch: [818/1000], Step: [100/301], Loss: 0.1574, Dev: 0.7160\n",
            "Epoch: [818/1000], Step: [200/301], Loss: 0.1823, Dev: 0.7480\n",
            "Epoch: [818/1000], Step: [300/301], Loss: 0.1694, Dev: 0.7360\n",
            "Epoch: [819/1000], Step: [100/301], Loss: 0.1315, Dev: 0.7470\n",
            "Epoch: [819/1000], Step: [200/301], Loss: 0.2268, Dev: 0.7330\n",
            "Epoch: [819/1000], Step: [300/301], Loss: 0.0980, Dev: 0.7090\n",
            "Epoch: [820/1000], Step: [100/301], Loss: 0.2160, Dev: 0.7310\n",
            "Epoch: [820/1000], Step: [200/301], Loss: 0.1081, Dev: 0.7520\n",
            "Epoch: [820/1000], Step: [300/301], Loss: 0.2129, Dev: 0.7160\n",
            "Epoch: [821/1000], Step: [100/301], Loss: 0.1318, Dev: 0.7400\n",
            "Epoch: [821/1000], Step: [200/301], Loss: 0.2624, Dev: 0.7190\n",
            "Epoch: [821/1000], Step: [300/301], Loss: 0.1594, Dev: 0.7160\n",
            "Epoch: [822/1000], Step: [100/301], Loss: 0.1212, Dev: 0.7170\n",
            "Epoch: [822/1000], Step: [200/301], Loss: 0.1737, Dev: 0.7330\n",
            "Epoch: [822/1000], Step: [300/301], Loss: 0.1508, Dev: 0.7160\n",
            "Epoch: [823/1000], Step: [100/301], Loss: 0.1193, Dev: 0.7430\n",
            "Epoch: [823/1000], Step: [200/301], Loss: 0.1721, Dev: 0.7570\n",
            "Epoch: [823/1000], Step: [300/301], Loss: 0.1410, Dev: 0.7220\n",
            "Epoch: [824/1000], Step: [100/301], Loss: 0.1946, Dev: 0.7200\n",
            "Epoch: [824/1000], Step: [200/301], Loss: 0.1939, Dev: 0.7470\n",
            "Epoch: [824/1000], Step: [300/301], Loss: 0.2110, Dev: 0.7270\n",
            "Epoch: [825/1000], Step: [100/301], Loss: 0.1934, Dev: 0.7150\n",
            "Epoch: [825/1000], Step: [200/301], Loss: 0.1773, Dev: 0.7380\n",
            "Epoch: [825/1000], Step: [300/301], Loss: 0.2113, Dev: 0.7190\n",
            "Epoch: [826/1000], Step: [100/301], Loss: 0.1118, Dev: 0.7350\n",
            "Epoch: [826/1000], Step: [200/301], Loss: 0.3658, Dev: 0.7230\n",
            "Epoch: [826/1000], Step: [300/301], Loss: 0.3095, Dev: 0.7190\n",
            "Epoch: [827/1000], Step: [100/301], Loss: 0.1338, Dev: 0.7690\n",
            "Epoch: [827/1000], Step: [200/301], Loss: 0.2399, Dev: 0.7070\n",
            "Epoch: [827/1000], Step: [300/301], Loss: 0.1198, Dev: 0.7110\n",
            "Epoch: [828/1000], Step: [100/301], Loss: 0.2247, Dev: 0.7450\n",
            "Epoch: [828/1000], Step: [200/301], Loss: 0.1835, Dev: 0.7120\n",
            "Epoch: [828/1000], Step: [300/301], Loss: 0.2973, Dev: 0.7450\n",
            "Epoch: [829/1000], Step: [100/301], Loss: 0.3104, Dev: 0.7330\n",
            "Epoch: [829/1000], Step: [200/301], Loss: 0.2211, Dev: 0.7530\n",
            "Epoch: [829/1000], Step: [300/301], Loss: 0.1681, Dev: 0.7350\n",
            "Epoch: [830/1000], Step: [100/301], Loss: 0.2647, Dev: 0.7280\n",
            "Epoch: [830/1000], Step: [200/301], Loss: 0.1886, Dev: 0.7090\n",
            "Epoch: [830/1000], Step: [300/301], Loss: 0.1310, Dev: 0.7610\n",
            "Epoch: [831/1000], Step: [100/301], Loss: 0.1884, Dev: 0.7460\n",
            "Epoch: [831/1000], Step: [200/301], Loss: 0.0958, Dev: 0.7280\n",
            "Epoch: [831/1000], Step: [300/301], Loss: 0.0874, Dev: 0.7240\n",
            "Epoch: [832/1000], Step: [100/301], Loss: 0.1431, Dev: 0.7470\n",
            "Epoch: [832/1000], Step: [200/301], Loss: 0.2737, Dev: 0.7230\n",
            "Epoch: [832/1000], Step: [300/301], Loss: 0.2347, Dev: 0.7300\n",
            "Epoch: [833/1000], Step: [100/301], Loss: 0.1923, Dev: 0.7410\n",
            "Epoch: [833/1000], Step: [200/301], Loss: 0.2735, Dev: 0.7200\n",
            "Epoch: [833/1000], Step: [300/301], Loss: 0.2336, Dev: 0.7090\n",
            "Epoch: [834/1000], Step: [100/301], Loss: 0.2147, Dev: 0.7500\n",
            "Epoch: [834/1000], Step: [200/301], Loss: 0.2286, Dev: 0.7180\n",
            "Epoch: [834/1000], Step: [300/301], Loss: 0.2249, Dev: 0.7260\n",
            "Epoch: [835/1000], Step: [100/301], Loss: 0.3010, Dev: 0.7420\n",
            "Epoch: [835/1000], Step: [200/301], Loss: 0.1557, Dev: 0.7330\n",
            "Epoch: [835/1000], Step: [300/301], Loss: 0.1257, Dev: 0.7130\n",
            "Epoch: [836/1000], Step: [100/301], Loss: 0.1526, Dev: 0.7270\n",
            "Epoch: [836/1000], Step: [200/301], Loss: 0.2011, Dev: 0.7390\n",
            "Epoch: [836/1000], Step: [300/301], Loss: 0.1853, Dev: 0.7230\n",
            "Epoch: [837/1000], Step: [100/301], Loss: 0.1618, Dev: 0.7360\n",
            "Epoch: [837/1000], Step: [200/301], Loss: 0.1914, Dev: 0.7260\n",
            "Epoch: [837/1000], Step: [300/301], Loss: 0.1948, Dev: 0.7490\n",
            "Epoch: [838/1000], Step: [100/301], Loss: 0.1646, Dev: 0.7410\n",
            "Epoch: [838/1000], Step: [200/301], Loss: 0.3181, Dev: 0.7380\n",
            "Epoch: [838/1000], Step: [300/301], Loss: 0.1548, Dev: 0.7180\n",
            "Epoch: [839/1000], Step: [100/301], Loss: 0.1417, Dev: 0.7340\n",
            "Epoch: [839/1000], Step: [200/301], Loss: 0.1347, Dev: 0.7370\n",
            "Epoch: [839/1000], Step: [300/301], Loss: 0.2295, Dev: 0.7280\n",
            "Epoch: [840/1000], Step: [100/301], Loss: 0.1059, Dev: 0.7060\n",
            "Epoch: [840/1000], Step: [200/301], Loss: 0.3385, Dev: 0.7470\n",
            "Epoch: [840/1000], Step: [300/301], Loss: 0.3070, Dev: 0.7230\n",
            "Epoch: [841/1000], Step: [100/301], Loss: 0.1205, Dev: 0.7490\n",
            "Epoch: [841/1000], Step: [200/301], Loss: 0.2219, Dev: 0.7510\n",
            "Epoch: [841/1000], Step: [300/301], Loss: 0.1576, Dev: 0.7220\n",
            "Epoch: [842/1000], Step: [100/301], Loss: 0.3118, Dev: 0.7360\n",
            "Epoch: [842/1000], Step: [200/301], Loss: 0.1628, Dev: 0.7270\n",
            "Epoch: [842/1000], Step: [300/301], Loss: 0.1647, Dev: 0.7140\n",
            "Epoch: [843/1000], Step: [100/301], Loss: 0.1481, Dev: 0.7310\n",
            "Epoch: [843/1000], Step: [200/301], Loss: 0.1297, Dev: 0.7180\n",
            "Epoch: [843/1000], Step: [300/301], Loss: 0.1306, Dev: 0.7070\n",
            "Epoch: [844/1000], Step: [100/301], Loss: 0.1588, Dev: 0.7450\n",
            "Epoch: [844/1000], Step: [200/301], Loss: 0.1379, Dev: 0.7280\n",
            "Epoch: [844/1000], Step: [300/301], Loss: 0.1705, Dev: 0.7430\n",
            "Epoch: [845/1000], Step: [100/301], Loss: 0.2059, Dev: 0.7160\n",
            "Epoch: [845/1000], Step: [200/301], Loss: 0.1572, Dev: 0.7430\n",
            "Epoch: [845/1000], Step: [300/301], Loss: 0.1922, Dev: 0.7460\n",
            "Epoch: [846/1000], Step: [100/301], Loss: 0.2129, Dev: 0.7260\n",
            "Epoch: [846/1000], Step: [200/301], Loss: 0.1625, Dev: 0.7410\n",
            "Epoch: [846/1000], Step: [300/301], Loss: 0.1701, Dev: 0.7290\n",
            "Epoch: [847/1000], Step: [100/301], Loss: 0.1776, Dev: 0.7020\n",
            "Epoch: [847/1000], Step: [200/301], Loss: 0.2718, Dev: 0.7460\n",
            "Epoch: [847/1000], Step: [300/301], Loss: 0.2028, Dev: 0.7480\n",
            "Epoch: [848/1000], Step: [100/301], Loss: 0.1665, Dev: 0.7650\n",
            "Epoch: [848/1000], Step: [200/301], Loss: 0.2257, Dev: 0.7230\n",
            "Epoch: [848/1000], Step: [300/301], Loss: 0.1549, Dev: 0.7190\n",
            "Epoch: [849/1000], Step: [100/301], Loss: 0.2263, Dev: 0.7220\n",
            "Epoch: [849/1000], Step: [200/301], Loss: 0.2357, Dev: 0.7370\n",
            "Epoch: [849/1000], Step: [300/301], Loss: 0.1639, Dev: 0.7430\n",
            "Epoch: [850/1000], Step: [100/301], Loss: 0.1594, Dev: 0.7330\n",
            "Epoch: [850/1000], Step: [200/301], Loss: 0.1608, Dev: 0.7240\n",
            "Epoch: [850/1000], Step: [300/301], Loss: 0.1466, Dev: 0.7210\n",
            "Epoch: [851/1000], Step: [100/301], Loss: 0.2263, Dev: 0.7400\n",
            "Epoch: [851/1000], Step: [200/301], Loss: 0.2656, Dev: 0.7360\n",
            "Epoch: [851/1000], Step: [300/301], Loss: 0.2712, Dev: 0.7240\n",
            "Epoch: [852/1000], Step: [100/301], Loss: 0.1325, Dev: 0.7210\n",
            "Epoch: [852/1000], Step: [200/301], Loss: 0.1705, Dev: 0.7250\n",
            "Epoch: [852/1000], Step: [300/301], Loss: 0.1940, Dev: 0.7020\n",
            "Epoch: [853/1000], Step: [100/301], Loss: 0.1843, Dev: 0.7170\n",
            "Epoch: [853/1000], Step: [200/301], Loss: 0.3242, Dev: 0.7490\n",
            "Epoch: [853/1000], Step: [300/301], Loss: 0.1658, Dev: 0.7200\n",
            "Epoch: [854/1000], Step: [100/301], Loss: 0.2073, Dev: 0.7190\n",
            "Epoch: [854/1000], Step: [200/301], Loss: 0.1196, Dev: 0.7330\n",
            "Epoch: [854/1000], Step: [300/301], Loss: 0.1418, Dev: 0.7320\n",
            "Epoch: [855/1000], Step: [100/301], Loss: 0.1015, Dev: 0.7380\n",
            "Epoch: [855/1000], Step: [200/301], Loss: 0.1959, Dev: 0.7370\n",
            "Epoch: [855/1000], Step: [300/301], Loss: 0.1678, Dev: 0.7080\n",
            "Epoch: [856/1000], Step: [100/301], Loss: 0.1652, Dev: 0.7200\n",
            "Epoch: [856/1000], Step: [200/301], Loss: 0.1782, Dev: 0.7250\n",
            "Epoch: [856/1000], Step: [300/301], Loss: 0.2184, Dev: 0.7190\n",
            "Epoch: [857/1000], Step: [100/301], Loss: 0.1345, Dev: 0.7390\n",
            "Epoch: [857/1000], Step: [200/301], Loss: 0.2026, Dev: 0.7120\n",
            "Epoch: [857/1000], Step: [300/301], Loss: 0.2007, Dev: 0.7090\n",
            "Epoch: [858/1000], Step: [100/301], Loss: 0.1600, Dev: 0.7150\n",
            "Epoch: [858/1000], Step: [200/301], Loss: 0.3199, Dev: 0.7170\n",
            "Epoch: [858/1000], Step: [300/301], Loss: 0.1862, Dev: 0.7460\n",
            "Epoch: [859/1000], Step: [100/301], Loss: 0.1368, Dev: 0.7300\n",
            "Epoch: [859/1000], Step: [200/301], Loss: 0.1513, Dev: 0.7240\n",
            "Epoch: [859/1000], Step: [300/301], Loss: 0.2693, Dev: 0.7350\n",
            "Epoch: [860/1000], Step: [100/301], Loss: 0.1195, Dev: 0.7200\n",
            "Epoch: [860/1000], Step: [200/301], Loss: 0.1929, Dev: 0.7320\n",
            "Epoch: [860/1000], Step: [300/301], Loss: 0.1948, Dev: 0.7460\n",
            "Epoch: [861/1000], Step: [100/301], Loss: 0.1205, Dev: 0.7400\n",
            "Epoch: [861/1000], Step: [200/301], Loss: 0.3544, Dev: 0.7140\n",
            "Epoch: [861/1000], Step: [300/301], Loss: 0.1636, Dev: 0.7080\n",
            "Epoch: [862/1000], Step: [100/301], Loss: 0.1936, Dev: 0.7210\n",
            "Epoch: [862/1000], Step: [200/301], Loss: 0.3457, Dev: 0.7460\n",
            "Epoch: [862/1000], Step: [300/301], Loss: 0.2477, Dev: 0.7490\n",
            "Epoch: [863/1000], Step: [100/301], Loss: 0.3146, Dev: 0.7130\n",
            "Epoch: [863/1000], Step: [200/301], Loss: 0.1721, Dev: 0.7390\n",
            "Epoch: [863/1000], Step: [300/301], Loss: 0.1986, Dev: 0.7460\n",
            "Epoch: [864/1000], Step: [100/301], Loss: 0.1519, Dev: 0.7480\n",
            "Epoch: [864/1000], Step: [200/301], Loss: 0.3126, Dev: 0.7350\n",
            "Epoch: [864/1000], Step: [300/301], Loss: 0.1639, Dev: 0.7210\n",
            "Epoch: [865/1000], Step: [100/301], Loss: 0.2326, Dev: 0.7250\n",
            "Epoch: [865/1000], Step: [200/301], Loss: 0.1516, Dev: 0.7450\n",
            "Epoch: [865/1000], Step: [300/301], Loss: 0.1432, Dev: 0.7210\n",
            "Epoch: [866/1000], Step: [100/301], Loss: 0.1473, Dev: 0.7310\n",
            "Epoch: [866/1000], Step: [200/301], Loss: 0.1738, Dev: 0.7380\n",
            "Epoch: [866/1000], Step: [300/301], Loss: 0.1960, Dev: 0.7340\n",
            "Epoch: [867/1000], Step: [100/301], Loss: 0.1171, Dev: 0.7200\n",
            "Epoch: [867/1000], Step: [200/301], Loss: 0.2112, Dev: 0.7120\n",
            "Epoch: [867/1000], Step: [300/301], Loss: 0.0895, Dev: 0.7450\n",
            "Epoch: [868/1000], Step: [100/301], Loss: 0.1918, Dev: 0.7220\n",
            "Epoch: [868/1000], Step: [200/301], Loss: 0.1969, Dev: 0.7210\n",
            "Epoch: [868/1000], Step: [300/301], Loss: 0.3129, Dev: 0.7210\n",
            "Epoch: [869/1000], Step: [100/301], Loss: 0.3177, Dev: 0.7220\n",
            "Epoch: [869/1000], Step: [200/301], Loss: 0.1686, Dev: 0.7540\n",
            "Epoch: [869/1000], Step: [300/301], Loss: 0.1806, Dev: 0.7340\n",
            "Epoch: [870/1000], Step: [100/301], Loss: 0.2251, Dev: 0.7320\n",
            "Epoch: [870/1000], Step: [200/301], Loss: 0.2155, Dev: 0.7390\n",
            "Epoch: [870/1000], Step: [300/301], Loss: 0.1392, Dev: 0.7170\n",
            "Epoch: [871/1000], Step: [100/301], Loss: 0.1786, Dev: 0.7140\n",
            "Epoch: [871/1000], Step: [200/301], Loss: 0.3449, Dev: 0.7460\n",
            "Epoch: [871/1000], Step: [300/301], Loss: 0.1451, Dev: 0.7410\n",
            "Epoch: [872/1000], Step: [100/301], Loss: 0.1316, Dev: 0.7270\n",
            "Epoch: [872/1000], Step: [200/301], Loss: 0.1248, Dev: 0.7370\n",
            "Epoch: [872/1000], Step: [300/301], Loss: 0.1714, Dev: 0.7030\n",
            "Epoch: [873/1000], Step: [100/301], Loss: 0.1860, Dev: 0.7550\n",
            "Epoch: [873/1000], Step: [200/301], Loss: 0.1598, Dev: 0.7120\n",
            "Epoch: [873/1000], Step: [300/301], Loss: 0.1661, Dev: 0.7150\n",
            "Epoch: [874/1000], Step: [100/301], Loss: 0.1234, Dev: 0.7260\n",
            "Epoch: [874/1000], Step: [200/301], Loss: 0.2362, Dev: 0.7190\n",
            "Epoch: [874/1000], Step: [300/301], Loss: 0.1434, Dev: 0.7210\n",
            "Epoch: [875/1000], Step: [100/301], Loss: 0.1471, Dev: 0.7140\n",
            "Epoch: [875/1000], Step: [200/301], Loss: 0.1487, Dev: 0.7280\n",
            "Epoch: [875/1000], Step: [300/301], Loss: 0.1539, Dev: 0.7110\n",
            "Epoch: [876/1000], Step: [100/301], Loss: 0.1915, Dev: 0.7320\n",
            "Epoch: [876/1000], Step: [200/301], Loss: 0.1050, Dev: 0.7330\n",
            "Epoch: [876/1000], Step: [300/301], Loss: 0.1626, Dev: 0.7280\n",
            "Epoch: [877/1000], Step: [100/301], Loss: 0.1648, Dev: 0.7200\n",
            "Epoch: [877/1000], Step: [200/301], Loss: 0.1509, Dev: 0.7280\n",
            "Epoch: [877/1000], Step: [300/301], Loss: 0.1601, Dev: 0.7170\n",
            "Epoch: [878/1000], Step: [100/301], Loss: 0.1248, Dev: 0.7280\n",
            "Epoch: [878/1000], Step: [200/301], Loss: 0.1402, Dev: 0.7200\n",
            "Epoch: [878/1000], Step: [300/301], Loss: 0.1208, Dev: 0.7170\n",
            "Epoch: [879/1000], Step: [100/301], Loss: 0.2011, Dev: 0.7150\n",
            "Epoch: [879/1000], Step: [200/301], Loss: 0.1826, Dev: 0.7390\n",
            "Epoch: [879/1000], Step: [300/301], Loss: 0.1304, Dev: 0.7330\n",
            "Epoch: [880/1000], Step: [100/301], Loss: 0.1896, Dev: 0.7350\n",
            "Epoch: [880/1000], Step: [200/301], Loss: 0.2290, Dev: 0.7510\n",
            "Epoch: [880/1000], Step: [300/301], Loss: 0.1098, Dev: 0.7210\n",
            "Epoch: [881/1000], Step: [100/301], Loss: 0.1178, Dev: 0.7340\n",
            "Epoch: [881/1000], Step: [200/301], Loss: 0.2421, Dev: 0.7210\n",
            "Epoch: [881/1000], Step: [300/301], Loss: 0.0995, Dev: 0.7360\n",
            "Epoch: [882/1000], Step: [100/301], Loss: 0.1557, Dev: 0.7460\n",
            "Epoch: [882/1000], Step: [200/301], Loss: 0.1472, Dev: 0.7150\n",
            "Epoch: [882/1000], Step: [300/301], Loss: 0.3079, Dev: 0.7280\n",
            "Epoch: [883/1000], Step: [100/301], Loss: 0.1696, Dev: 0.7190\n",
            "Epoch: [883/1000], Step: [200/301], Loss: 0.2026, Dev: 0.7130\n",
            "Epoch: [883/1000], Step: [300/301], Loss: 0.1935, Dev: 0.7270\n",
            "Epoch: [884/1000], Step: [100/301], Loss: 0.1560, Dev: 0.7340\n",
            "Epoch: [884/1000], Step: [200/301], Loss: 0.2268, Dev: 0.7140\n",
            "Epoch: [884/1000], Step: [300/301], Loss: 0.2374, Dev: 0.7220\n",
            "Epoch: [885/1000], Step: [100/301], Loss: 0.3092, Dev: 0.7220\n",
            "Epoch: [885/1000], Step: [200/301], Loss: 0.1743, Dev: 0.7380\n",
            "Epoch: [885/1000], Step: [300/301], Loss: 0.1485, Dev: 0.7220\n",
            "Epoch: [886/1000], Step: [100/301], Loss: 0.2316, Dev: 0.7430\n",
            "Epoch: [886/1000], Step: [200/301], Loss: 0.2125, Dev: 0.7230\n",
            "Epoch: [886/1000], Step: [300/301], Loss: 0.2156, Dev: 0.7190\n",
            "Epoch: [887/1000], Step: [100/301], Loss: 0.1553, Dev: 0.7400\n",
            "Epoch: [887/1000], Step: [200/301], Loss: 0.1565, Dev: 0.7360\n",
            "Epoch: [887/1000], Step: [300/301], Loss: 0.1764, Dev: 0.7260\n",
            "Epoch: [888/1000], Step: [100/301], Loss: 0.1850, Dev: 0.7180\n",
            "Epoch: [888/1000], Step: [200/301], Loss: 0.1906, Dev: 0.7130\n",
            "Epoch: [888/1000], Step: [300/301], Loss: 0.2687, Dev: 0.7240\n",
            "Epoch: [889/1000], Step: [100/301], Loss: 0.1883, Dev: 0.7200\n",
            "Epoch: [889/1000], Step: [200/301], Loss: 0.1985, Dev: 0.7440\n",
            "Epoch: [889/1000], Step: [300/301], Loss: 0.1576, Dev: 0.7050\n",
            "Epoch: [890/1000], Step: [100/301], Loss: 0.2711, Dev: 0.7370\n",
            "Epoch: [890/1000], Step: [200/301], Loss: 0.1647, Dev: 0.7300\n",
            "Epoch: [890/1000], Step: [300/301], Loss: 0.1941, Dev: 0.7220\n",
            "Epoch: [891/1000], Step: [100/301], Loss: 0.0956, Dev: 0.7240\n",
            "Epoch: [891/1000], Step: [200/301], Loss: 0.2168, Dev: 0.7320\n",
            "Epoch: [891/1000], Step: [300/301], Loss: 0.4065, Dev: 0.7300\n",
            "Epoch: [892/1000], Step: [100/301], Loss: 0.2967, Dev: 0.7380\n",
            "Epoch: [892/1000], Step: [200/301], Loss: 0.2105, Dev: 0.7500\n",
            "Epoch: [892/1000], Step: [300/301], Loss: 0.1343, Dev: 0.7100\n",
            "Epoch: [893/1000], Step: [100/301], Loss: 0.1459, Dev: 0.7320\n",
            "Epoch: [893/1000], Step: [200/301], Loss: 0.3035, Dev: 0.7360\n",
            "Epoch: [893/1000], Step: [300/301], Loss: 0.2168, Dev: 0.7320\n",
            "Epoch: [894/1000], Step: [100/301], Loss: 0.1859, Dev: 0.7190\n",
            "Epoch: [894/1000], Step: [200/301], Loss: 0.1784, Dev: 0.7070\n",
            "Epoch: [894/1000], Step: [300/301], Loss: 0.1548, Dev: 0.7350\n",
            "Epoch: [895/1000], Step: [100/301], Loss: 0.1905, Dev: 0.7510\n",
            "Epoch: [895/1000], Step: [200/301], Loss: 0.1233, Dev: 0.7460\n",
            "Epoch: [895/1000], Step: [300/301], Loss: 0.1610, Dev: 0.7260\n",
            "Epoch: [896/1000], Step: [100/301], Loss: 0.2486, Dev: 0.7150\n",
            "Epoch: [896/1000], Step: [200/301], Loss: 0.1919, Dev: 0.7490\n",
            "Epoch: [896/1000], Step: [300/301], Loss: 0.2724, Dev: 0.7390\n",
            "Epoch: [897/1000], Step: [100/301], Loss: 0.1259, Dev: 0.7270\n",
            "Epoch: [897/1000], Step: [200/301], Loss: 0.2804, Dev: 0.7270\n",
            "Epoch: [897/1000], Step: [300/301], Loss: 0.2900, Dev: 0.7320\n",
            "Epoch: [898/1000], Step: [100/301], Loss: 0.1416, Dev: 0.7290\n",
            "Epoch: [898/1000], Step: [200/301], Loss: 0.2522, Dev: 0.7230\n",
            "Epoch: [898/1000], Step: [300/301], Loss: 0.1916, Dev: 0.7180\n",
            "Epoch: [899/1000], Step: [100/301], Loss: 0.1531, Dev: 0.7270\n",
            "Epoch: [899/1000], Step: [200/301], Loss: 0.1845, Dev: 0.7310\n",
            "Epoch: [899/1000], Step: [300/301], Loss: 0.2088, Dev: 0.7340\n",
            "Epoch: [900/1000], Step: [100/301], Loss: 0.1804, Dev: 0.7370\n",
            "Epoch: [900/1000], Step: [200/301], Loss: 0.1943, Dev: 0.7210\n",
            "Epoch: [900/1000], Step: [300/301], Loss: 0.1917, Dev: 0.7480\n",
            "Epoch: [901/1000], Step: [100/301], Loss: 0.2327, Dev: 0.7160\n",
            "Epoch: [901/1000], Step: [200/301], Loss: 0.3188, Dev: 0.7290\n",
            "Epoch: [901/1000], Step: [300/301], Loss: 0.1285, Dev: 0.7650\n",
            "Epoch: [902/1000], Step: [100/301], Loss: 0.1214, Dev: 0.7160\n",
            "Epoch: [902/1000], Step: [200/301], Loss: 0.1972, Dev: 0.7310\n",
            "Epoch: [902/1000], Step: [300/301], Loss: 0.1980, Dev: 0.7300\n",
            "Epoch: [903/1000], Step: [100/301], Loss: 0.1483, Dev: 0.7370\n",
            "Epoch: [903/1000], Step: [200/301], Loss: 0.1352, Dev: 0.7330\n",
            "Epoch: [903/1000], Step: [300/301], Loss: 0.1168, Dev: 0.7240\n",
            "Epoch: [904/1000], Step: [100/301], Loss: 0.3022, Dev: 0.7220\n",
            "Epoch: [904/1000], Step: [200/301], Loss: 0.2394, Dev: 0.7360\n",
            "Epoch: [904/1000], Step: [300/301], Loss: 0.1961, Dev: 0.7200\n",
            "Epoch: [905/1000], Step: [100/301], Loss: 0.2410, Dev: 0.7330\n",
            "Epoch: [905/1000], Step: [200/301], Loss: 0.1471, Dev: 0.7330\n",
            "Epoch: [905/1000], Step: [300/301], Loss: 0.2430, Dev: 0.7210\n",
            "Epoch: [906/1000], Step: [100/301], Loss: 0.1208, Dev: 0.7250\n",
            "Epoch: [906/1000], Step: [200/301], Loss: 0.1619, Dev: 0.7230\n",
            "Epoch: [906/1000], Step: [300/301], Loss: 0.1617, Dev: 0.7220\n",
            "Epoch: [907/1000], Step: [100/301], Loss: 0.1856, Dev: 0.7170\n",
            "Epoch: [907/1000], Step: [200/301], Loss: 0.1108, Dev: 0.7480\n",
            "Epoch: [907/1000], Step: [300/301], Loss: 0.4515, Dev: 0.7190\n",
            "Epoch: [908/1000], Step: [100/301], Loss: 0.1562, Dev: 0.7110\n",
            "Epoch: [908/1000], Step: [200/301], Loss: 0.1211, Dev: 0.7250\n",
            "Epoch: [908/1000], Step: [300/301], Loss: 0.1342, Dev: 0.7180\n",
            "Epoch: [909/1000], Step: [100/301], Loss: 0.1507, Dev: 0.7350\n",
            "Epoch: [909/1000], Step: [200/301], Loss: 0.1737, Dev: 0.7310\n",
            "Epoch: [909/1000], Step: [300/301], Loss: 0.1427, Dev: 0.7260\n",
            "Epoch: [910/1000], Step: [100/301], Loss: 0.1823, Dev: 0.7240\n",
            "Epoch: [910/1000], Step: [200/301], Loss: 0.2360, Dev: 0.7430\n",
            "Epoch: [910/1000], Step: [300/301], Loss: 0.1591, Dev: 0.7150\n",
            "Epoch: [911/1000], Step: [100/301], Loss: 0.1665, Dev: 0.7090\n",
            "Epoch: [911/1000], Step: [200/301], Loss: 0.1640, Dev: 0.7340\n",
            "Epoch: [911/1000], Step: [300/301], Loss: 0.1342, Dev: 0.7200\n",
            "Epoch: [912/1000], Step: [100/301], Loss: 0.1571, Dev: 0.6960\n",
            "Epoch: [912/1000], Step: [200/301], Loss: 0.1126, Dev: 0.7260\n",
            "Epoch: [912/1000], Step: [300/301], Loss: 0.0839, Dev: 0.7360\n",
            "Epoch: [913/1000], Step: [100/301], Loss: 0.3089, Dev: 0.7320\n",
            "Epoch: [913/1000], Step: [200/301], Loss: 0.1941, Dev: 0.7180\n",
            "Epoch: [913/1000], Step: [300/301], Loss: 0.1792, Dev: 0.7290\n",
            "Epoch: [914/1000], Step: [100/301], Loss: 0.2561, Dev: 0.7250\n",
            "Epoch: [914/1000], Step: [200/301], Loss: 0.1598, Dev: 0.7160\n",
            "Epoch: [914/1000], Step: [300/301], Loss: 0.2394, Dev: 0.7390\n",
            "Epoch: [915/1000], Step: [100/301], Loss: 0.1492, Dev: 0.7330\n",
            "Epoch: [915/1000], Step: [200/301], Loss: 0.2291, Dev: 0.7230\n",
            "Epoch: [915/1000], Step: [300/301], Loss: 0.1807, Dev: 0.7280\n",
            "Epoch: [916/1000], Step: [100/301], Loss: 0.1905, Dev: 0.7260\n",
            "Epoch: [916/1000], Step: [200/301], Loss: 0.1389, Dev: 0.7360\n",
            "Epoch: [916/1000], Step: [300/301], Loss: 0.2213, Dev: 0.7390\n",
            "Epoch: [917/1000], Step: [100/301], Loss: 0.2130, Dev: 0.7280\n",
            "Epoch: [917/1000], Step: [200/301], Loss: 0.1683, Dev: 0.7260\n",
            "Epoch: [917/1000], Step: [300/301], Loss: 0.1750, Dev: 0.7170\n",
            "Epoch: [918/1000], Step: [100/301], Loss: 0.3391, Dev: 0.7220\n",
            "Epoch: [918/1000], Step: [200/301], Loss: 0.2217, Dev: 0.7320\n",
            "Epoch: [918/1000], Step: [300/301], Loss: 0.1833, Dev: 0.7390\n",
            "Epoch: [919/1000], Step: [100/301], Loss: 0.2374, Dev: 0.7250\n",
            "Epoch: [919/1000], Step: [200/301], Loss: 0.1471, Dev: 0.7190\n",
            "Epoch: [919/1000], Step: [300/301], Loss: 0.2279, Dev: 0.7300\n",
            "Epoch: [920/1000], Step: [100/301], Loss: 0.1838, Dev: 0.7430\n",
            "Epoch: [920/1000], Step: [200/301], Loss: 0.2473, Dev: 0.7390\n",
            "Epoch: [920/1000], Step: [300/301], Loss: 0.2728, Dev: 0.7200\n",
            "Epoch: [921/1000], Step: [100/301], Loss: 0.1240, Dev: 0.7200\n",
            "Epoch: [921/1000], Step: [200/301], Loss: 0.0833, Dev: 0.7240\n",
            "Epoch: [921/1000], Step: [300/301], Loss: 0.1312, Dev: 0.7320\n",
            "Epoch: [922/1000], Step: [100/301], Loss: 0.1371, Dev: 0.7220\n",
            "Epoch: [922/1000], Step: [200/301], Loss: 0.3731, Dev: 0.7150\n",
            "Epoch: [922/1000], Step: [300/301], Loss: 0.1816, Dev: 0.7340\n",
            "Epoch: [923/1000], Step: [100/301], Loss: 0.1143, Dev: 0.7240\n",
            "Epoch: [923/1000], Step: [200/301], Loss: 0.2587, Dev: 0.7110\n",
            "Epoch: [923/1000], Step: [300/301], Loss: 0.2522, Dev: 0.7190\n",
            "Epoch: [924/1000], Step: [100/301], Loss: 0.1289, Dev: 0.7220\n",
            "Epoch: [924/1000], Step: [200/301], Loss: 0.1819, Dev: 0.7170\n",
            "Epoch: [924/1000], Step: [300/301], Loss: 0.1879, Dev: 0.7170\n",
            "Epoch: [925/1000], Step: [100/301], Loss: 0.1125, Dev: 0.7250\n",
            "Epoch: [925/1000], Step: [200/301], Loss: 0.1436, Dev: 0.7180\n",
            "Epoch: [925/1000], Step: [300/301], Loss: 0.1733, Dev: 0.7470\n",
            "Epoch: [926/1000], Step: [100/301], Loss: 0.2906, Dev: 0.7450\n",
            "Epoch: [926/1000], Step: [200/301], Loss: 0.2046, Dev: 0.7240\n",
            "Epoch: [926/1000], Step: [300/301], Loss: 0.2489, Dev: 0.7200\n",
            "Epoch: [927/1000], Step: [100/301], Loss: 0.1760, Dev: 0.7580\n",
            "Epoch: [927/1000], Step: [200/301], Loss: 0.3120, Dev: 0.7300\n",
            "Epoch: [927/1000], Step: [300/301], Loss: 0.1883, Dev: 0.7280\n",
            "Epoch: [928/1000], Step: [100/301], Loss: 0.2004, Dev: 0.7300\n",
            "Epoch: [928/1000], Step: [200/301], Loss: 0.1409, Dev: 0.7310\n",
            "Epoch: [928/1000], Step: [300/301], Loss: 0.1331, Dev: 0.7280\n",
            "Epoch: [929/1000], Step: [100/301], Loss: 0.0923, Dev: 0.7200\n",
            "Epoch: [929/1000], Step: [200/301], Loss: 0.1403, Dev: 0.6990\n",
            "Epoch: [929/1000], Step: [300/301], Loss: 0.1521, Dev: 0.7430\n",
            "Epoch: [930/1000], Step: [100/301], Loss: 0.1126, Dev: 0.7270\n",
            "Epoch: [930/1000], Step: [200/301], Loss: 0.1390, Dev: 0.7100\n",
            "Epoch: [930/1000], Step: [300/301], Loss: 0.1564, Dev: 0.7190\n",
            "Epoch: [931/1000], Step: [100/301], Loss: 0.2381, Dev: 0.7140\n",
            "Epoch: [931/1000], Step: [200/301], Loss: 0.1321, Dev: 0.7230\n",
            "Epoch: [931/1000], Step: [300/301], Loss: 0.2158, Dev: 0.7340\n",
            "Epoch: [932/1000], Step: [100/301], Loss: 0.2757, Dev: 0.7320\n",
            "Epoch: [932/1000], Step: [200/301], Loss: 0.1809, Dev: 0.7160\n",
            "Epoch: [932/1000], Step: [300/301], Loss: 0.1905, Dev: 0.7480\n",
            "Epoch: [933/1000], Step: [100/301], Loss: 0.1364, Dev: 0.7220\n",
            "Epoch: [933/1000], Step: [200/301], Loss: 0.2559, Dev: 0.7120\n",
            "Epoch: [933/1000], Step: [300/301], Loss: 0.1313, Dev: 0.7610\n",
            "Epoch: [934/1000], Step: [100/301], Loss: 0.2986, Dev: 0.7250\n",
            "Epoch: [934/1000], Step: [200/301], Loss: 0.1534, Dev: 0.7080\n",
            "Epoch: [934/1000], Step: [300/301], Loss: 0.3077, Dev: 0.7240\n",
            "Epoch: [935/1000], Step: [100/301], Loss: 0.1276, Dev: 0.7270\n",
            "Epoch: [935/1000], Step: [200/301], Loss: 0.1146, Dev: 0.7260\n",
            "Epoch: [935/1000], Step: [300/301], Loss: 0.1575, Dev: 0.7190\n",
            "Epoch: [936/1000], Step: [100/301], Loss: 0.1469, Dev: 0.7460\n",
            "Epoch: [936/1000], Step: [200/301], Loss: 0.2414, Dev: 0.7090\n",
            "Epoch: [936/1000], Step: [300/301], Loss: 0.2649, Dev: 0.7240\n",
            "Epoch: [937/1000], Step: [100/301], Loss: 0.1346, Dev: 0.7480\n",
            "Epoch: [937/1000], Step: [200/301], Loss: 0.1739, Dev: 0.7130\n",
            "Epoch: [937/1000], Step: [300/301], Loss: 0.1431, Dev: 0.7430\n",
            "Epoch: [938/1000], Step: [100/301], Loss: 0.1340, Dev: 0.7310\n",
            "Epoch: [938/1000], Step: [200/301], Loss: 0.1638, Dev: 0.7510\n",
            "Epoch: [938/1000], Step: [300/301], Loss: 0.1765, Dev: 0.7230\n",
            "Epoch: [939/1000], Step: [100/301], Loss: 0.1151, Dev: 0.7220\n",
            "Epoch: [939/1000], Step: [200/301], Loss: 0.1836, Dev: 0.7260\n",
            "Epoch: [939/1000], Step: [300/301], Loss: 0.2209, Dev: 0.7210\n",
            "Epoch: [940/1000], Step: [100/301], Loss: 0.2586, Dev: 0.7240\n",
            "Epoch: [940/1000], Step: [200/301], Loss: 0.1334, Dev: 0.7420\n",
            "Epoch: [940/1000], Step: [300/301], Loss: 0.1438, Dev: 0.7240\n",
            "Epoch: [941/1000], Step: [100/301], Loss: 0.1864, Dev: 0.7080\n",
            "Epoch: [941/1000], Step: [200/301], Loss: 0.1397, Dev: 0.7550\n",
            "Epoch: [941/1000], Step: [300/301], Loss: 0.1717, Dev: 0.7320\n",
            "Epoch: [942/1000], Step: [100/301], Loss: 0.2024, Dev: 0.7440\n",
            "Epoch: [942/1000], Step: [200/301], Loss: 0.2550, Dev: 0.7600\n",
            "Epoch: [942/1000], Step: [300/301], Loss: 0.1647, Dev: 0.7140\n",
            "Epoch: [943/1000], Step: [100/301], Loss: 0.1633, Dev: 0.7180\n",
            "Epoch: [943/1000], Step: [200/301], Loss: 0.1860, Dev: 0.7390\n",
            "Epoch: [943/1000], Step: [300/301], Loss: 0.1608, Dev: 0.7310\n",
            "Epoch: [944/1000], Step: [100/301], Loss: 0.1517, Dev: 0.7100\n",
            "Epoch: [944/1000], Step: [200/301], Loss: 0.1622, Dev: 0.7310\n",
            "Epoch: [944/1000], Step: [300/301], Loss: 0.2284, Dev: 0.7160\n",
            "Epoch: [945/1000], Step: [100/301], Loss: 0.2763, Dev: 0.7270\n",
            "Epoch: [945/1000], Step: [200/301], Loss: 0.1687, Dev: 0.7510\n",
            "Epoch: [945/1000], Step: [300/301], Loss: 0.2125, Dev: 0.7220\n",
            "Epoch: [946/1000], Step: [100/301], Loss: 0.2557, Dev: 0.7440\n",
            "Epoch: [946/1000], Step: [200/301], Loss: 0.0927, Dev: 0.7110\n",
            "Epoch: [946/1000], Step: [300/301], Loss: 0.1174, Dev: 0.7240\n",
            "Epoch: [947/1000], Step: [100/301], Loss: 0.1149, Dev: 0.7210\n",
            "Epoch: [947/1000], Step: [200/301], Loss: 0.3135, Dev: 0.7550\n",
            "Epoch: [947/1000], Step: [300/301], Loss: 0.2380, Dev: 0.7200\n",
            "Epoch: [948/1000], Step: [100/301], Loss: 0.2515, Dev: 0.7240\n",
            "Epoch: [948/1000], Step: [200/301], Loss: 0.1285, Dev: 0.7090\n",
            "Epoch: [948/1000], Step: [300/301], Loss: 0.3047, Dev: 0.7430\n",
            "Epoch: [949/1000], Step: [100/301], Loss: 0.1267, Dev: 0.7500\n",
            "Epoch: [949/1000], Step: [200/301], Loss: 0.1806, Dev: 0.7200\n",
            "Epoch: [949/1000], Step: [300/301], Loss: 0.1062, Dev: 0.7080\n",
            "Epoch: [950/1000], Step: [100/301], Loss: 0.1500, Dev: 0.7070\n",
            "Epoch: [950/1000], Step: [200/301], Loss: 0.2962, Dev: 0.7430\n",
            "Epoch: [950/1000], Step: [300/301], Loss: 0.1641, Dev: 0.7380\n",
            "Epoch: [951/1000], Step: [100/301], Loss: 0.1579, Dev: 0.7240\n",
            "Epoch: [951/1000], Step: [200/301], Loss: 0.2953, Dev: 0.7460\n",
            "Epoch: [951/1000], Step: [300/301], Loss: 0.2272, Dev: 0.7350\n",
            "Epoch: [952/1000], Step: [100/301], Loss: 0.1287, Dev: 0.7150\n",
            "Epoch: [952/1000], Step: [200/301], Loss: 0.3072, Dev: 0.7400\n",
            "Epoch: [952/1000], Step: [300/301], Loss: 0.1817, Dev: 0.7450\n",
            "Epoch: [953/1000], Step: [100/301], Loss: 0.1459, Dev: 0.7260\n",
            "Epoch: [953/1000], Step: [200/301], Loss: 0.1309, Dev: 0.7230\n",
            "Epoch: [953/1000], Step: [300/301], Loss: 0.1537, Dev: 0.7220\n",
            "Epoch: [954/1000], Step: [100/301], Loss: 0.1581, Dev: 0.7100\n",
            "Epoch: [954/1000], Step: [200/301], Loss: 0.1729, Dev: 0.7250\n",
            "Epoch: [954/1000], Step: [300/301], Loss: 0.1359, Dev: 0.7290\n",
            "Epoch: [955/1000], Step: [100/301], Loss: 0.2035, Dev: 0.7080\n",
            "Epoch: [955/1000], Step: [200/301], Loss: 0.2263, Dev: 0.7370\n",
            "Epoch: [955/1000], Step: [300/301], Loss: 0.1325, Dev: 0.7260\n",
            "Epoch: [956/1000], Step: [100/301], Loss: 0.1483, Dev: 0.7090\n",
            "Epoch: [956/1000], Step: [200/301], Loss: 0.2041, Dev: 0.7540\n",
            "Epoch: [956/1000], Step: [300/301], Loss: 0.2101, Dev: 0.7360\n",
            "Epoch: [957/1000], Step: [100/301], Loss: 0.1590, Dev: 0.7100\n",
            "Epoch: [957/1000], Step: [200/301], Loss: 0.2097, Dev: 0.7210\n",
            "Epoch: [957/1000], Step: [300/301], Loss: 0.1431, Dev: 0.7330\n",
            "Epoch: [958/1000], Step: [100/301], Loss: 0.1413, Dev: 0.7210\n",
            "Epoch: [958/1000], Step: [200/301], Loss: 0.1240, Dev: 0.7320\n",
            "Epoch: [958/1000], Step: [300/301], Loss: 0.2218, Dev: 0.7310\n",
            "Epoch: [959/1000], Step: [100/301], Loss: 0.1082, Dev: 0.7200\n",
            "Epoch: [959/1000], Step: [200/301], Loss: 0.2184, Dev: 0.7210\n",
            "Epoch: [959/1000], Step: [300/301], Loss: 0.1649, Dev: 0.7150\n",
            "Epoch: [960/1000], Step: [100/301], Loss: 0.1906, Dev: 0.7190\n",
            "Epoch: [960/1000], Step: [200/301], Loss: 0.2161, Dev: 0.7190\n",
            "Epoch: [960/1000], Step: [300/301], Loss: 0.3715, Dev: 0.7180\n",
            "Epoch: [961/1000], Step: [100/301], Loss: 0.1741, Dev: 0.7130\n",
            "Epoch: [961/1000], Step: [200/301], Loss: 0.1184, Dev: 0.7360\n",
            "Epoch: [961/1000], Step: [300/301], Loss: 0.1799, Dev: 0.7210\n",
            "Epoch: [962/1000], Step: [100/301], Loss: 0.2392, Dev: 0.7190\n",
            "Epoch: [962/1000], Step: [200/301], Loss: 0.1416, Dev: 0.7300\n",
            "Epoch: [962/1000], Step: [300/301], Loss: 0.1831, Dev: 0.7160\n",
            "Epoch: [963/1000], Step: [100/301], Loss: 0.1455, Dev: 0.7270\n",
            "Epoch: [963/1000], Step: [200/301], Loss: 0.1571, Dev: 0.7130\n",
            "Epoch: [963/1000], Step: [300/301], Loss: 0.1576, Dev: 0.7220\n",
            "Epoch: [964/1000], Step: [100/301], Loss: 0.2150, Dev: 0.7260\n",
            "Epoch: [964/1000], Step: [200/301], Loss: 0.1740, Dev: 0.7360\n",
            "Epoch: [964/1000], Step: [300/301], Loss: 0.1523, Dev: 0.7360\n",
            "Epoch: [965/1000], Step: [100/301], Loss: 0.2729, Dev: 0.7110\n",
            "Epoch: [965/1000], Step: [200/301], Loss: 0.1045, Dev: 0.7360\n",
            "Epoch: [965/1000], Step: [300/301], Loss: 0.2484, Dev: 0.7320\n",
            "Epoch: [966/1000], Step: [100/301], Loss: 0.2979, Dev: 0.7250\n",
            "Epoch: [966/1000], Step: [200/301], Loss: 0.1848, Dev: 0.7350\n",
            "Epoch: [966/1000], Step: [300/301], Loss: 0.1585, Dev: 0.7260\n",
            "Epoch: [967/1000], Step: [100/301], Loss: 0.1378, Dev: 0.7280\n",
            "Epoch: [967/1000], Step: [200/301], Loss: 0.1949, Dev: 0.7210\n",
            "Epoch: [967/1000], Step: [300/301], Loss: 0.1415, Dev: 0.7160\n",
            "Epoch: [968/1000], Step: [100/301], Loss: 0.1736, Dev: 0.7190\n",
            "Epoch: [968/1000], Step: [200/301], Loss: 0.1360, Dev: 0.7160\n",
            "Epoch: [968/1000], Step: [300/301], Loss: 0.1778, Dev: 0.7350\n",
            "Epoch: [969/1000], Step: [100/301], Loss: 0.1663, Dev: 0.7230\n",
            "Epoch: [969/1000], Step: [200/301], Loss: 0.1609, Dev: 0.7220\n",
            "Epoch: [969/1000], Step: [300/301], Loss: 0.1387, Dev: 0.7240\n",
            "Epoch: [970/1000], Step: [100/301], Loss: 0.2782, Dev: 0.7300\n",
            "Epoch: [970/1000], Step: [200/301], Loss: 0.2270, Dev: 0.7180\n",
            "Epoch: [970/1000], Step: [300/301], Loss: 0.2732, Dev: 0.7300\n",
            "Epoch: [971/1000], Step: [100/301], Loss: 0.2024, Dev: 0.7300\n",
            "Epoch: [971/1000], Step: [200/301], Loss: 0.1180, Dev: 0.7360\n",
            "Epoch: [971/1000], Step: [300/301], Loss: 0.1433, Dev: 0.7230\n",
            "Epoch: [972/1000], Step: [100/301], Loss: 0.1900, Dev: 0.7540\n",
            "Epoch: [972/1000], Step: [200/301], Loss: 0.1955, Dev: 0.7220\n",
            "Epoch: [972/1000], Step: [300/301], Loss: 0.1551, Dev: 0.7210\n",
            "Epoch: [973/1000], Step: [100/301], Loss: 0.1439, Dev: 0.7260\n",
            "Epoch: [973/1000], Step: [200/301], Loss: 0.1747, Dev: 0.7270\n",
            "Epoch: [973/1000], Step: [300/301], Loss: 0.1823, Dev: 0.7280\n",
            "Epoch: [974/1000], Step: [100/301], Loss: 0.2145, Dev: 0.7170\n",
            "Epoch: [974/1000], Step: [200/301], Loss: 0.1354, Dev: 0.7290\n",
            "Epoch: [974/1000], Step: [300/301], Loss: 0.1229, Dev: 0.7300\n",
            "Epoch: [975/1000], Step: [100/301], Loss: 0.1589, Dev: 0.7320\n",
            "Epoch: [975/1000], Step: [200/301], Loss: 0.1954, Dev: 0.7190\n",
            "Epoch: [975/1000], Step: [300/301], Loss: 0.2066, Dev: 0.7140\n",
            "Epoch: [976/1000], Step: [100/301], Loss: 0.1452, Dev: 0.7230\n",
            "Epoch: [976/1000], Step: [200/301], Loss: 0.2583, Dev: 0.7400\n",
            "Epoch: [976/1000], Step: [300/301], Loss: 0.1738, Dev: 0.7310\n",
            "Epoch: [977/1000], Step: [100/301], Loss: 0.1943, Dev: 0.7430\n",
            "Epoch: [977/1000], Step: [200/301], Loss: 0.1582, Dev: 0.7170\n",
            "Epoch: [977/1000], Step: [300/301], Loss: 0.1190, Dev: 0.7290\n",
            "Epoch: [978/1000], Step: [100/301], Loss: 0.1253, Dev: 0.7190\n",
            "Epoch: [978/1000], Step: [200/301], Loss: 0.1805, Dev: 0.7300\n",
            "Epoch: [978/1000], Step: [300/301], Loss: 0.2122, Dev: 0.7240\n",
            "Epoch: [979/1000], Step: [100/301], Loss: 0.1781, Dev: 0.7290\n",
            "Epoch: [979/1000], Step: [200/301], Loss: 0.1744, Dev: 0.7280\n",
            "Epoch: [979/1000], Step: [300/301], Loss: 0.1292, Dev: 0.7310\n",
            "Epoch: [980/1000], Step: [100/301], Loss: 0.2022, Dev: 0.7230\n",
            "Epoch: [980/1000], Step: [200/301], Loss: 0.1804, Dev: 0.7220\n",
            "Epoch: [980/1000], Step: [300/301], Loss: 0.1601, Dev: 0.7400\n",
            "Epoch: [981/1000], Step: [100/301], Loss: 0.2368, Dev: 0.7390\n",
            "Epoch: [981/1000], Step: [200/301], Loss: 0.2222, Dev: 0.7090\n",
            "Epoch: [981/1000], Step: [300/301], Loss: 0.1882, Dev: 0.7190\n",
            "Epoch: [982/1000], Step: [100/301], Loss: 0.2603, Dev: 0.7120\n",
            "Epoch: [982/1000], Step: [200/301], Loss: 0.1564, Dev: 0.7110\n",
            "Epoch: [982/1000], Step: [300/301], Loss: 0.0975, Dev: 0.7320\n",
            "Epoch: [983/1000], Step: [100/301], Loss: 0.2683, Dev: 0.7270\n",
            "Epoch: [983/1000], Step: [200/301], Loss: 0.1719, Dev: 0.7130\n",
            "Epoch: [983/1000], Step: [300/301], Loss: 0.1635, Dev: 0.7210\n",
            "Epoch: [984/1000], Step: [100/301], Loss: 0.2419, Dev: 0.7350\n",
            "Epoch: [984/1000], Step: [200/301], Loss: 0.1827, Dev: 0.7310\n",
            "Epoch: [984/1000], Step: [300/301], Loss: 0.2496, Dev: 0.7280\n",
            "Epoch: [985/1000], Step: [100/301], Loss: 0.1422, Dev: 0.7170\n",
            "Epoch: [985/1000], Step: [200/301], Loss: 0.2811, Dev: 0.7290\n",
            "Epoch: [985/1000], Step: [300/301], Loss: 0.1804, Dev: 0.7220\n",
            "Epoch: [986/1000], Step: [100/301], Loss: 0.2760, Dev: 0.7210\n",
            "Epoch: [986/1000], Step: [200/301], Loss: 0.1398, Dev: 0.7230\n",
            "Epoch: [986/1000], Step: [300/301], Loss: 0.0986, Dev: 0.7380\n",
            "Epoch: [987/1000], Step: [100/301], Loss: 0.1938, Dev: 0.7230\n",
            "Epoch: [987/1000], Step: [200/301], Loss: 0.2078, Dev: 0.7240\n",
            "Epoch: [987/1000], Step: [300/301], Loss: 0.2368, Dev: 0.7360\n",
            "Epoch: [988/1000], Step: [100/301], Loss: 0.2584, Dev: 0.7140\n",
            "Epoch: [988/1000], Step: [200/301], Loss: 0.2170, Dev: 0.7350\n",
            "Epoch: [988/1000], Step: [300/301], Loss: 0.1869, Dev: 0.7180\n",
            "Epoch: [989/1000], Step: [100/301], Loss: 0.2004, Dev: 0.7370\n",
            "Epoch: [989/1000], Step: [200/301], Loss: 0.1445, Dev: 0.7250\n",
            "Epoch: [989/1000], Step: [300/301], Loss: 0.1859, Dev: 0.7430\n",
            "Epoch: [990/1000], Step: [100/301], Loss: 0.1892, Dev: 0.7150\n",
            "Epoch: [990/1000], Step: [200/301], Loss: 0.2147, Dev: 0.7150\n",
            "Epoch: [990/1000], Step: [300/301], Loss: 0.2601, Dev: 0.7440\n",
            "Epoch: [991/1000], Step: [100/301], Loss: 0.2534, Dev: 0.7310\n",
            "Epoch: [991/1000], Step: [200/301], Loss: 0.1891, Dev: 0.7310\n",
            "Epoch: [991/1000], Step: [300/301], Loss: 0.2383, Dev: 0.7310\n",
            "Epoch: [992/1000], Step: [100/301], Loss: 0.1181, Dev: 0.7240\n",
            "Epoch: [992/1000], Step: [200/301], Loss: 0.1764, Dev: 0.7200\n",
            "Epoch: [992/1000], Step: [300/301], Loss: 0.2902, Dev: 0.7410\n",
            "Epoch: [993/1000], Step: [100/301], Loss: 0.1253, Dev: 0.7260\n",
            "Epoch: [993/1000], Step: [200/301], Loss: 0.2538, Dev: 0.7390\n",
            "Epoch: [993/1000], Step: [300/301], Loss: 0.1294, Dev: 0.7260\n",
            "Epoch: [994/1000], Step: [100/301], Loss: 0.2034, Dev: 0.7310\n",
            "Epoch: [994/1000], Step: [200/301], Loss: 0.2836, Dev: 0.7220\n",
            "Epoch: [994/1000], Step: [300/301], Loss: 0.2970, Dev: 0.7140\n",
            "Epoch: [995/1000], Step: [100/301], Loss: 0.2226, Dev: 0.7250\n",
            "Epoch: [995/1000], Step: [200/301], Loss: 0.1813, Dev: 0.7280\n",
            "Epoch: [995/1000], Step: [300/301], Loss: 0.1876, Dev: 0.7390\n",
            "Epoch: [996/1000], Step: [100/301], Loss: 0.1937, Dev: 0.7350\n",
            "Epoch: [996/1000], Step: [200/301], Loss: 0.1802, Dev: 0.7370\n",
            "Epoch: [996/1000], Step: [300/301], Loss: 0.1862, Dev: 0.7110\n",
            "Epoch: [997/1000], Step: [100/301], Loss: 0.2205, Dev: 0.7200\n",
            "Epoch: [997/1000], Step: [200/301], Loss: 0.1206, Dev: 0.7210\n",
            "Epoch: [997/1000], Step: [300/301], Loss: 0.0919, Dev: 0.7370\n",
            "Epoch: [998/1000], Step: [100/301], Loss: 0.1354, Dev: 0.7080\n",
            "Epoch: [998/1000], Step: [200/301], Loss: 0.1324, Dev: 0.7170\n",
            "Epoch: [998/1000], Step: [300/301], Loss: 0.1569, Dev: 0.7510\n",
            "Epoch: [999/1000], Step: [100/301], Loss: 0.1659, Dev: 0.7130\n",
            "Epoch: [999/1000], Step: [200/301], Loss: 0.2075, Dev: 0.7250\n",
            "Epoch: [999/1000], Step: [300/301], Loss: 0.1943, Dev: 0.7130\n",
            "Epoch: [1000/1000], Step: [100/301], Loss: 0.1799, Dev: 0.7250\n",
            "Epoch: [1000/1000], Step: [200/301], Loss: 0.2111, Dev: 0.7240\n",
            "Epoch: [1000/1000], Step: [300/301], Loss: 0.1208, Dev: 0.7370\n",
            "Epoch: [1/1000], Step: [100/301], Loss: 1.0981, Dev: 0.3930\n",
            "Epoch: [1/1000], Step: [200/301], Loss: 1.7027, Dev: 0.3930\n",
            "Epoch: [1/1000], Step: [300/301], Loss: 1.6843, Dev: 0.4050\n",
            "Epoch: [2/1000], Step: [100/301], Loss: 1.1338, Dev: 0.4140\n",
            "Epoch: [2/1000], Step: [200/301], Loss: 1.2793, Dev: 0.4210\n",
            "Epoch: [2/1000], Step: [300/301], Loss: 1.2222, Dev: 0.4690\n",
            "Epoch: [3/1000], Step: [100/301], Loss: 1.3211, Dev: 0.3810\n",
            "Epoch: [3/1000], Step: [200/301], Loss: 1.3234, Dev: 0.4210\n",
            "Epoch: [3/1000], Step: [300/301], Loss: 0.7065, Dev: 0.4600\n",
            "Epoch: [4/1000], Step: [100/301], Loss: 0.8578, Dev: 0.4520\n",
            "Epoch: [4/1000], Step: [200/301], Loss: 1.4782, Dev: 0.4560\n",
            "Epoch: [4/1000], Step: [300/301], Loss: 1.3631, Dev: 0.4100\n",
            "Epoch: [5/1000], Step: [100/301], Loss: 0.8342, Dev: 0.4020\n",
            "Epoch: [5/1000], Step: [200/301], Loss: 0.7436, Dev: 0.4610\n",
            "Epoch: [5/1000], Step: [300/301], Loss: 1.5589, Dev: 0.4820\n",
            "Epoch: [6/1000], Step: [100/301], Loss: 1.5318, Dev: 0.4140\n",
            "Epoch: [6/1000], Step: [200/301], Loss: 0.9148, Dev: 0.4740\n",
            "Epoch: [6/1000], Step: [300/301], Loss: 1.3372, Dev: 0.4070\n",
            "Epoch: [7/1000], Step: [100/301], Loss: 0.8071, Dev: 0.4730\n",
            "Epoch: [7/1000], Step: [200/301], Loss: 0.6787, Dev: 0.4270\n",
            "Epoch: [7/1000], Step: [300/301], Loss: 0.9286, Dev: 0.4350\n",
            "Epoch: [8/1000], Step: [100/301], Loss: 0.8568, Dev: 0.4300\n",
            "Epoch: [8/1000], Step: [200/301], Loss: 1.8849, Dev: 0.4890\n",
            "Epoch: [8/1000], Step: [300/301], Loss: 0.6925, Dev: 0.3960\n",
            "Epoch: [9/1000], Step: [100/301], Loss: 1.7192, Dev: 0.4410\n",
            "Epoch: [9/1000], Step: [200/301], Loss: 0.7208, Dev: 0.4160\n",
            "Epoch: [9/1000], Step: [300/301], Loss: 1.4619, Dev: 0.4400\n",
            "Epoch: [10/1000], Step: [100/301], Loss: 1.0166, Dev: 0.5050\n",
            "Epoch: [10/1000], Step: [200/301], Loss: 1.2414, Dev: 0.4490\n",
            "Epoch: [10/1000], Step: [300/301], Loss: 0.6207, Dev: 0.4890\n",
            "Epoch: [11/1000], Step: [100/301], Loss: 1.4727, Dev: 0.4480\n",
            "Epoch: [11/1000], Step: [200/301], Loss: 1.5195, Dev: 0.4310\n",
            "Epoch: [11/1000], Step: [300/301], Loss: 0.8238, Dev: 0.4710\n",
            "Epoch: [12/1000], Step: [100/301], Loss: 0.7643, Dev: 0.4720\n",
            "Epoch: [12/1000], Step: [200/301], Loss: 1.0853, Dev: 0.4650\n",
            "Epoch: [12/1000], Step: [300/301], Loss: 0.7204, Dev: 0.4760\n",
            "Epoch: [13/1000], Step: [100/301], Loss: 0.8494, Dev: 0.4790\n",
            "Epoch: [13/1000], Step: [200/301], Loss: 0.6924, Dev: 0.4940\n",
            "Epoch: [13/1000], Step: [300/301], Loss: 0.7957, Dev: 0.4620\n",
            "Epoch: [14/1000], Step: [100/301], Loss: 1.0129, Dev: 0.4610\n",
            "Epoch: [14/1000], Step: [200/301], Loss: 0.7898, Dev: 0.4660\n",
            "Epoch: [14/1000], Step: [300/301], Loss: 0.9262, Dev: 0.4590\n",
            "Epoch: [15/1000], Step: [100/301], Loss: 1.5004, Dev: 0.4820\n",
            "Epoch: [15/1000], Step: [200/301], Loss: 1.1777, Dev: 0.5030\n",
            "Epoch: [15/1000], Step: [300/301], Loss: 0.6453, Dev: 0.4740\n",
            "Epoch: [16/1000], Step: [100/301], Loss: 0.8753, Dev: 0.4810\n",
            "Epoch: [16/1000], Step: [200/301], Loss: 0.4427, Dev: 0.4660\n",
            "Epoch: [16/1000], Step: [300/301], Loss: 1.2368, Dev: 0.4730\n",
            "Epoch: [17/1000], Step: [100/301], Loss: 1.2508, Dev: 0.4640\n",
            "Epoch: [17/1000], Step: [200/301], Loss: 0.8197, Dev: 0.4760\n",
            "Epoch: [17/1000], Step: [300/301], Loss: 0.9914, Dev: 0.4560\n",
            "Epoch: [18/1000], Step: [100/301], Loss: 0.8952, Dev: 0.4410\n",
            "Epoch: [18/1000], Step: [200/301], Loss: 0.8333, Dev: 0.4740\n",
            "Epoch: [18/1000], Step: [300/301], Loss: 0.6492, Dev: 0.4640\n",
            "Epoch: [19/1000], Step: [100/301], Loss: 0.8515, Dev: 0.4770\n",
            "Epoch: [19/1000], Step: [200/301], Loss: 0.7783, Dev: 0.4790\n",
            "Epoch: [19/1000], Step: [300/301], Loss: 0.6175, Dev: 0.5470\n",
            "Epoch: [20/1000], Step: [100/301], Loss: 0.9353, Dev: 0.4790\n",
            "Epoch: [20/1000], Step: [200/301], Loss: 0.8918, Dev: 0.4690\n",
            "Epoch: [20/1000], Step: [300/301], Loss: 1.8258, Dev: 0.4770\n",
            "Epoch: [21/1000], Step: [100/301], Loss: 1.0431, Dev: 0.4780\n",
            "Epoch: [21/1000], Step: [200/301], Loss: 0.8205, Dev: 0.4950\n",
            "Epoch: [21/1000], Step: [300/301], Loss: 0.8623, Dev: 0.5200\n",
            "Epoch: [22/1000], Step: [100/301], Loss: 0.5813, Dev: 0.5140\n",
            "Epoch: [22/1000], Step: [200/301], Loss: 0.6456, Dev: 0.4870\n",
            "Epoch: [22/1000], Step: [300/301], Loss: 0.8283, Dev: 0.4970\n",
            "Epoch: [23/1000], Step: [100/301], Loss: 0.6743, Dev: 0.4770\n",
            "Epoch: [23/1000], Step: [200/301], Loss: 0.8102, Dev: 0.4870\n",
            "Epoch: [23/1000], Step: [300/301], Loss: 0.5581, Dev: 0.5080\n",
            "Epoch: [24/1000], Step: [100/301], Loss: 0.8250, Dev: 0.5290\n",
            "Epoch: [24/1000], Step: [200/301], Loss: 0.8257, Dev: 0.5160\n",
            "Epoch: [24/1000], Step: [300/301], Loss: 0.4424, Dev: 0.4980\n",
            "Epoch: [25/1000], Step: [100/301], Loss: 0.9714, Dev: 0.4510\n",
            "Epoch: [25/1000], Step: [200/301], Loss: 0.5572, Dev: 0.4900\n",
            "Epoch: [25/1000], Step: [300/301], Loss: 1.0812, Dev: 0.5490\n",
            "Epoch: [26/1000], Step: [100/301], Loss: 0.5967, Dev: 0.4950\n",
            "Epoch: [26/1000], Step: [200/301], Loss: 0.5749, Dev: 0.4860\n",
            "Epoch: [26/1000], Step: [300/301], Loss: 0.7766, Dev: 0.5170\n",
            "Epoch: [27/1000], Step: [100/301], Loss: 0.6408, Dev: 0.4960\n",
            "Epoch: [27/1000], Step: [200/301], Loss: 0.4302, Dev: 0.5000\n",
            "Epoch: [27/1000], Step: [300/301], Loss: 0.8413, Dev: 0.4910\n",
            "Epoch: [28/1000], Step: [100/301], Loss: 0.6762, Dev: 0.4920\n",
            "Epoch: [28/1000], Step: [200/301], Loss: 0.3110, Dev: 0.5090\n",
            "Epoch: [28/1000], Step: [300/301], Loss: 0.3577, Dev: 0.4970\n",
            "Epoch: [29/1000], Step: [100/301], Loss: 0.8069, Dev: 0.4780\n",
            "Epoch: [29/1000], Step: [200/301], Loss: 0.6067, Dev: 0.5040\n",
            "Epoch: [29/1000], Step: [300/301], Loss: 0.8976, Dev: 0.5310\n",
            "Epoch: [30/1000], Step: [100/301], Loss: 0.7027, Dev: 0.5110\n",
            "Epoch: [30/1000], Step: [200/301], Loss: 0.5770, Dev: 0.5120\n",
            "Epoch: [30/1000], Step: [300/301], Loss: 0.4126, Dev: 0.5150\n",
            "Epoch: [31/1000], Step: [100/301], Loss: 0.5060, Dev: 0.5040\n",
            "Epoch: [31/1000], Step: [200/301], Loss: 1.1598, Dev: 0.5180\n",
            "Epoch: [31/1000], Step: [300/301], Loss: 0.4147, Dev: 0.4890\n",
            "Epoch: [32/1000], Step: [100/301], Loss: 0.6007, Dev: 0.5060\n",
            "Epoch: [32/1000], Step: [200/301], Loss: 0.6459, Dev: 0.4950\n",
            "Epoch: [32/1000], Step: [300/301], Loss: 0.7121, Dev: 0.4970\n",
            "Epoch: [33/1000], Step: [100/301], Loss: 0.6927, Dev: 0.5050\n",
            "Epoch: [33/1000], Step: [200/301], Loss: 0.5771, Dev: 0.4770\n",
            "Epoch: [33/1000], Step: [300/301], Loss: 0.4889, Dev: 0.5280\n",
            "Epoch: [34/1000], Step: [100/301], Loss: 0.5680, Dev: 0.5050\n",
            "Epoch: [34/1000], Step: [200/301], Loss: 0.5447, Dev: 0.5400\n",
            "Epoch: [34/1000], Step: [300/301], Loss: 0.5852, Dev: 0.5100\n",
            "Epoch: [35/1000], Step: [100/301], Loss: 0.6083, Dev: 0.5350\n",
            "Epoch: [35/1000], Step: [200/301], Loss: 1.1955, Dev: 0.5050\n",
            "Epoch: [35/1000], Step: [300/301], Loss: 0.7093, Dev: 0.5380\n",
            "Epoch: [36/1000], Step: [100/301], Loss: 0.5541, Dev: 0.5380\n",
            "Epoch: [36/1000], Step: [200/301], Loss: 0.8189, Dev: 0.5170\n",
            "Epoch: [36/1000], Step: [300/301], Loss: 0.8869, Dev: 0.5070\n",
            "Epoch: [37/1000], Step: [100/301], Loss: 0.7070, Dev: 0.5060\n",
            "Epoch: [37/1000], Step: [200/301], Loss: 0.5615, Dev: 0.5280\n",
            "Epoch: [37/1000], Step: [300/301], Loss: 0.7982, Dev: 0.5240\n",
            "Epoch: [38/1000], Step: [100/301], Loss: 0.5995, Dev: 0.5110\n",
            "Epoch: [38/1000], Step: [200/301], Loss: 0.4924, Dev: 0.5220\n",
            "Epoch: [38/1000], Step: [300/301], Loss: 0.4106, Dev: 0.4830\n",
            "Epoch: [39/1000], Step: [100/301], Loss: 0.6025, Dev: 0.5210\n",
            "Epoch: [39/1000], Step: [200/301], Loss: 0.7683, Dev: 0.5560\n",
            "Epoch: [39/1000], Step: [300/301], Loss: 0.3282, Dev: 0.5430\n",
            "Epoch: [40/1000], Step: [100/301], Loss: 0.6409, Dev: 0.5180\n",
            "Epoch: [40/1000], Step: [200/301], Loss: 0.5241, Dev: 0.4980\n",
            "Epoch: [40/1000], Step: [300/301], Loss: 1.0322, Dev: 0.5480\n",
            "Epoch: [41/1000], Step: [100/301], Loss: 0.3575, Dev: 0.5350\n",
            "Epoch: [41/1000], Step: [200/301], Loss: 0.3986, Dev: 0.5330\n",
            "Epoch: [41/1000], Step: [300/301], Loss: 0.7242, Dev: 0.5330\n",
            "Epoch: [42/1000], Step: [100/301], Loss: 0.7129, Dev: 0.5470\n",
            "Epoch: [42/1000], Step: [200/301], Loss: 0.4999, Dev: 0.5070\n",
            "Epoch: [42/1000], Step: [300/301], Loss: 0.7081, Dev: 0.5240\n",
            "Epoch: [43/1000], Step: [100/301], Loss: 0.5925, Dev: 0.5350\n",
            "Epoch: [43/1000], Step: [200/301], Loss: 0.6363, Dev: 0.5100\n",
            "Epoch: [43/1000], Step: [300/301], Loss: 0.5814, Dev: 0.5290\n",
            "Epoch: [44/1000], Step: [100/301], Loss: 0.7701, Dev: 0.5270\n",
            "Epoch: [44/1000], Step: [200/301], Loss: 0.7817, Dev: 0.5470\n",
            "Epoch: [44/1000], Step: [300/301], Loss: 0.8488, Dev: 0.5600\n",
            "Epoch: [45/1000], Step: [100/301], Loss: 0.6305, Dev: 0.5130\n",
            "Epoch: [45/1000], Step: [200/301], Loss: 0.7122, Dev: 0.5260\n",
            "Epoch: [45/1000], Step: [300/301], Loss: 0.7043, Dev: 0.5400\n",
            "Epoch: [46/1000], Step: [100/301], Loss: 0.5368, Dev: 0.5330\n",
            "Epoch: [46/1000], Step: [200/301], Loss: 0.4090, Dev: 0.5400\n",
            "Epoch: [46/1000], Step: [300/301], Loss: 0.3962, Dev: 0.5450\n",
            "Epoch: [47/1000], Step: [100/301], Loss: 0.6696, Dev: 0.5250\n",
            "Epoch: [47/1000], Step: [200/301], Loss: 0.6420, Dev: 0.5350\n",
            "Epoch: [47/1000], Step: [300/301], Loss: 0.6127, Dev: 0.5670\n",
            "Epoch: [48/1000], Step: [100/301], Loss: 0.6039, Dev: 0.5520\n",
            "Epoch: [48/1000], Step: [200/301], Loss: 0.4624, Dev: 0.5240\n",
            "Epoch: [48/1000], Step: [300/301], Loss: 0.6245, Dev: 0.5410\n",
            "Epoch: [49/1000], Step: [100/301], Loss: 0.7389, Dev: 0.5410\n",
            "Epoch: [49/1000], Step: [200/301], Loss: 0.8605, Dev: 0.5570\n",
            "Epoch: [49/1000], Step: [300/301], Loss: 0.3647, Dev: 0.5500\n",
            "Epoch: [50/1000], Step: [100/301], Loss: 0.2865, Dev: 0.5510\n",
            "Epoch: [50/1000], Step: [200/301], Loss: 0.6264, Dev: 0.5050\n",
            "Epoch: [50/1000], Step: [300/301], Loss: 0.8058, Dev: 0.5380\n",
            "Epoch: [51/1000], Step: [100/301], Loss: 0.7002, Dev: 0.5270\n",
            "Epoch: [51/1000], Step: [200/301], Loss: 0.5785, Dev: 0.5680\n",
            "Epoch: [51/1000], Step: [300/301], Loss: 0.8893, Dev: 0.5480\n",
            "Epoch: [52/1000], Step: [100/301], Loss: 0.4865, Dev: 0.5490\n",
            "Epoch: [52/1000], Step: [200/301], Loss: 0.4110, Dev: 0.5500\n",
            "Epoch: [52/1000], Step: [300/301], Loss: 0.6964, Dev: 0.5500\n",
            "Epoch: [53/1000], Step: [100/301], Loss: 0.6027, Dev: 0.5530\n",
            "Epoch: [53/1000], Step: [200/301], Loss: 0.9364, Dev: 0.5420\n",
            "Epoch: [53/1000], Step: [300/301], Loss: 0.5773, Dev: 0.5720\n",
            "Epoch: [54/1000], Step: [100/301], Loss: 0.4401, Dev: 0.5420\n",
            "Epoch: [54/1000], Step: [200/301], Loss: 0.5550, Dev: 0.5550\n",
            "Epoch: [54/1000], Step: [300/301], Loss: 0.5735, Dev: 0.5530\n",
            "Epoch: [55/1000], Step: [100/301], Loss: 0.7288, Dev: 0.5460\n",
            "Epoch: [55/1000], Step: [200/301], Loss: 0.6075, Dev: 0.5440\n",
            "Epoch: [55/1000], Step: [300/301], Loss: 0.3003, Dev: 0.5530\n",
            "Epoch: [56/1000], Step: [100/301], Loss: 0.6030, Dev: 0.5350\n",
            "Epoch: [56/1000], Step: [200/301], Loss: 0.4443, Dev: 0.5350\n",
            "Epoch: [56/1000], Step: [300/301], Loss: 0.5372, Dev: 0.5410\n",
            "Epoch: [57/1000], Step: [100/301], Loss: 0.3964, Dev: 0.5500\n",
            "Epoch: [57/1000], Step: [200/301], Loss: 0.3161, Dev: 0.5420\n",
            "Epoch: [57/1000], Step: [300/301], Loss: 0.5737, Dev: 0.5470\n",
            "Epoch: [58/1000], Step: [100/301], Loss: 0.3309, Dev: 0.5680\n",
            "Epoch: [58/1000], Step: [200/301], Loss: 0.3798, Dev: 0.5530\n",
            "Epoch: [58/1000], Step: [300/301], Loss: 1.0754, Dev: 0.5650\n",
            "Epoch: [59/1000], Step: [100/301], Loss: 0.5842, Dev: 0.5450\n",
            "Epoch: [59/1000], Step: [200/301], Loss: 0.4004, Dev: 0.5590\n",
            "Epoch: [59/1000], Step: [300/301], Loss: 0.5336, Dev: 0.5450\n",
            "Epoch: [60/1000], Step: [100/301], Loss: 0.6220, Dev: 0.5450\n",
            "Epoch: [60/1000], Step: [200/301], Loss: 0.3542, Dev: 0.5440\n",
            "Epoch: [60/1000], Step: [300/301], Loss: 0.2743, Dev: 0.5630\n",
            "Epoch: [61/1000], Step: [100/301], Loss: 0.3437, Dev: 0.5570\n",
            "Epoch: [61/1000], Step: [200/301], Loss: 0.3479, Dev: 0.5410\n",
            "Epoch: [61/1000], Step: [300/301], Loss: 0.5141, Dev: 0.5470\n",
            "Epoch: [62/1000], Step: [100/301], Loss: 0.3600, Dev: 0.5390\n",
            "Epoch: [62/1000], Step: [200/301], Loss: 0.2672, Dev: 0.5610\n",
            "Epoch: [62/1000], Step: [300/301], Loss: 0.4786, Dev: 0.5190\n",
            "Epoch: [63/1000], Step: [100/301], Loss: 0.6201, Dev: 0.5240\n",
            "Epoch: [63/1000], Step: [200/301], Loss: 0.5186, Dev: 0.5460\n",
            "Epoch: [63/1000], Step: [300/301], Loss: 0.4229, Dev: 0.5660\n",
            "Epoch: [64/1000], Step: [100/301], Loss: 0.3291, Dev: 0.5540\n",
            "Epoch: [64/1000], Step: [200/301], Loss: 0.5914, Dev: 0.5430\n",
            "Epoch: [64/1000], Step: [300/301], Loss: 0.4471, Dev: 0.5650\n",
            "Epoch: [65/1000], Step: [100/301], Loss: 0.5568, Dev: 0.5450\n",
            "Epoch: [65/1000], Step: [200/301], Loss: 0.4816, Dev: 0.5450\n",
            "Epoch: [65/1000], Step: [300/301], Loss: 0.3506, Dev: 0.5610\n",
            "Epoch: [66/1000], Step: [100/301], Loss: 0.3554, Dev: 0.5430\n",
            "Epoch: [66/1000], Step: [200/301], Loss: 0.8441, Dev: 0.5610\n",
            "Epoch: [66/1000], Step: [300/301], Loss: 0.4949, Dev: 0.5540\n",
            "Epoch: [67/1000], Step: [100/301], Loss: 0.3111, Dev: 0.5490\n",
            "Epoch: [67/1000], Step: [200/301], Loss: 0.6477, Dev: 0.5600\n",
            "Epoch: [67/1000], Step: [300/301], Loss: 0.4349, Dev: 0.5580\n",
            "Epoch: [68/1000], Step: [100/301], Loss: 0.5389, Dev: 0.5520\n",
            "Epoch: [68/1000], Step: [200/301], Loss: 0.5005, Dev: 0.5640\n",
            "Epoch: [68/1000], Step: [300/301], Loss: 0.4711, Dev: 0.5220\n",
            "Epoch: [69/1000], Step: [100/301], Loss: 0.2901, Dev: 0.5620\n",
            "Epoch: [69/1000], Step: [200/301], Loss: 0.3641, Dev: 0.5490\n",
            "Epoch: [69/1000], Step: [300/301], Loss: 0.4922, Dev: 0.5910\n",
            "Epoch: [70/1000], Step: [100/301], Loss: 1.0096, Dev: 0.5740\n",
            "Epoch: [70/1000], Step: [200/301], Loss: 0.6402, Dev: 0.5410\n",
            "Epoch: [70/1000], Step: [300/301], Loss: 0.3167, Dev: 0.5580\n",
            "Epoch: [71/1000], Step: [100/301], Loss: 0.3996, Dev: 0.5290\n",
            "Epoch: [71/1000], Step: [200/301], Loss: 0.7178, Dev: 0.5650\n",
            "Epoch: [71/1000], Step: [300/301], Loss: 0.4682, Dev: 0.5550\n",
            "Epoch: [72/1000], Step: [100/301], Loss: 0.5002, Dev: 0.5630\n",
            "Epoch: [72/1000], Step: [200/301], Loss: 0.2714, Dev: 0.5330\n",
            "Epoch: [72/1000], Step: [300/301], Loss: 0.5788, Dev: 0.5480\n",
            "Epoch: [73/1000], Step: [100/301], Loss: 0.2560, Dev: 0.5510\n",
            "Epoch: [73/1000], Step: [200/301], Loss: 0.4619, Dev: 0.5540\n",
            "Epoch: [73/1000], Step: [300/301], Loss: 0.4611, Dev: 0.5530\n",
            "Epoch: [74/1000], Step: [100/301], Loss: 0.5380, Dev: 0.5790\n",
            "Epoch: [74/1000], Step: [200/301], Loss: 0.6796, Dev: 0.5340\n",
            "Epoch: [74/1000], Step: [300/301], Loss: 0.2241, Dev: 0.5580\n",
            "Epoch: [75/1000], Step: [100/301], Loss: 0.3940, Dev: 0.5760\n",
            "Epoch: [75/1000], Step: [200/301], Loss: 0.4345, Dev: 0.5320\n",
            "Epoch: [75/1000], Step: [300/301], Loss: 0.1462, Dev: 0.5170\n",
            "Epoch: [76/1000], Step: [100/301], Loss: 0.7922, Dev: 0.5470\n",
            "Epoch: [76/1000], Step: [200/301], Loss: 0.3774, Dev: 0.5310\n",
            "Epoch: [76/1000], Step: [300/301], Loss: 0.4671, Dev: 0.5410\n",
            "Epoch: [77/1000], Step: [100/301], Loss: 0.5032, Dev: 0.5410\n",
            "Epoch: [77/1000], Step: [200/301], Loss: 0.5434, Dev: 0.5550\n",
            "Epoch: [77/1000], Step: [300/301], Loss: 0.2948, Dev: 0.5750\n",
            "Epoch: [78/1000], Step: [100/301], Loss: 0.4897, Dev: 0.5810\n",
            "Epoch: [78/1000], Step: [200/301], Loss: 0.5786, Dev: 0.5470\n",
            "Epoch: [78/1000], Step: [300/301], Loss: 0.7173, Dev: 0.5420\n",
            "Epoch: [79/1000], Step: [100/301], Loss: 0.2316, Dev: 0.5580\n",
            "Epoch: [79/1000], Step: [200/301], Loss: 0.1984, Dev: 0.5590\n",
            "Epoch: [79/1000], Step: [300/301], Loss: 0.3778, Dev: 0.5670\n",
            "Epoch: [80/1000], Step: [100/301], Loss: 0.3744, Dev: 0.5520\n",
            "Epoch: [80/1000], Step: [200/301], Loss: 0.2876, Dev: 0.5590\n",
            "Epoch: [80/1000], Step: [300/301], Loss: 0.2667, Dev: 0.5330\n",
            "Epoch: [81/1000], Step: [100/301], Loss: 0.3727, Dev: 0.5430\n",
            "Epoch: [81/1000], Step: [200/301], Loss: 0.3589, Dev: 0.5760\n",
            "Epoch: [81/1000], Step: [300/301], Loss: 0.3223, Dev: 0.5540\n",
            "Epoch: [82/1000], Step: [100/301], Loss: 0.3889, Dev: 0.5530\n",
            "Epoch: [82/1000], Step: [200/301], Loss: 0.4102, Dev: 0.5400\n",
            "Epoch: [82/1000], Step: [300/301], Loss: 0.6278, Dev: 0.5760\n",
            "Epoch: [83/1000], Step: [100/301], Loss: 0.2036, Dev: 0.5580\n",
            "Epoch: [83/1000], Step: [200/301], Loss: 0.5720, Dev: 0.5480\n",
            "Epoch: [83/1000], Step: [300/301], Loss: 0.4445, Dev: 0.5600\n",
            "Epoch: [84/1000], Step: [100/301], Loss: 0.3566, Dev: 0.5400\n",
            "Epoch: [84/1000], Step: [200/301], Loss: 0.4023, Dev: 0.5380\n",
            "Epoch: [84/1000], Step: [300/301], Loss: 0.4731, Dev: 0.5580\n",
            "Epoch: [85/1000], Step: [100/301], Loss: 0.4415, Dev: 0.5370\n",
            "Epoch: [85/1000], Step: [200/301], Loss: 0.4635, Dev: 0.5670\n",
            "Epoch: [85/1000], Step: [300/301], Loss: 0.6994, Dev: 0.5360\n",
            "Epoch: [86/1000], Step: [100/301], Loss: 0.2881, Dev: 0.5970\n",
            "Epoch: [86/1000], Step: [200/301], Loss: 0.5857, Dev: 0.5830\n",
            "Epoch: [86/1000], Step: [300/301], Loss: 0.5548, Dev: 0.5440\n",
            "Epoch: [87/1000], Step: [100/301], Loss: 0.1503, Dev: 0.5530\n",
            "Epoch: [87/1000], Step: [200/301], Loss: 0.4238, Dev: 0.5450\n",
            "Epoch: [87/1000], Step: [300/301], Loss: 0.2492, Dev: 0.5390\n",
            "Epoch: [88/1000], Step: [100/301], Loss: 0.3230, Dev: 0.5470\n",
            "Epoch: [88/1000], Step: [200/301], Loss: 0.3756, Dev: 0.5540\n",
            "Epoch: [88/1000], Step: [300/301], Loss: 0.2128, Dev: 0.5730\n",
            "Epoch: [89/1000], Step: [100/301], Loss: 0.5298, Dev: 0.5480\n",
            "Epoch: [89/1000], Step: [200/301], Loss: 0.1698, Dev: 0.5530\n",
            "Epoch: [89/1000], Step: [300/301], Loss: 0.3549, Dev: 0.5650\n",
            "Epoch: [90/1000], Step: [100/301], Loss: 0.3915, Dev: 0.5360\n",
            "Epoch: [90/1000], Step: [200/301], Loss: 0.5596, Dev: 0.5810\n",
            "Epoch: [90/1000], Step: [300/301], Loss: 0.4642, Dev: 0.5740\n",
            "Epoch: [91/1000], Step: [100/301], Loss: 0.5464, Dev: 0.5930\n",
            "Epoch: [91/1000], Step: [200/301], Loss: 0.1252, Dev: 0.5230\n",
            "Epoch: [91/1000], Step: [300/301], Loss: 0.4084, Dev: 0.5510\n",
            "Epoch: [92/1000], Step: [100/301], Loss: 0.2386, Dev: 0.5420\n",
            "Epoch: [92/1000], Step: [200/301], Loss: 0.5605, Dev: 0.5760\n",
            "Epoch: [92/1000], Step: [300/301], Loss: 0.5944, Dev: 0.5860\n",
            "Epoch: [93/1000], Step: [100/301], Loss: 0.6238, Dev: 0.5560\n",
            "Epoch: [93/1000], Step: [200/301], Loss: 0.3184, Dev: 0.5540\n",
            "Epoch: [93/1000], Step: [300/301], Loss: 0.3409, Dev: 0.5460\n",
            "Epoch: [94/1000], Step: [100/301], Loss: 0.3717, Dev: 0.5470\n",
            "Epoch: [94/1000], Step: [200/301], Loss: 0.5213, Dev: 0.5550\n",
            "Epoch: [94/1000], Step: [300/301], Loss: 0.4653, Dev: 0.5530\n",
            "Epoch: [95/1000], Step: [100/301], Loss: 0.2886, Dev: 0.5490\n",
            "Epoch: [95/1000], Step: [200/301], Loss: 0.5255, Dev: 0.5540\n",
            "Epoch: [95/1000], Step: [300/301], Loss: 0.4801, Dev: 0.5800\n",
            "Epoch: [96/1000], Step: [100/301], Loss: 0.2681, Dev: 0.5650\n",
            "Epoch: [96/1000], Step: [200/301], Loss: 0.5368, Dev: 0.5630\n",
            "Epoch: [96/1000], Step: [300/301], Loss: 0.4683, Dev: 0.5500\n",
            "Epoch: [97/1000], Step: [100/301], Loss: 0.5451, Dev: 0.5660\n",
            "Epoch: [97/1000], Step: [200/301], Loss: 0.2660, Dev: 0.5550\n",
            "Epoch: [97/1000], Step: [300/301], Loss: 0.3196, Dev: 0.5690\n",
            "Epoch: [98/1000], Step: [100/301], Loss: 0.4612, Dev: 0.5600\n",
            "Epoch: [98/1000], Step: [200/301], Loss: 0.3187, Dev: 0.5590\n",
            "Epoch: [98/1000], Step: [300/301], Loss: 0.3082, Dev: 0.5560\n",
            "Epoch: [99/1000], Step: [100/301], Loss: 0.3943, Dev: 0.5630\n",
            "Epoch: [99/1000], Step: [200/301], Loss: 0.3763, Dev: 0.5650\n",
            "Epoch: [99/1000], Step: [300/301], Loss: 0.5113, Dev: 0.5900\n",
            "Epoch: [100/1000], Step: [100/301], Loss: 0.4748, Dev: 0.5660\n",
            "Epoch: [100/1000], Step: [200/301], Loss: 0.2007, Dev: 0.5930\n",
            "Epoch: [100/1000], Step: [300/301], Loss: 0.3578, Dev: 0.5690\n",
            "Epoch: [101/1000], Step: [100/301], Loss: 0.4078, Dev: 0.5610\n",
            "Epoch: [101/1000], Step: [200/301], Loss: 0.5854, Dev: 0.5580\n",
            "Epoch: [101/1000], Step: [300/301], Loss: 0.3676, Dev: 0.5610\n",
            "Epoch: [102/1000], Step: [100/301], Loss: 0.3415, Dev: 0.5700\n",
            "Epoch: [102/1000], Step: [200/301], Loss: 0.4760, Dev: 0.5530\n",
            "Epoch: [102/1000], Step: [300/301], Loss: 0.3518, Dev: 0.5510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101596npWy9o",
        "outputId": "edd52414-d648-46d5-8439-de9549263bbc"
      },
      "source": [
        "best_acc, best_epochs, best_lr, best_l2"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.893, 2, 1.0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRUlRGIWiqW9"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zDrPphzsTBf"
      },
      "source": [
        "## Validity Check\n",
        "This is a way for you to check whether you accidentially renamed answer variables or functions that we will use for automatic evaluation. Note that this is not a comprehensive list and we do not check here whether you accidentially changed the function signatures, so failing this validity check is only a sufficient condition for telling you something went wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEnoXoVjsVQ_"
      },
      "source": [
        "for answer in [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, QFormulation]:\n",
        "  assert isinstance(answer, str)\n",
        "\n",
        "for fun in [\n",
        "    construct_scaled_identity, \n",
        "    mean_diagonal, \n",
        "    bottom_right_matrix,\n",
        "    transpose_sum,\n",
        "    matrixvector1,\n",
        "    matrixvector2,\n",
        "    matrixvector3,\n",
        "    matrixvector4,\n",
        "    matrixvector5,\n",
        "    fw,\n",
        "    bw,\n",
        "    SortBy,\n",
        "    collapse_mr_dot,\n",
        "    LogisticRegressionMeanPooling,\n",
        "    add_features,\n",
        "    accuracy,\n",
        "    training_loop\n",
        "    ]:\n",
        "  assert callable(fun)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKyuktsCfKZK"
      },
      "source": [
        ""
      ],
      "execution_count": 50,
      "outputs": []
    }
  ]
}
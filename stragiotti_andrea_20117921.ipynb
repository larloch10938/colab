{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stragiotti_andrea_20117921.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lich070192/colab/blob/main/stragiotti_andrea_20117921.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DNj4zyIGKa"
      },
      "source": [
        "# Assignment 1\n",
        "The first assignment has two parts. The first part concerns PyTorch and the second part is about feature engineering for a basic NLP task.\n",
        "## Instructions\n",
        "\n",
        "1.   Make a copy of this notebook \n",
        "  - Click on \"File -> Save a copy in Drive\" and open it in Colab afterwards\n",
        "  - Alternatively, download the notebook and work on it on your local machine. However, keep in mind that you will have to make sure it still runs on Colab afterwards and does not depend on any packages that you installed locally\n",
        "2.   Rename your notebook to **surname_forename_studentnumber.ipynb**\n",
        "  - Make sure to exactly follow this naming scheme (don't replace `_` with `-` or something like that)\n",
        "  - **Failure to comply with this scheme results in -10 points!**\n",
        "3.   For math exercises, use $\\LaTeX$  to typset your answer\n",
        "4.   For coding exercises, insert your code at `# TODO` statements\n",
        "5.   For multiple-choice questions, choose an answer from the drop-down list\n",
        "6.   Before submitting your notebook, **make sure that it runs without errors when executed from start to end on Colab**\n",
        "  - To check this, reload your notebook and the Python kernel, and run the notebook from the first to the last cell\n",
        "  - **If your notebook throws any errors, you will be penalized by -25 points in addition to any penalities from incorrect answers**\n",
        "  - We are not going to fix any errors (no matter how small) to make your code work\n",
        "7.  Download your notebook and submit it on Moodle\n",
        "  - Click on \"File -> Download .ipynb\"\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvB4c3R4II91"
      },
      "source": [
        "## Notebook Setup [don't change!]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0w7l3EpExU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f39f61-befb-408b-d048-78ce0d97c2bc"
      },
      "source": [
        "%%shell\n",
        "pip install torch"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqig2nu9Eztp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "19c5027d-f8ab-4f55-e237-fa6d130c4549"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "torch.__version__ "
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDNBpMBGhsj"
      },
      "source": [
        "# Part I: PyTorch [50 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucOi5QgPG3Jg"
      },
      "source": [
        "## Linear Algebra [30 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJ4t0Mk0jlz"
      },
      "source": [
        "### PyTorch Tensors [5 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZNaDhZHKYFL"
      },
      "source": [
        "#### Construct Scaled Identity Matrix [1 point]\n",
        "Given $n \\in \\mathbb{N}$ and $c \\in \\mathbb{R}$, construct a matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\ \\times\\ n}$ where $\\mathbf{X}$ has $c$ on its diagonal and zeros everywhere else."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwYNaam7L0Aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86049db5-fd6f-4588-d37a-9f6d54a5ffd0"
      },
      "source": [
        "def construct_scaled_identity(n, c):\n",
        "  a = torch.zeros((n, n))\n",
        "  # np.fill_diagonal(a, c)\n",
        "  torch.diagonal(a).fill_(c)\n",
        "  return a\n",
        "\n",
        "construct_scaled_identity(4, 3.2)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 3.2000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 3.2000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 3.2000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LN669ufJMsq"
      },
      "source": [
        "#### Mean Diagonal [1 point]\n",
        "Given a square matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\ \\times\\ n}$, return the mean of its diagonal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cqjyj6rJ6Pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e411e82-79e0-417f-a4ce-a2dbef9727ea"
      },
      "source": [
        "def mean_diagonal(x):\n",
        "  a = torch.mean(torch.diagonal(x))\n",
        "  return a\n",
        "\n",
        "x = torch.arange(0, 16, dtype=torch.float).view(4, 4)\n",
        "mean_diagonal(x)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv9v5j0AMfpO"
      },
      "source": [
        "#### Indexing [1 point]\n",
        "Given a matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\ \\times\\ m}$ and $i,j \\in \\mathbb{N}$, return the submatrix $\\mathbf{Y}\\in\\mathbb{R}^{i\\ \\times\\ j}$ of the last i rows and last j columns of $\\mathbf{X}$ (i.e. the bottom right submatrix of the given size). You can assume that $i \\leq n$ and $j \\leq m$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXTcoO6Nj_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6814d2-f729-43ab-80a6-7d32c678f6ad"
      },
      "source": [
        "def bottom_right_matrix(x, i, j):\n",
        "  a = x[-i:, -j:]\n",
        "  return a\n",
        "\n",
        "x = torch.arange(0, 12).view(3, 4)\n",
        "bottom_right_matrix(x, 2, 2)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6,  7],\n",
              "        [10, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWooDl4O0ooy"
      },
      "source": [
        "#### Transpose Sum [2 points]\n",
        "Given a tensor $\\mathcal{X}\\in\\mathbb{R}^{i\\ \\times\\ j\\ \\times\\ k}$, return a transposed tensor $\\mathcal{y}\\in\\mathbb{R}^{j\\ \\times\\ i}$ whose values in the third dimension are summed up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_o8MrXj0nQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558ccf81-b600-4db3-9552-9c40964a8ce7"
      },
      "source": [
        "def transpose_sum(x):\n",
        "  a = torch.sum(x, dim=2)\n",
        "  a = torch.transpose(a, 0, 1)\n",
        "  return a\n",
        "\n",
        "x = torch.arange(0, 12).view(2, 3, 2)\n",
        "transpose_sum(x)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1, 13],\n",
              "        [ 5, 17],\n",
              "        [ 9, 21]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhuGQ1I_klmv"
      },
      "source": [
        "### Matrix-vector Multiplication [10 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ7YPXIrIpOg"
      },
      "source": [
        "Implement five unique ways for multiplying a matrix A with a vector b. **Each PyTorch function is allowed to be used in only one of the five implementations**. For instance, if you use `unsqueeze` in one of the methods, you are not allowed to use it for the other five implementations. Furthermore, functions in `torch` and in `torch.Tensor` are treated as the same function (i.e. using `torch.add(x, y)`, `x.add(y)` and `x + y` are all treated as the same function and hence are not allowed to be used in more than one implementation). Your code needs to be applicable to any matrix $A \\in \\mathbb{R}^{n\\ \\times\\ m }$ and vector $b\\in\\mathbb{R}^m$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FPwXxLEIdPL"
      },
      "source": [
        "def matrixvector1(A, b):  \n",
        "  return torch.matmul(A, b)\n",
        "\n",
        "def matrixvector2(A, b):\n",
        "  return A @ b\n",
        "\n",
        "def matrixvector3(A, b):\n",
        "  return A.mm(b.unsqueeze(1)).squeeze(-1)\n",
        "\n",
        "def matrixvector4(A, b):\n",
        "  return torch.einsum('ij,j->i', A, b)\n",
        "\n",
        "def matrixvector5(A, b):\n",
        "  return torch.mv(A, b)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG2Uk7oRkqeO"
      },
      "source": [
        "### Backprop [15 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-AL4Dl8xJF8"
      },
      "source": [
        "#### Forward [2 points]\n",
        "Implement $\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)$ in PyTorch without using a linear layer implementation (i.e. do the matrix-vector mulitplication and addition of a bias term yourself). Note that we are not looking for a batched implementation, so assume $\\mathbf{y},\\mathbf{b} \\in \\mathbb{R}^n, \\mathbf{x}\\in\\mathbb{R}^m$ and $\\mathbf{W}\\in\\mathbb{R}^{n\\ \\times\\ m}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcCqnNPtv0nu"
      },
      "source": [
        "def fw(y, W, x, b):\n",
        "  result = y * torch.tanh(torch.matmul(W, x) + b)\n",
        "  return result"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xav_9KwaxkFL"
      },
      "source": [
        "#### Gradient [10 points]\n",
        "Derive $\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}}\\left[\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)\\right]$ analytically. Here $\\mathbf{z}$ is an _upstream (error) gradient_ and we are interested in calculating the _downstream gradient_ for $\\mathbf{x}$. Make sure to write down all intermediate steps and not just the final result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn3Bu-y8y1Co"
      },
      "source": [
        "Let's define:\n",
        "\\begin{equation}\n",
        "h = \\text{tanh}(z)\n",
        "\\\\\n",
        "z = Wx + b\n",
        "\\end{equation}\n",
        "We can therefore write:\n",
        "\\begin{align}\n",
        "\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}}\\left[\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)\\right] \n",
        "&=\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}]\n",
        "\\end{align}\n",
        "Also, by using the chain rule:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial \\mathbf{x}} = \\frac{\\partial}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}}\n",
        "\\end{equation}\n",
        "And, deriving the product:\n",
        "\\begin{equation}\n",
        "\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}] = \\mathbf{z}^\\top \\mathbf{y} \\frac{\\partial} {\\partial \\mathbf{x}} \\odot{h} + {h} \\odot \\frac{\\partial} {\\partial \\mathbf{x}} \\mathbf{y}\n",
        "\\end{equation}\n",
        "We can remove the second portion since:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial} {\\partial \\mathbf{x}} \\mathbf{y} = 0\n",
        "\\end{equation}\n",
        "So finally:\n",
        "\\begin{equation}\n",
        "\\\\\n",
        "\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}] = \\mathbf{z}^\\top\\frac{\\partial}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}]\n",
        "\\\\\n",
        "\\mathbf{z}^\\top\\frac{\\partial}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}} [\\mathbf{y}\\odot{h}] = \\mathbf{z}^\\top diag(\\mathbf{y}\\odot\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}}[h])\n",
        "\\\\\n",
        "\\mathbf{z}^\\top diag(\\mathbf{y}\\odot\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\mathbf{x}}[\\text{tanh}(z)] = \\mathbf{z}^\\top diag(\\mathbf{y}\\odot(1 - \\text{tanh}(z)^2))\\frac{\\partial z}{\\partial \\mathbf{x}}[z]\n",
        "\\\\\n",
        "\\mathbf{z}^\\top diag(\\mathbf{y}\\odot(1 - \\text{tanh}(z)^2))\\frac{\\partial z}{\\partial \\mathbf{x}}[z] = \\mathbf{z}^\\top diag(\\mathbf{y}\\odot(1 - \\text{tanh}(z)^2))W\n",
        "\\end{equation}\n",
        "Where:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial h} = diag(\\mathbf{y})\n",
        "\\\\\n",
        "\\frac{\\partial h}{\\partial z} = 1 - tanh(z)^2\n",
        "\\\\\n",
        "\\frac{\\partial z}{\\partial \\mathbf{x}} = W\n",
        "\\end{equation}\n",
        "The first derivative is defined as such by using a property of the Hadamard product where:\n",
        "\\begin{equation}\n",
        "\\mathbf{A}\\odot\\mathbf{X} = diag(\\mathbf{A})\\mathbf{X}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by45qCyMxrBr"
      },
      "source": [
        "#### Backward [3 points]\n",
        "Implement the calculation for $\\mathbf{z}^\\top\\frac{\\partial}{\\partial \\mathbf{x}}\\left[\\mathbf{y}\\odot\\text{tanh}\\left(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\right)\\right]$  in PyTorch (i.e. without using PyTorch Autograd's `.backward`) using your derivation above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqIFSUV7yd6m"
      },
      "source": [
        "def bw(y, W, x, b, grad_output):\n",
        "  result = torch.diag(y * (1 - torch.tanh(torch.matmul(W, x) + b)**2))\n",
        "  result = grad_output.T.matmul(result.matmul(W))\n",
        "  return result"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvBLkJKhG6Rg"
      },
      "source": [
        "## SortBy PyTorch Autograd Function [10 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsLf0kvGG9W4"
      },
      "source": [
        "Implement a PyTorch Autograd function `SortBy` which takes two inputs:\n",
        "- `x` is a matrix of size `m x n` \n",
        "- `s` is an accompanying vector of size `m`\n",
        "\n",
        "`SortBy` should sort the position of the row vectors in `x` using the accompanying scores in `s` in ascending order. For example, given\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{X} &= \\left[\\begin{matrix}\n",
        "0.2 & -0.4 & 0.3\\\\\n",
        "1.2 & 2.3 & -2.1\\\\\n",
        "0.1 & -0.1 & 2\n",
        "\\end{matrix}\\right]\n",
        "&\\mathbf{s} &=\\left[\\begin{matrix}\n",
        "0.2\\\\\n",
        "-0.1\\\\\n",
        "3\n",
        "\\end{matrix}\\right]\n",
        "\\end{align}\n",
        "$$ the forward pass of `SortBy` should return\n",
        "$$\n",
        "\\mathbf{Y} = \\left[\\begin{matrix}\n",
        "1.2 & 2.3 & -2.1\\\\\n",
        "0.2 & -0.4 & 0.3\\\\\n",
        "0.1 & -0.1 & 2\n",
        "\\end{matrix}\\right]\n",
        "$$.\n",
        "\n",
        "Furthermore, given an upstream gradient `grad_output`  (i.e. a matrix of the same size as X), the backward pass of `SortBy` should calculate the gradient of `x`, effectively rerouting the gradient to the original position of the vectors before sorting. For example, if the first row vector of the upstream gradient  in our example above is a vector $\\mathbf{z}$, the gradient of `x` would have $\\mathbf{z}$ as its second row vector.\n",
        "\n",
        "Note that, `SortBy` will only be differentiable w.r.t. to x, and is not be differentiable w.r.t. the sorting procedure to provie a gradient for `s`. **You are not allowed to use any Python loops in your implementation. If you use Python loops for your solution, we will only give you half of the points!**\n",
        "\n",
        "Hints:\n",
        "- You are allowed to use `torch.sort` in your implementation of the forward pass.\n",
        "- Similarly to the example we had in the lecture, you can use the context `ctx` to save tensors on the forward pass that you might need to reuse on the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-YgSz5uQI_Q"
      },
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class SortBy(Function): \n",
        "  @staticmethod\n",
        "  def forward(ctx, x, s):\n",
        "    result = x[s.sort().indices]\n",
        "    ctx.save_for_backward(result, s)\n",
        "    return result\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    result, s = ctx.saved_tensors\n",
        "    return grad_output[s.sort().indices], None"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_BF7G3UHFcp"
      },
      "source": [
        "## Multiple Choice Quiz [10 points]\n",
        "\n",
        "Answer the following questions by selecting the correct most specific answer (or `None` in case all answers are wrong).\n",
        "\n",
        "1. Which of the following operations cannot be calculated using `@`?\n",
        "2. What is gradient checking for?\n",
        "3. Why don't we use the finite differences method of gradient checking to calculate gradients instead of using backpropagation?\n",
        "4. Which of the following operations cannot be expressed as a single einsum string?\n",
        "5. When should you prefer using `view` instead of `reshape`?\n",
        "6. Which of the following statements is true if you construct a PyTorch tensor from a NumPy array using `torch.from_numpy`?\n",
        "7. Which one is a sufficient condition for being able to broadcast an operation between two tensors?\n",
        "8. What is the difference between a torch.Tensor and a torch.nn.Parameter?\n",
        "9. Given a convex loss function and a sufficiently small learning rate, stochastic gradient descent is guaranteed to?\n",
        "10. Given a non-convex loss function and a very large learning rate, stochastic gradient descent is guaranteed to?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWbew_PsFG5u"
      },
      "source": [
        "#@title Answers { run: \"auto\" }\n",
        "Q1 = \"None of the above\" #@param [\"Matrix-matrix multiplication\", \"Matrix-vector multiplication\", \"Vector-vector multiplication\", \"Tensor-matrix multiplication\", \"Tensor-vector multiplication\", \"None of the above\"]\n",
        "Q2 = \"It tests whether the forward pass of a function is consistent with the backward pass\" #@param [\"It tests whether the forward pass of a function is consistent with the backward pass\", \"It is used at runtime to check for numerical instabilities in the backward pass\", \"It tests wether the function and its gradient have been implemented correctly\", \"It tests whether the norm of the gradients of a function are bounded\", \"None of the above\"]\n",
        "Q3 = \"It would be too slow\" #@param [\"It cannot be used to approximate the gradient accurately enough\", \"It can only be used to calculate the gradient of single functions and not for chained functions which are commonly used in deep learning models\", \"It would be too slow\", \"None of the above\"]\n",
        "Q4 = \"None of the above\" #@param [\"The transpose of an order-three tensor\", \"The sum of the diagonal of a square matrix\", \"The outer product of two matrices\", \"None of the above\"]\n",
        "Q5 = \"When the tensor is contiguous\" #@param [\"When the tensor is non-contiguous\", \"When the tensor is contiguous\", \"None of the above\"]\n",
        "Q6 = \"They point to the same memory and altering one will change the other\" #@param [\"Gradients can be calculated using both, the PyTorch tensor and the NumPy array\", \"They point to the same memory and altering one will change the other\", \"The PyTorch tensor cannot be mapped back to a NumPy array\", \"None of the above\"]\n",
        "Q7 = \"One of the two tensors is a scalar\" #@param [\"One of the two tensors is a scalar\", \"One of the tensors has a singleton dimension\", \"The two tensors have the same number of dimensions\", \"None of the above\"]\n",
        "Q8 = \"Parameters get associated with a model when assigned to a member of the model's modules\" #@param [\"Parameters are mutable and tensors are not\", \"Parameters get associated with a model when assigned to a member of the model's modules\", \"Parameters need to be flattened into vectors whereas tensors can be high-dimensional\", \"None of the above\"]\n",
        "Q9 = \"All of the above\" #@param [\"Find a local optimum\", \"Find the global optimum\", \"All of the above\", \"None of the above\"]\n",
        "Q10 = \"None of the above\" #@param [\"Find a local optimum\", \"Find the global optimum\", \"Converge to a saddle point\", \"All of the above\", \"None of the above\"]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZwY626nGlbb"
      },
      "source": [
        "# Part II: Feature Engineering [50 points]\n",
        "\n",
        "In this section you will develop a logistic regression model for sentiment prediction.  \n",
        "\n",
        "## Setup \n",
        "First we download the [sentence polarity dataset v1.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz) from this [website](http://www.cs.cornell.edu/people/pabo/movie-review-data/) using a few shell commands. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwR7cadCuLD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2486047d-ba40-4a89-b93b-97a984a9e97d"
      },
      "source": [
        "%%shell\n",
        "wget http://www.cs.cornell.edu/People/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
        "tar -xzf rt-polaritydata.tar.gz\n",
        "mv rt-polaritydata.README.1.0.txt rt-polaritydata\n",
        "cd rt-polaritydata\n",
        "iconv -f cp1252 -t utf-8 < rt-polarity.neg > rt-polarity.neg.utf8\n",
        "iconv -f cp1252 -t utf-8 < rt-polarity.pos > rt-polarity.pos.utf8\n",
        "perl -ne 'print \"neg\\t\" . $_' <  rt-polarity.neg.utf8 > rt-polarity.neg.utf8.tsv\n",
        "perl -ne 'print \"pos\\t\" . $_' <  rt-polarity.pos.utf8 > rt-polarity.pos.utf8.tsv\n",
        "cat rt-polarity.neg.utf8.tsv rt-polarity.pos.utf8.tsv > rt-polarity.utf8.tsv"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 21:25:20--  http://www.cs.cornell.edu/People/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz [following]\n",
            "--2021-01-31 21:25:20--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
            "Reusing existing connection to www.cs.cornell.edu:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 487770 (476K) [application/x-gzip]\n",
            "Saving to: ‘rt-polaritydata.tar.gz.5’\n",
            "\n",
            "rt-polaritydata.tar 100%[===================>] 476.34K  2.47MB/s    in 0.2s    \n",
            "\n",
            "2021-01-31 21:25:20 (2.47 MB/s) - ‘rt-polaritydata.tar.gz.5’ saved [487770/487770]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2MTMQSMWPr9"
      },
      "source": [
        "Now we install [AllenNLP](https://allennlp.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf64vki1zt0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cba1ca-1635-4492-fad9-bbc74c2e613b"
      },
      "source": [
        "%%shell\n",
        "pip install allennlp==0.9"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp==0.9 in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.22.2.post1)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.16.63)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (21.1.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.6.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.2.5)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.5.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2018.9)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.8.1)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.6.2)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.1)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.7.0)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.7.0+cu101)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.2.2)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.0)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.1.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (5.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.19.5)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.12.1)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (3.0.10)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.10.0)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.4.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.5.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (0.17.0)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (2.1.9)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp==0.9) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9) (1.25.11)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==0.9) (0.3.4)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.63 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==0.9) (1.19.63)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==0.9) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9) (51.3.3)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9) (5.2.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9) (1.0.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9) (4.5.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (20.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (1.15.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9) (8.6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp==0.9) (2019.12.20)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==0.9) (3.12.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp==0.9) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp==0.9) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp==0.9) (3.7.4.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9) (2.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9) (2.4.7)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp==0.9) (1.8.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp==0.9) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==0.9) (3.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (0.8.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (2.0.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (1.0.5)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9) (0.9.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp==0.9) (0.1.95)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp==0.9) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (20.8)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (2.6.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (2.9.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (0.7.12)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (0.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (1.2.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==0.9) (3.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9) (1.1.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkhShc8WX9F"
      },
      "source": [
        "Next we implement a AllenNLP data loader for this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34_HDMys1sLg"
      },
      "source": [
        "from typing import Iterator, List, Dict, Optional\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "\n",
        "class PolarityDatasetReader(DatasetReader):\n",
        "    \"\"\"\n",
        "    DatasetReader for polarity data like \n",
        "\n",
        "        neg\\tI had better gone to Imperial\n",
        "    \"\"\"\n",
        "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None,\n",
        "                 tokenize_and_preprocess = lambda text: text.split(\" \")) -> None:\n",
        "        super().__init__(lazy=False)\n",
        "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "        self.tokenize_and_preprocess = tokenize_and_preprocess\n",
        "\n",
        "    def text_to_instance(self, tokens: List[Token], label: Optional[str] = None) -> Instance:\n",
        "        sentence_field = TextField(tokens, self.token_indexers)\n",
        "        fields = {\"sentence\": sentence_field}\n",
        "\n",
        "        if label:\n",
        "            label_field = LabelField(label=label)\n",
        "            fields[\"label\"] = label_field\n",
        "\n",
        "        return Instance(fields)\n",
        "\n",
        "    def _tokenize_and_preprocess(text):\n",
        "        return text.split(\" \")\n",
        "\n",
        "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                label, text = line.split(\"\\t\")\n",
        "                tokens = [Token(word) for word in self.tokenize_and_preprocess(text)]\n",
        "                yield self.text_to_instance(tokens, label)\n",
        "\n",
        "\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isDjFO7LXYf-"
      },
      "source": [
        "# Preprocessing [7pts]\n",
        "\n",
        "In order to fit our model, we will need to tokenize and preprocess the data. Write a dataset loader that preprocesses the data.\n",
        "\n",
        "Tokenization is an important field of NLP, and can make a large difference to downstream performance. Luckily fo us, the dataset has already been tokenized, so we just need to split the input text by whitespace to get the tokens. The tokenization is not perfect though. Your preprocessing function should should fix all instances where \"Mr.\" have been tokenized as two tokens to instances where \"Mr.\" is a single token.\n",
        "\n",
        "Implement the above by changing and possibly extending the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3As4zBmSyZhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287177ff-2854-4f9d-8810-7428325169c7"
      },
      "source": [
        "def collapse_mr_dot(text):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      tokens: a list of tokens\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"mr .\", \"mr.\")\n",
        "    text = text.replace(\"mr  .\", \"mr.\")\n",
        "    result = text.split(\" \")    \n",
        "    return result\n",
        "\n",
        "torch.manual_seed(1)\n",
        "reader = PolarityDatasetReader(tokenize_and_preprocess=collapse_mr_dot)\n",
        "data_pre = reader.read(cached_path(\"rt-polaritydata/rt-polarity.utf8.tsv\"))\n",
        "rt_polarity_pre = data_pre\n",
        "for instance in data_pre:\n",
        "  if \"mr.\" in [t.text for t in instance['sentence']]:\n",
        "    print(instance['sentence'][:])\n",
        "    break"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662it [00:00, 10675.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[as, is, often, the, case, with, ambitious, ,, eager, first-time, filmmakers, ,, mr., murray, ,, a, prolific, director, of, music, videos, ,, stuffs, his, debut, with, more, plot, than, it, can, comfortably, hold, ., \n",
            "]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-JYoGSCDjuj"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Below we provide a simple implementation of a model, that combined with the corresponding loss, amounts to logistic regression.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBogFeKugF0v"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Model\n",
        "class LogisticRegression(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Logistic Regression implementation based on torchtext input format.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.normal(torch.zeros(num_features)), \n",
        "                                requires_grad=True)\n",
        "            \n",
        "    def forward(self, sentence):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sentence: a dictionary of ...\n",
        "        \"\"\"\n",
        "        tokens = sentence['tokens']\n",
        "        active_tokens_mask = get_text_field_mask(sentence)\n",
        "        # retrieve weights and set those to zero that come from padding cells \n",
        "        filtered = active_tokens_mask * self.weights[tokens]\n",
        "        # sum pooling along the token position dimension \n",
        "        logits = filtered.sum(dim=1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# model = LogisticRegression(vocab.get_vocab_size(\"tokens\"))\n",
        "# model.forward(sentence)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKGwAaYEEm0-"
      },
      "source": [
        "## Formulation [5pts]\n",
        "\n",
        "In the class we have presented the model as encoder $f(\\mathbf{x})$ follwed by a linear decoder\n",
        "$$s(\\mathbf{x}) = \\boldsymbol{\\theta}^T  f(\\mathbf{x}) = \\boldsymbol{\\theta}^T \\sum_{w\\in \\mathbf{x}} f(w) $$ where $f(\\mathbf{x})$ is the representation of the input text. \n",
        "\n",
        "The implementation here achieves the same output, but the calculation is performed slightly differently due to technical reasons when working with pytorch. Can you give a mathematical description of this implementation here that captures the order in which computation happens? Below $f(w)$ is a one-hot representation of a word, as per lecture 2. \n",
        "\n",
        "The candidate answers are:\n",
        "\n",
        "1. $s(\\mathbf{x}) = \\left[\\sum_{w\\in \\mathbf{x}}  f(w)  \\right]^T \\boldsymbol{\\theta} $\n",
        "2. $s(\\mathbf{x}) = \\sum_{w\\in \\mathbf{x}}  \\boldsymbol{\\theta}^T f(w)$\n",
        "3. $s(\\mathbf{x}) = \\frac{1}{|\\mathbf{x}|}\\boldsymbol{\\theta}^T \\sum_{\\mathbf{x}\\in x}  f(w)$\n",
        "4. $s(\\mathbf{x}) = \\left[\\sum_{w\\in \\mathbf{x}}  f(w)  \\right]^T \\boldsymbol{\\theta} \\frac{1}{|\\mathbf{x}|} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjKT0eJzvyw8"
      },
      "source": [
        "#@title Answers { run: \"auto\" }\n",
        "QFormulation = \"Eq 1\" #@param [\"Eq 1\", \"Eq 2\", \"Eq 3\", \"Eq 4\", \"None of the above\"]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ybvSIOFiLR"
      },
      "source": [
        "## Mean Pooling [8pts]\n",
        "\n",
        "Create a new version of the logistic regression module, using mean pooling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPYSGdhmFpQH"
      },
      "source": [
        "# Model\n",
        "class LogisticRegressionMeanPooling(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Logistic Regression implementation based on torchtext input format.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        super(LogisticRegressionMeanPooling, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.normal(torch.zeros(num_features)), \n",
        "                                requires_grad=True)\n",
        "            \n",
        "    def forward(self, sentence):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sentence: a dictionary of ...\n",
        "        \"\"\"\n",
        "        tokens = sentence['tokens']\n",
        "        active_tokens_mask = get_text_field_mask(sentence)\n",
        "        # retrieve weights and set those to zero that come from padding cells \n",
        "        filtered = active_tokens_mask * self.weights[tokens]\n",
        "        # sum pooling along the token position dimension \n",
        "        logits = filtered.sum(dim=1)\n",
        "        # Get the number of words for each sentence\n",
        "        n = active_tokens_mask.sum(dim=1)\n",
        "        # Divide by the number of words\n",
        "        mean_logits = logits/n\n",
        "        \n",
        "        return mean_logits\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9QcbnWG6PK"
      },
      "source": [
        "## Add Features [20pts]\n",
        "\n",
        "Add the features below to the preprocessing pipeline shown below. \n",
        "\n",
        "### Bias Feature [6 pts]\n",
        "\n",
        "It is common practice to add a *bias* term of linear classifiers:\n",
        "\n",
        "$$s(\\mathbf{x}) = \\boldsymbol{\\theta}^T  f(\\mathbf{x}) + b $$\n",
        "\n",
        "One way to achieve this in general is to augment $f(\\mathbf{x}) $ with an extra component that is always set to $1$. In our implementation, this can be achieved by augmenting the sentence field appropriately when loading the data, and setting the `add_features` argument in the dataset loader. Implement this below.  \n",
        "\n",
        "### Bigram Feature [7 pts]\n",
        "\n",
        "Use the `add_features` pipeline to implement a feature that captures whether word *pairs* $w_1, w_2$ appear consecutively in the sentence.  This feature should be *combined* with the standard unigram and bias features.   \n",
        "\n",
        "### Max Pooling [7 pts]\n",
        "\n",
        "Use the `add_features` pipeline to implement max pooling such that any feature appearing more than once in the sentence is only counted once, as per the lecture slides of week 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmLgBoZFHms6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4b553f-da23-4628-93bd-5890b7926ba3"
      },
      "source": [
        "def add_features(text):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    features: a list of tokens\n",
        "  \"\"\"\n",
        "  # TODO implement this function based on instructions above. \n",
        "  toks = collapse_mr_dot(text)\n",
        "\n",
        "  # Bias Feature\n",
        "  toks.append('--ciao--')\n",
        "\n",
        "  # Biagram\n",
        "  # I am doing this after the bias as the question is asking for it.\n",
        "  sentence_len = len(toks)\n",
        "  for i in range(sentence_len - 1):\n",
        "    toks.append(toks[i] + ' ' + toks[i + 1])\n",
        "\n",
        "  # Max Pooling\n",
        "  unique_toks = list(dict.fromkeys(toks))\n",
        "  result = list(unique_toks)\n",
        "\n",
        "  return result\n",
        "\n",
        "reader = PolarityDatasetReader(tokenize_and_preprocess=add_features)\n",
        "data_pre_2 = reader.read(cached_path(\"rt-polaritydata/rt-polarity.utf8.tsv\"))\n",
        "data_pre_2[10]['sentence'][:]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662it [00:01, 7681.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[a,\n",
              " sentimental,\n",
              " mess,\n",
              " that,\n",
              " never,\n",
              " rings,\n",
              " true,\n",
              " .,\n",
              " \n",
              ",\n",
              " --ciao--,\n",
              " a sentimental,\n",
              " sentimental mess,\n",
              " mess that,\n",
              " that never,\n",
              " never rings,\n",
              " rings true,\n",
              " true .,\n",
              " . \n",
              ",\n",
              " \n",
              " --ciao--]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy8JnMkZJQEw"
      },
      "source": [
        "## Hyperparameters Search and Analysis [10 pts]\n",
        "\n",
        "### Early Stopping [5pts]\n",
        "\n",
        "Finding the right number of iterations is important (why can running to convergence be bad?). One way to to do this is to iterate for a max number K, and then choose the iteration with the largest dev set performance. But this can be slow and unnecessary if we assume that dev-set performance doesn't go up again once it starts to go down (dev set performance concave). Implement a variant of the training loop that implements this idea.  Specifically, the loop should terminate if there has been no increase in development set accuracy when comparing the current accuracy to that from 10 epochs ago. \n",
        "\n",
        "### Grid Search [5pts]\n",
        "Using all the features you developed in the above \"Add Features\" section (or the base model in case you could not address the question), find the best combination of \n",
        "\n",
        "* Learning Rate in {1.0, 0.1}\n",
        "* Number of Training epochs (via early stopping, use 1000 as maximum)\n",
        "* L2 regularisation weight in {0.001, 0.0001, 0}\n",
        "\n",
        "After grid search, the value of the variables `best_acc`, `best_l2`, `best_lr` and `best_epochs` should be appropriately. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwHMvGhWo6ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed7d4ab-c28e-4762-813f-59034bfd64e5"
      },
      "source": [
        "def accuracy(dataset, model, iterator):\n",
        "  # Testing the model and returning the accuracy on the given dataset\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for batch in iterator(dataset, num_epochs=1):\n",
        "      sentence = batch['sentence']\n",
        "      label = batch['label']\n",
        "      output = model(sentence)\n",
        "      total += len(label)\n",
        "      prediction = (output > 0).long()\n",
        "      correct += (prediction == label).sum()\n",
        "\n",
        "  return float(correct) / total  \n",
        "\n",
        "def training_loop(model, iterator, train_set, dev_set, num_epochs=100,\n",
        "                  lr=0.1, weight_decay=0.0, early_stop_decision = 'simple'):\n",
        "  \"\"\"\n",
        "  Should return the best dev_set accuracy and the number of epochs used. \n",
        "  \"\"\"\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, \n",
        "                              weight_decay=weight_decay)  \n",
        "  # Training the Model\n",
        "  epoch_accuracies = []\n",
        "  best_epoch = 0\n",
        "  best_accuracy = 0.0\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, batch in enumerate(iterator(train_set,num_epochs=1)):\n",
        "          sentence = batch['sentence']\n",
        "          label = batch['label'].float()\n",
        "\n",
        "          # Forward + Backward + Optimize\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(sentence)\n",
        "          loss = criterion(logits, label)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Dev: %.4f' \n",
        "                     % (epoch+1, num_epochs, i+1, len(train_set)//iterator._batch_size, \n",
        "                        loss.data, accuracy(dev_set, model, iterator)))\n",
        "      \n",
        "      epoch_accuracies.append(accuracy(dev_set, model, iterator))\n",
        "      if epoch_accuracies[-1] > best_accuracy:\n",
        "          best_accuracy = epoch_accuracies[-1]\n",
        "          best_epoch = epoch\n",
        "          \n",
        "      # TODO: implement early stopping here \n",
        "      # The exercise require the early stopping to happen if the comparison with the epoch accuracy has not gone up in th elast 10 iterations.\n",
        "      # We can compare this with the 10th last result or we can use a moving average to ensure the smoothness of the function.\n",
        "      # I added an argument \"early_stop_decision\" in training_loop with a default to simple to check for this\n",
        "      # I use best set accuracy as specified here: https://moodle.ucl.ac.uk/mod/forum/discuss.php?d=539287\n",
        "      if early_stop_decision == 'simple':\n",
        "        if len(epoch_accuracies) > 10:\n",
        "          if best_accuracy <= epoch_accuracies[-11]:\n",
        "            print(\"Early Stop\")\n",
        "            break    \n",
        "      elif early_stop_decision == 'complex': \n",
        "        if len(epoch_accuracies) > 10:\n",
        "          if best_accuracy <= sum(epoch_accuracies[-11:-2])/10:\n",
        "            print(\"Early Stop\")\n",
        "            break \n",
        "\n",
        "  return best_accuracy, best_epoch\n",
        "\n",
        "reader = PolarityDatasetReader(tokenize_and_preprocess=add_features)\n",
        "data = reader.read(cached_path(\"rt-polaritydata/rt-polarity.utf8.tsv\"))\n",
        "training_data = data[0:-1000]\n",
        "dev_data = data[-1000:]\n",
        "vocab = Vocabulary.from_instances(training_data)\n",
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "print(len(training_data))\n",
        "print(len(dev_data))\n",
        "print(len(data))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662it [00:01, 8418.81it/s]\n",
            "100%|██████████| 9662/9662 [00:00<00:00, 25092.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9662\n",
            "1000\n",
            "10662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101596npWy9o",
        "outputId": "ff38db4a-f08e-46ff-9900-4c789679d975"
      },
      "source": [
        "best_acc = 0.0 # best accuracy achieved \r\n",
        "best_lr = 0.0 # best learning rate at best accuracy \r\n",
        "best_l2 = 0.0 # best l2 regularizing weight\r\n",
        "best_epochs = 0 # best number of epochs\r\n",
        "# TODO: implement grid search to make sure the above 4 variable have \r\n",
        "for lr in [1.0, 0.1]:\r\n",
        "  for l2 in [0.001, 0.0001, 0]:\r\n",
        "    for early_stop_decision in [\"simple\"]:\r\n",
        "      model = LogisticRegression(num_features=vocab.get_vocab_size(\"tokens\"))\r\n",
        "      acc, epochs = training_loop(model, iterator, training_data, dev_data, lr=lr, weight_decay = l2, early_stop_decision = early_stop_decision)\r\n",
        "    \r\n",
        "    if acc > best_acc:\r\n",
        "      print(epochs)\r\n",
        "      best_acc = acc\r\n",
        "      best_lr = lr\r\n",
        "      best_l2 = l2\r\n",
        "      best_epochs = epochs\r\n",
        "      best_early_stop_decision = early_stop_decision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/100], Step: [100/301], Loss: 1.4559, Dev: 0.0520\n",
            "Epoch: [1/100], Step: [200/301], Loss: 1.1467, Dev: 0.1230\n",
            "Epoch: [1/100], Step: [300/301], Loss: 1.4785, Dev: 0.2320\n",
            "Epoch: [2/100], Step: [100/301], Loss: 0.7089, Dev: 0.1500\n",
            "Epoch: [2/100], Step: [200/301], Loss: 1.6162, Dev: 0.1940\n",
            "Epoch: [2/100], Step: [300/301], Loss: 1.5791, Dev: 0.1800\n",
            "Epoch: [3/100], Step: [100/301], Loss: 0.3642, Dev: 0.1820\n",
            "Epoch: [3/100], Step: [200/301], Loss: 0.4366, Dev: 0.1570\n",
            "Epoch: [3/100], Step: [300/301], Loss: 0.4403, Dev: 0.2490\n",
            "Epoch: [4/100], Step: [100/301], Loss: 0.5128, Dev: 0.2030\n",
            "Epoch: [4/100], Step: [200/301], Loss: 0.2862, Dev: 0.2890\n",
            "Epoch: [4/100], Step: [300/301], Loss: 0.4523, Dev: 0.1280\n",
            "Epoch: [5/100], Step: [100/301], Loss: 0.3859, Dev: 0.1980\n",
            "Epoch: [5/100], Step: [200/301], Loss: 0.2553, Dev: 0.3160\n",
            "Epoch: [5/100], Step: [300/301], Loss: 0.3606, Dev: 0.0990\n",
            "Epoch: [6/100], Step: [100/301], Loss: 0.2964, Dev: 0.3880\n",
            "Epoch: [6/100], Step: [200/301], Loss: 0.2903, Dev: 0.2960\n",
            "Epoch: [6/100], Step: [300/301], Loss: 0.1996, Dev: 0.3820\n",
            "Epoch: [7/100], Step: [100/301], Loss: 0.2575, Dev: 0.2130\n",
            "Epoch: [7/100], Step: [200/301], Loss: 0.5000, Dev: 0.4030\n",
            "Epoch: [7/100], Step: [300/301], Loss: 0.2980, Dev: 0.3260\n",
            "Epoch: [8/100], Step: [100/301], Loss: 0.3516, Dev: 0.2920\n",
            "Epoch: [8/100], Step: [200/301], Loss: 0.4383, Dev: 0.3030\n",
            "Epoch: [8/100], Step: [300/301], Loss: 0.4936, Dev: 0.3660\n",
            "Epoch: [9/100], Step: [100/301], Loss: 0.2985, Dev: 0.3900\n",
            "Epoch: [9/100], Step: [200/301], Loss: 0.4400, Dev: 0.4810\n",
            "Epoch: [9/100], Step: [300/301], Loss: 0.2607, Dev: 0.4560\n",
            "Epoch: [10/100], Step: [100/301], Loss: 0.3103, Dev: 0.4450\n",
            "Epoch: [10/100], Step: [200/301], Loss: 0.3205, Dev: 0.3060\n",
            "Epoch: [10/100], Step: [300/301], Loss: 0.3208, Dev: 0.4750\n",
            "Epoch: [11/100], Step: [100/301], Loss: 0.2134, Dev: 0.6550\n",
            "Epoch: [11/100], Step: [200/301], Loss: 0.3331, Dev: 0.4310\n",
            "Epoch: [11/100], Step: [300/301], Loss: 0.4035, Dev: 0.6230\n",
            "Epoch: [12/100], Step: [100/301], Loss: 0.2734, Dev: 0.3830\n",
            "Epoch: [12/100], Step: [200/301], Loss: 0.2755, Dev: 0.5710\n",
            "Epoch: [12/100], Step: [300/301], Loss: 0.2858, Dev: 0.6990\n",
            "Epoch: [13/100], Step: [100/301], Loss: 0.2632, Dev: 0.6680\n",
            "Epoch: [13/100], Step: [200/301], Loss: 0.2713, Dev: 0.5080\n",
            "Epoch: [13/100], Step: [300/301], Loss: 0.3483, Dev: 0.5090\n",
            "Epoch: [14/100], Step: [100/301], Loss: 0.3990, Dev: 0.4050\n",
            "Epoch: [14/100], Step: [200/301], Loss: 0.2287, Dev: 0.6710\n",
            "Epoch: [14/100], Step: [300/301], Loss: 0.4430, Dev: 0.7220\n",
            "Epoch: [15/100], Step: [100/301], Loss: 0.2522, Dev: 0.5620\n",
            "Epoch: [15/100], Step: [200/301], Loss: 0.2944, Dev: 0.6850\n",
            "Epoch: [15/100], Step: [300/301], Loss: 0.3541, Dev: 0.6700\n",
            "Epoch: [16/100], Step: [100/301], Loss: 0.2353, Dev: 0.5120\n",
            "Epoch: [16/100], Step: [200/301], Loss: 0.3435, Dev: 0.7440\n",
            "Epoch: [16/100], Step: [300/301], Loss: 0.3038, Dev: 0.4770\n",
            "Epoch: [17/100], Step: [100/301], Loss: 0.5110, Dev: 0.5910\n",
            "Epoch: [17/100], Step: [200/301], Loss: 0.3071, Dev: 0.7940\n",
            "Epoch: [17/100], Step: [300/301], Loss: 0.2930, Dev: 0.5570\n",
            "Epoch: [18/100], Step: [100/301], Loss: 0.4586, Dev: 0.8030\n",
            "Epoch: [18/100], Step: [200/301], Loss: 0.6322, Dev: 0.8430\n",
            "Epoch: [18/100], Step: [300/301], Loss: 0.2360, Dev: 0.4280\n",
            "Epoch: [19/100], Step: [100/301], Loss: 0.2561, Dev: 0.6150\n",
            "Epoch: [19/100], Step: [200/301], Loss: 0.3284, Dev: 0.5410\n",
            "Epoch: [19/100], Step: [300/301], Loss: 0.4122, Dev: 0.6620\n",
            "Epoch: [20/100], Step: [100/301], Loss: 0.3822, Dev: 0.6850\n",
            "Epoch: [20/100], Step: [200/301], Loss: 0.3494, Dev: 0.4880\n",
            "Epoch: [20/100], Step: [300/301], Loss: 0.3429, Dev: 0.7360\n",
            "Epoch: [21/100], Step: [100/301], Loss: 0.4701, Dev: 0.6510\n",
            "Epoch: [21/100], Step: [200/301], Loss: 0.3758, Dev: 0.7510\n",
            "Epoch: [21/100], Step: [300/301], Loss: 0.2770, Dev: 0.7450\n",
            "Epoch: [22/100], Step: [100/301], Loss: 0.2697, Dev: 0.6870\n",
            "Epoch: [22/100], Step: [200/301], Loss: 0.2595, Dev: 0.7130\n",
            "Epoch: [22/100], Step: [300/301], Loss: 0.2757, Dev: 0.4960\n",
            "Epoch: [23/100], Step: [100/301], Loss: 0.3821, Dev: 0.5650\n",
            "Epoch: [23/100], Step: [200/301], Loss: 0.2945, Dev: 0.6560\n",
            "Epoch: [23/100], Step: [300/301], Loss: 0.3284, Dev: 0.6440\n",
            "Early Stop\n",
            "12\n",
            "Epoch: [1/100], Step: [100/301], Loss: 1.6424, Dev: 0.0240\n",
            "Epoch: [1/100], Step: [200/301], Loss: 3.6091, Dev: 0.0180\n",
            "Epoch: [1/100], Step: [300/301], Loss: 2.1036, Dev: 0.0130\n",
            "Epoch: [2/100], Step: [100/301], Loss: 1.8285, Dev: 0.0260\n",
            "Epoch: [2/100], Step: [200/301], Loss: 1.7917, Dev: 0.0470\n",
            "Epoch: [2/100], Step: [300/301], Loss: 0.7627, Dev: 0.0180\n",
            "Epoch: [3/100], Step: [100/301], Loss: 0.6130, Dev: 0.0250\n",
            "Epoch: [3/100], Step: [200/301], Loss: 0.8282, Dev: 0.0340\n",
            "Epoch: [3/100], Step: [300/301], Loss: 0.8517, Dev: 0.0340\n",
            "Epoch: [4/100], Step: [100/301], Loss: 1.3121, Dev: 0.0300\n",
            "Epoch: [4/100], Step: [200/301], Loss: 0.8706, Dev: 0.0290\n",
            "Epoch: [4/100], Step: [300/301], Loss: 0.4668, Dev: 0.0260\n",
            "Epoch: [5/100], Step: [100/301], Loss: 0.2821, Dev: 0.0270\n",
            "Epoch: [5/100], Step: [200/301], Loss: 0.4368, Dev: 0.0310\n",
            "Epoch: [5/100], Step: [300/301], Loss: 0.4570, Dev: 0.0310\n",
            "Epoch: [6/100], Step: [100/301], Loss: 0.6758, Dev: 0.0310\n",
            "Epoch: [6/100], Step: [200/301], Loss: 0.7175, Dev: 0.0510\n",
            "Epoch: [6/100], Step: [300/301], Loss: 0.5930, Dev: 0.0370\n",
            "Epoch: [7/100], Step: [100/301], Loss: 0.4121, Dev: 0.0400\n",
            "Epoch: [7/100], Step: [200/301], Loss: 0.1992, Dev: 0.0370\n",
            "Epoch: [7/100], Step: [300/301], Loss: 0.4256, Dev: 0.0280\n",
            "Epoch: [8/100], Step: [100/301], Loss: 0.4322, Dev: 0.0290\n",
            "Epoch: [8/100], Step: [200/301], Loss: 0.2670, Dev: 0.0340\n",
            "Epoch: [8/100], Step: [300/301], Loss: 0.5311, Dev: 0.0510\n",
            "Epoch: [9/100], Step: [100/301], Loss: 0.2981, Dev: 0.0430\n",
            "Epoch: [9/100], Step: [200/301], Loss: 0.1693, Dev: 0.0400\n",
            "Epoch: [9/100], Step: [300/301], Loss: 0.1138, Dev: 0.0310\n",
            "Epoch: [10/100], Step: [100/301], Loss: 0.3287, Dev: 0.0370\n",
            "Epoch: [10/100], Step: [200/301], Loss: 0.1274, Dev: 0.0310\n",
            "Epoch: [10/100], Step: [300/301], Loss: 0.2726, Dev: 0.0360\n",
            "Epoch: [11/100], Step: [100/301], Loss: 0.1348, Dev: 0.0420\n",
            "Epoch: [11/100], Step: [200/301], Loss: 0.3804, Dev: 0.0450\n",
            "Epoch: [11/100], Step: [300/301], Loss: 0.0845, Dev: 0.0370\n",
            "Epoch: [12/100], Step: [100/301], Loss: 0.3895, Dev: 0.0260\n",
            "Epoch: [12/100], Step: [200/301], Loss: 0.1271, Dev: 0.0310\n",
            "Epoch: [12/100], Step: [300/301], Loss: 0.3747, Dev: 0.0570\n",
            "Epoch: [13/100], Step: [100/301], Loss: 0.0984, Dev: 0.0490\n",
            "Epoch: [13/100], Step: [200/301], Loss: 0.1866, Dev: 0.0460\n",
            "Epoch: [13/100], Step: [300/301], Loss: 0.0737, Dev: 0.0510\n",
            "Epoch: [14/100], Step: [100/301], Loss: 0.0650, Dev: 0.0460\n",
            "Epoch: [14/100], Step: [200/301], Loss: 0.0480, Dev: 0.0410\n",
            "Epoch: [14/100], Step: [300/301], Loss: 0.2039, Dev: 0.0460\n",
            "Epoch: [15/100], Step: [100/301], Loss: 0.0349, Dev: 0.0440\n",
            "Epoch: [15/100], Step: [200/301], Loss: 0.0728, Dev: 0.0480\n",
            "Epoch: [15/100], Step: [300/301], Loss: 0.1325, Dev: 0.0430\n",
            "Epoch: [16/100], Step: [100/301], Loss: 0.2624, Dev: 0.0400\n",
            "Epoch: [16/100], Step: [200/301], Loss: 0.0662, Dev: 0.0400\n",
            "Epoch: [16/100], Step: [300/301], Loss: 0.0812, Dev: 0.0420\n",
            "Epoch: [17/100], Step: [100/301], Loss: 0.1889, Dev: 0.0490\n",
            "Epoch: [17/100], Step: [200/301], Loss: 0.2693, Dev: 0.0510\n",
            "Epoch: [17/100], Step: [300/301], Loss: 0.1185, Dev: 0.0610\n",
            "Epoch: [18/100], Step: [100/301], Loss: 0.0757, Dev: 0.0480\n",
            "Epoch: [18/100], Step: [200/301], Loss: 0.0723, Dev: 0.0470\n",
            "Epoch: [18/100], Step: [300/301], Loss: 0.2063, Dev: 0.0480\n",
            "Epoch: [19/100], Step: [100/301], Loss: 0.0693, Dev: 0.0570\n",
            "Epoch: [19/100], Step: [200/301], Loss: 0.2067, Dev: 0.0530\n",
            "Epoch: [19/100], Step: [300/301], Loss: 0.0703, Dev: 0.0480\n",
            "Epoch: [20/100], Step: [100/301], Loss: 0.0574, Dev: 0.0580\n",
            "Epoch: [20/100], Step: [200/301], Loss: 0.1129, Dev: 0.0510\n",
            "Epoch: [20/100], Step: [300/301], Loss: 0.1329, Dev: 0.0530\n",
            "Epoch: [21/100], Step: [100/301], Loss: 0.0539, Dev: 0.0490\n",
            "Epoch: [21/100], Step: [200/301], Loss: 0.3870, Dev: 0.0590\n",
            "Epoch: [21/100], Step: [300/301], Loss: 0.0681, Dev: 0.0530\n",
            "Epoch: [22/100], Step: [100/301], Loss: 0.0604, Dev: 0.0570\n",
            "Epoch: [22/100], Step: [200/301], Loss: 0.0616, Dev: 0.0520\n",
            "Epoch: [22/100], Step: [300/301], Loss: 0.0395, Dev: 0.0520\n",
            "Epoch: [23/100], Step: [100/301], Loss: 0.0392, Dev: 0.0510\n",
            "Epoch: [23/100], Step: [200/301], Loss: 0.0512, Dev: 0.0620\n",
            "Epoch: [23/100], Step: [300/301], Loss: 0.0765, Dev: 0.0540\n",
            "Epoch: [24/100], Step: [100/301], Loss: 0.0731, Dev: 0.0610\n",
            "Epoch: [24/100], Step: [200/301], Loss: 0.0716, Dev: 0.0650\n",
            "Epoch: [24/100], Step: [300/301], Loss: 0.0785, Dev: 0.0620\n",
            "Epoch: [25/100], Step: [100/301], Loss: 0.0987, Dev: 0.0620\n",
            "Epoch: [25/100], Step: [200/301], Loss: 0.0843, Dev: 0.0580\n",
            "Epoch: [25/100], Step: [300/301], Loss: 0.0738, Dev: 0.0600\n",
            "Epoch: [26/100], Step: [100/301], Loss: 0.0733, Dev: 0.0570\n",
            "Epoch: [26/100], Step: [200/301], Loss: 0.1747, Dev: 0.0640\n",
            "Epoch: [26/100], Step: [300/301], Loss: 0.0653, Dev: 0.0650\n",
            "Epoch: [27/100], Step: [100/301], Loss: 0.0777, Dev: 0.0760\n",
            "Epoch: [27/100], Step: [200/301], Loss: 0.0645, Dev: 0.0630\n",
            "Epoch: [27/100], Step: [300/301], Loss: 0.2499, Dev: 0.0730\n",
            "Epoch: [28/100], Step: [100/301], Loss: 0.0592, Dev: 0.0780\n",
            "Epoch: [28/100], Step: [200/301], Loss: 0.1332, Dev: 0.0800\n",
            "Epoch: [28/100], Step: [300/301], Loss: 0.0761, Dev: 0.0600\n",
            "Epoch: [29/100], Step: [100/301], Loss: 0.1114, Dev: 0.0610\n",
            "Epoch: [29/100], Step: [200/301], Loss: 0.0349, Dev: 0.0790\n",
            "Epoch: [29/100], Step: [300/301], Loss: 0.0667, Dev: 0.0830\n",
            "Epoch: [30/100], Step: [100/301], Loss: 0.0468, Dev: 0.0940\n",
            "Epoch: [30/100], Step: [200/301], Loss: 0.0821, Dev: 0.0830\n",
            "Epoch: [30/100], Step: [300/301], Loss: 0.0835, Dev: 0.0810\n",
            "Epoch: [31/100], Step: [100/301], Loss: 0.0900, Dev: 0.0790\n",
            "Epoch: [31/100], Step: [200/301], Loss: 0.0395, Dev: 0.0810\n",
            "Epoch: [31/100], Step: [300/301], Loss: 0.0774, Dev: 0.0770\n",
            "Epoch: [32/100], Step: [100/301], Loss: 0.0556, Dev: 0.0760\n",
            "Epoch: [32/100], Step: [200/301], Loss: 0.0911, Dev: 0.0790\n",
            "Epoch: [32/100], Step: [300/301], Loss: 0.0502, Dev: 0.0840\n",
            "Epoch: [33/100], Step: [100/301], Loss: 0.0705, Dev: 0.0830\n",
            "Epoch: [33/100], Step: [200/301], Loss: 0.0682, Dev: 0.0870\n",
            "Epoch: [33/100], Step: [300/301], Loss: 0.0676, Dev: 0.0870\n",
            "Epoch: [34/100], Step: [100/301], Loss: 0.0513, Dev: 0.0850\n",
            "Epoch: [34/100], Step: [200/301], Loss: 0.1277, Dev: 0.0840\n",
            "Epoch: [34/100], Step: [300/301], Loss: 0.1319, Dev: 0.0830\n",
            "Epoch: [35/100], Step: [100/301], Loss: 0.1483, Dev: 0.1020\n",
            "Epoch: [35/100], Step: [200/301], Loss: 0.0559, Dev: 0.0840\n",
            "Epoch: [35/100], Step: [300/301], Loss: 0.1952, Dev: 0.1020\n",
            "Epoch: [36/100], Step: [100/301], Loss: 0.0793, Dev: 0.0930\n",
            "Epoch: [36/100], Step: [200/301], Loss: 0.0573, Dev: 0.1020\n",
            "Epoch: [36/100], Step: [300/301], Loss: 0.1023, Dev: 0.0930\n",
            "Epoch: [37/100], Step: [100/301], Loss: 0.2511, Dev: 0.0900\n",
            "Epoch: [37/100], Step: [200/301], Loss: 0.1095, Dev: 0.1120\n",
            "Epoch: [37/100], Step: [300/301], Loss: 0.0448, Dev: 0.1090\n",
            "Epoch: [38/100], Step: [100/301], Loss: 0.0644, Dev: 0.1070\n",
            "Epoch: [38/100], Step: [200/301], Loss: 0.0600, Dev: 0.0920\n",
            "Epoch: [38/100], Step: [300/301], Loss: 0.0837, Dev: 0.1060\n",
            "Epoch: [39/100], Step: [100/301], Loss: 0.0549, Dev: 0.1180\n",
            "Epoch: [39/100], Step: [200/301], Loss: 0.0837, Dev: 0.1080\n",
            "Epoch: [39/100], Step: [300/301], Loss: 0.0531, Dev: 0.1110\n",
            "Epoch: [40/100], Step: [100/301], Loss: 0.0611, Dev: 0.1010\n",
            "Epoch: [40/100], Step: [200/301], Loss: 0.0821, Dev: 0.1340\n",
            "Epoch: [40/100], Step: [300/301], Loss: 0.1190, Dev: 0.1140\n",
            "Epoch: [41/100], Step: [100/301], Loss: 0.0791, Dev: 0.1230\n",
            "Epoch: [41/100], Step: [200/301], Loss: 0.0340, Dev: 0.1090\n",
            "Epoch: [41/100], Step: [300/301], Loss: 0.0588, Dev: 0.1300\n",
            "Epoch: [42/100], Step: [100/301], Loss: 0.0643, Dev: 0.1210\n",
            "Epoch: [42/100], Step: [200/301], Loss: 0.0741, Dev: 0.1310\n",
            "Epoch: [42/100], Step: [300/301], Loss: 0.0736, Dev: 0.1030\n",
            "Epoch: [43/100], Step: [100/301], Loss: 0.0560, Dev: 0.1240\n",
            "Epoch: [43/100], Step: [200/301], Loss: 0.0696, Dev: 0.1220\n",
            "Epoch: [43/100], Step: [300/301], Loss: 0.2060, Dev: 0.1430\n",
            "Epoch: [44/100], Step: [100/301], Loss: 0.0731, Dev: 0.1340\n",
            "Epoch: [44/100], Step: [200/301], Loss: 0.0841, Dev: 0.1450\n",
            "Epoch: [44/100], Step: [300/301], Loss: 0.0825, Dev: 0.1230\n",
            "Epoch: [45/100], Step: [100/301], Loss: 0.0403, Dev: 0.1320\n",
            "Epoch: [45/100], Step: [200/301], Loss: 0.0656, Dev: 0.1290\n",
            "Epoch: [45/100], Step: [300/301], Loss: 0.0571, Dev: 0.1510\n",
            "Epoch: [46/100], Step: [100/301], Loss: 0.0755, Dev: 0.1510\n",
            "Epoch: [46/100], Step: [200/301], Loss: 0.1184, Dev: 0.1230\n",
            "Epoch: [46/100], Step: [300/301], Loss: 0.0774, Dev: 0.1460\n",
            "Epoch: [47/100], Step: [100/301], Loss: 0.1422, Dev: 0.1660\n",
            "Epoch: [47/100], Step: [200/301], Loss: 0.0492, Dev: 0.1490\n",
            "Epoch: [47/100], Step: [300/301], Loss: 0.0700, Dev: 0.1650\n",
            "Epoch: [48/100], Step: [100/301], Loss: 0.0402, Dev: 0.1550\n",
            "Epoch: [48/100], Step: [200/301], Loss: 0.0385, Dev: 0.1460\n",
            "Epoch: [48/100], Step: [300/301], Loss: 0.0675, Dev: 0.1760\n",
            "Epoch: [49/100], Step: [100/301], Loss: 0.0603, Dev: 0.1610\n",
            "Epoch: [49/100], Step: [200/301], Loss: 0.2733, Dev: 0.1550\n",
            "Epoch: [49/100], Step: [300/301], Loss: 0.0606, Dev: 0.1700\n",
            "Epoch: [50/100], Step: [100/301], Loss: 0.1006, Dev: 0.1840\n",
            "Epoch: [50/100], Step: [200/301], Loss: 0.0788, Dev: 0.1590\n",
            "Epoch: [50/100], Step: [300/301], Loss: 0.0858, Dev: 0.1560\n",
            "Epoch: [51/100], Step: [100/301], Loss: 0.1337, Dev: 0.1690\n",
            "Epoch: [51/100], Step: [200/301], Loss: 0.0721, Dev: 0.1770\n",
            "Epoch: [51/100], Step: [300/301], Loss: 0.0730, Dev: 0.1710\n",
            "Epoch: [52/100], Step: [100/301], Loss: 0.0492, Dev: 0.1690\n",
            "Epoch: [52/100], Step: [200/301], Loss: 0.0909, Dev: 0.1860\n",
            "Epoch: [52/100], Step: [300/301], Loss: 0.1153, Dev: 0.2190\n",
            "Epoch: [53/100], Step: [100/301], Loss: 0.0876, Dev: 0.1840\n",
            "Epoch: [53/100], Step: [200/301], Loss: 0.0538, Dev: 0.1910\n",
            "Epoch: [53/100], Step: [300/301], Loss: 0.1873, Dev: 0.1980\n",
            "Epoch: [54/100], Step: [100/301], Loss: 0.0512, Dev: 0.1840\n",
            "Epoch: [54/100], Step: [200/301], Loss: 0.0368, Dev: 0.2090\n",
            "Epoch: [54/100], Step: [300/301], Loss: 0.0428, Dev: 0.1920\n",
            "Epoch: [55/100], Step: [100/301], Loss: 0.0439, Dev: 0.1720\n",
            "Epoch: [55/100], Step: [200/301], Loss: 0.0677, Dev: 0.1990\n",
            "Epoch: [55/100], Step: [300/301], Loss: 0.0884, Dev: 0.1740\n",
            "Epoch: [56/100], Step: [100/301], Loss: 0.2795, Dev: 0.2150\n",
            "Epoch: [56/100], Step: [200/301], Loss: 0.0597, Dev: 0.1930\n",
            "Epoch: [56/100], Step: [300/301], Loss: 0.0561, Dev: 0.1990\n",
            "Epoch: [57/100], Step: [100/301], Loss: 0.0858, Dev: 0.1870\n",
            "Epoch: [57/100], Step: [200/301], Loss: 0.0548, Dev: 0.1870\n",
            "Epoch: [57/100], Step: [300/301], Loss: 0.0656, Dev: 0.2040\n",
            "Epoch: [58/100], Step: [100/301], Loss: 0.0453, Dev: 0.2010\n",
            "Epoch: [58/100], Step: [200/301], Loss: 0.2152, Dev: 0.2250\n",
            "Epoch: [58/100], Step: [300/301], Loss: 0.0697, Dev: 0.1850\n",
            "Epoch: [59/100], Step: [100/301], Loss: 0.0641, Dev: 0.2150\n",
            "Epoch: [59/100], Step: [200/301], Loss: 0.0632, Dev: 0.2190\n",
            "Epoch: [59/100], Step: [300/301], Loss: 0.0928, Dev: 0.2030\n",
            "Epoch: [60/100], Step: [100/301], Loss: 0.0476, Dev: 0.2290\n",
            "Epoch: [60/100], Step: [200/301], Loss: 0.0554, Dev: 0.2520\n",
            "Epoch: [60/100], Step: [300/301], Loss: 0.0606, Dev: 0.2210\n",
            "Epoch: [61/100], Step: [100/301], Loss: 0.0808, Dev: 0.2140\n",
            "Epoch: [61/100], Step: [200/301], Loss: 0.1382, Dev: 0.2390\n",
            "Epoch: [61/100], Step: [300/301], Loss: 0.0496, Dev: 0.2310\n",
            "Epoch: [62/100], Step: [100/301], Loss: 0.0803, Dev: 0.2600\n",
            "Epoch: [62/100], Step: [200/301], Loss: 0.0614, Dev: 0.2690\n",
            "Epoch: [62/100], Step: [300/301], Loss: 0.0732, Dev: 0.2400\n",
            "Epoch: [63/100], Step: [100/301], Loss: 0.0753, Dev: 0.2360\n",
            "Epoch: [63/100], Step: [200/301], Loss: 0.0302, Dev: 0.2630\n",
            "Epoch: [63/100], Step: [300/301], Loss: 0.1051, Dev: 0.2480\n",
            "Epoch: [64/100], Step: [100/301], Loss: 0.0670, Dev: 0.2590\n",
            "Epoch: [64/100], Step: [200/301], Loss: 0.0647, Dev: 0.3050\n",
            "Epoch: [64/100], Step: [300/301], Loss: 0.1384, Dev: 0.2520\n",
            "Epoch: [65/100], Step: [100/301], Loss: 0.0673, Dev: 0.2500\n",
            "Epoch: [65/100], Step: [200/301], Loss: 0.1640, Dev: 0.2850\n",
            "Epoch: [65/100], Step: [300/301], Loss: 0.0766, Dev: 0.2860\n",
            "Epoch: [66/100], Step: [100/301], Loss: 0.1398, Dev: 0.2760\n",
            "Epoch: [66/100], Step: [200/301], Loss: 0.0893, Dev: 0.2510\n",
            "Epoch: [66/100], Step: [300/301], Loss: 0.0753, Dev: 0.2880\n",
            "Epoch: [67/100], Step: [100/301], Loss: 0.0661, Dev: 0.2830\n",
            "Epoch: [67/100], Step: [200/301], Loss: 0.0550, Dev: 0.2820\n",
            "Epoch: [67/100], Step: [300/301], Loss: 0.0442, Dev: 0.3020\n",
            "Epoch: [68/100], Step: [100/301], Loss: 0.0463, Dev: 0.3100\n",
            "Epoch: [68/100], Step: [200/301], Loss: 0.0860, Dev: 0.2880\n",
            "Epoch: [68/100], Step: [300/301], Loss: 0.0469, Dev: 0.2960\n",
            "Epoch: [69/100], Step: [100/301], Loss: 0.0459, Dev: 0.3230\n",
            "Epoch: [69/100], Step: [200/301], Loss: 0.0766, Dev: 0.2990\n",
            "Epoch: [69/100], Step: [300/301], Loss: 0.0518, Dev: 0.2950\n",
            "Epoch: [70/100], Step: [100/301], Loss: 0.1350, Dev: 0.2850\n",
            "Epoch: [70/100], Step: [200/301], Loss: 0.0575, Dev: 0.2930\n",
            "Epoch: [70/100], Step: [300/301], Loss: 0.0778, Dev: 0.2920\n",
            "Epoch: [71/100], Step: [100/301], Loss: 0.0654, Dev: 0.2980\n",
            "Epoch: [71/100], Step: [200/301], Loss: 0.0781, Dev: 0.3070\n",
            "Epoch: [71/100], Step: [300/301], Loss: 0.0654, Dev: 0.3020\n",
            "Epoch: [72/100], Step: [100/301], Loss: 0.0440, Dev: 0.3020\n",
            "Epoch: [72/100], Step: [200/301], Loss: 0.0507, Dev: 0.3560\n",
            "Epoch: [72/100], Step: [300/301], Loss: 0.1018, Dev: 0.3060\n",
            "Epoch: [73/100], Step: [100/301], Loss: 0.0699, Dev: 0.3420\n",
            "Epoch: [73/100], Step: [200/301], Loss: 0.0565, Dev: 0.3140\n",
            "Epoch: [73/100], Step: [300/301], Loss: 0.0791, Dev: 0.3680\n",
            "Epoch: [74/100], Step: [100/301], Loss: 0.0670, Dev: 0.3030\n",
            "Epoch: [74/100], Step: [200/301], Loss: 0.0672, Dev: 0.3250\n",
            "Epoch: [74/100], Step: [300/301], Loss: 0.1561, Dev: 0.3190\n",
            "Epoch: [75/100], Step: [100/301], Loss: 0.1545, Dev: 0.3310\n",
            "Epoch: [75/100], Step: [200/301], Loss: 0.0793, Dev: 0.3850\n",
            "Epoch: [75/100], Step: [300/301], Loss: 0.0481, Dev: 0.3050\n",
            "Epoch: [76/100], Step: [100/301], Loss: 0.1892, Dev: 0.3850\n",
            "Epoch: [76/100], Step: [200/301], Loss: 0.0723, Dev: 0.3460\n",
            "Epoch: [76/100], Step: [300/301], Loss: 0.0764, Dev: 0.3360\n",
            "Epoch: [77/100], Step: [100/301], Loss: 0.1294, Dev: 0.3330\n",
            "Epoch: [77/100], Step: [200/301], Loss: 0.0667, Dev: 0.3360\n",
            "Epoch: [77/100], Step: [300/301], Loss: 0.1068, Dev: 0.3960\n",
            "Epoch: [78/100], Step: [100/301], Loss: 0.0713, Dev: 0.3580\n",
            "Epoch: [78/100], Step: [200/301], Loss: 0.0516, Dev: 0.3490\n",
            "Epoch: [78/100], Step: [300/301], Loss: 0.0570, Dev: 0.3430\n",
            "Epoch: [79/100], Step: [100/301], Loss: 0.0758, Dev: 0.3540\n",
            "Epoch: [79/100], Step: [200/301], Loss: 0.0912, Dev: 0.3580\n",
            "Epoch: [79/100], Step: [300/301], Loss: 0.0579, Dev: 0.3630\n",
            "Epoch: [80/100], Step: [100/301], Loss: 0.1367, Dev: 0.3960\n",
            "Epoch: [80/100], Step: [200/301], Loss: 0.0977, Dev: 0.4130\n",
            "Epoch: [80/100], Step: [300/301], Loss: 0.1235, Dev: 0.4090\n",
            "Epoch: [81/100], Step: [100/301], Loss: 0.0663, Dev: 0.4040\n",
            "Epoch: [81/100], Step: [200/301], Loss: 0.1099, Dev: 0.3650\n",
            "Epoch: [81/100], Step: [300/301], Loss: 0.0835, Dev: 0.3920\n",
            "Epoch: [82/100], Step: [100/301], Loss: 0.0942, Dev: 0.4070\n",
            "Epoch: [82/100], Step: [200/301], Loss: 0.1277, Dev: 0.4490\n",
            "Epoch: [82/100], Step: [300/301], Loss: 0.2139, Dev: 0.4300\n",
            "Epoch: [83/100], Step: [100/301], Loss: 0.1264, Dev: 0.4130\n",
            "Epoch: [83/100], Step: [200/301], Loss: 0.0734, Dev: 0.4030\n",
            "Epoch: [83/100], Step: [300/301], Loss: 0.0458, Dev: 0.4040\n",
            "Epoch: [84/100], Step: [100/301], Loss: 0.0598, Dev: 0.4290\n",
            "Epoch: [84/100], Step: [200/301], Loss: 0.0764, Dev: 0.3830\n",
            "Epoch: [84/100], Step: [300/301], Loss: 0.0686, Dev: 0.3930\n",
            "Epoch: [85/100], Step: [100/301], Loss: 0.0565, Dev: 0.3960\n",
            "Epoch: [85/100], Step: [200/301], Loss: 0.0565, Dev: 0.4390\n",
            "Epoch: [85/100], Step: [300/301], Loss: 0.0534, Dev: 0.4500\n",
            "Epoch: [86/100], Step: [100/301], Loss: 0.0748, Dev: 0.4280\n",
            "Epoch: [86/100], Step: [200/301], Loss: 0.0761, Dev: 0.4510\n",
            "Epoch: [86/100], Step: [300/301], Loss: 0.0479, Dev: 0.4520\n",
            "Epoch: [87/100], Step: [100/301], Loss: 0.0602, Dev: 0.4300\n",
            "Epoch: [87/100], Step: [200/301], Loss: 0.0699, Dev: 0.4510\n",
            "Epoch: [87/100], Step: [300/301], Loss: 0.1013, Dev: 0.4180\n",
            "Epoch: [88/100], Step: [100/301], Loss: 0.1072, Dev: 0.4280\n",
            "Epoch: [88/100], Step: [200/301], Loss: 0.0688, Dev: 0.4380\n",
            "Epoch: [88/100], Step: [300/301], Loss: 0.1377, Dev: 0.4500\n",
            "Epoch: [89/100], Step: [100/301], Loss: 0.1037, Dev: 0.4720\n",
            "Epoch: [89/100], Step: [200/301], Loss: 0.0509, Dev: 0.4380\n",
            "Epoch: [89/100], Step: [300/301], Loss: 0.1054, Dev: 0.4220\n",
            "Epoch: [90/100], Step: [100/301], Loss: 0.0917, Dev: 0.4330\n",
            "Epoch: [90/100], Step: [200/301], Loss: 0.1154, Dev: 0.4450\n",
            "Epoch: [90/100], Step: [300/301], Loss: 0.0753, Dev: 0.4590\n",
            "Epoch: [91/100], Step: [100/301], Loss: 0.0600, Dev: 0.4210\n",
            "Epoch: [91/100], Step: [200/301], Loss: 0.0607, Dev: 0.4120\n",
            "Epoch: [91/100], Step: [300/301], Loss: 0.0555, Dev: 0.4590\n",
            "Epoch: [92/100], Step: [100/301], Loss: 0.0667, Dev: 0.4850\n",
            "Epoch: [92/100], Step: [200/301], Loss: 0.0838, Dev: 0.4510\n",
            "Epoch: [92/100], Step: [300/301], Loss: 0.1253, Dev: 0.4620\n",
            "Epoch: [93/100], Step: [100/301], Loss: 0.0438, Dev: 0.4550\n",
            "Epoch: [93/100], Step: [200/301], Loss: 0.1016, Dev: 0.4610\n",
            "Epoch: [93/100], Step: [300/301], Loss: 0.0862, Dev: 0.4890\n",
            "Epoch: [94/100], Step: [100/301], Loss: 0.0765, Dev: 0.4880\n",
            "Epoch: [94/100], Step: [200/301], Loss: 0.0574, Dev: 0.4620\n",
            "Epoch: [94/100], Step: [300/301], Loss: 0.1118, Dev: 0.4820\n",
            "Epoch: [95/100], Step: [100/301], Loss: 0.0853, Dev: 0.4510\n",
            "Epoch: [95/100], Step: [200/301], Loss: 0.1068, Dev: 0.5200\n",
            "Epoch: [95/100], Step: [300/301], Loss: 0.0739, Dev: 0.4560\n",
            "Epoch: [96/100], Step: [100/301], Loss: 0.0811, Dev: 0.4870\n",
            "Epoch: [96/100], Step: [200/301], Loss: 0.1146, Dev: 0.4810\n",
            "Epoch: [96/100], Step: [300/301], Loss: 0.0659, Dev: 0.5340\n",
            "Epoch: [97/100], Step: [100/301], Loss: 0.0404, Dev: 0.5010\n",
            "Epoch: [97/100], Step: [200/301], Loss: 0.0774, Dev: 0.5020\n",
            "Epoch: [97/100], Step: [300/301], Loss: 0.0685, Dev: 0.5120\n",
            "Epoch: [98/100], Step: [100/301], Loss: 0.0477, Dev: 0.4980\n",
            "Epoch: [98/100], Step: [200/301], Loss: 0.1067, Dev: 0.4720\n",
            "Epoch: [98/100], Step: [300/301], Loss: 0.0773, Dev: 0.5230\n",
            "Epoch: [99/100], Step: [100/301], Loss: 0.0964, Dev: 0.4860\n",
            "Epoch: [99/100], Step: [200/301], Loss: 0.0759, Dev: 0.5010\n",
            "Epoch: [99/100], Step: [300/301], Loss: 0.0817, Dev: 0.4960\n",
            "Epoch: [100/100], Step: [100/301], Loss: 0.0869, Dev: 0.5290\n",
            "Epoch: [100/100], Step: [200/301], Loss: 0.0682, Dev: 0.4860\n",
            "Epoch: [100/100], Step: [300/301], Loss: 0.0706, Dev: 0.5610\n",
            "Epoch: [1/100], Step: [100/301], Loss: 2.8316, Dev: 0.9270\n",
            "Epoch: [1/100], Step: [200/301], Loss: 2.8528, Dev: 0.9470\n",
            "Epoch: [1/100], Step: [300/301], Loss: 1.9199, Dev: 0.9550\n",
            "Epoch: [2/100], Step: [100/301], Loss: 1.3349, Dev: 0.9660\n",
            "Epoch: [2/100], Step: [200/301], Loss: 1.3225, Dev: 0.9610\n",
            "Epoch: [2/100], Step: [300/301], Loss: 2.2108, Dev: 0.9760\n",
            "Epoch: [3/100], Step: [100/301], Loss: 0.8902, Dev: 0.9860\n",
            "Epoch: [3/100], Step: [200/301], Loss: 1.0413, Dev: 0.9610\n",
            "Epoch: [3/100], Step: [300/301], Loss: 1.2342, Dev: 0.9620\n",
            "Epoch: [4/100], Step: [100/301], Loss: 0.6835, Dev: 0.9790\n",
            "Epoch: [4/100], Step: [200/301], Loss: 0.9269, Dev: 0.9810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDwN2u8xnkLB"
      },
      "source": [
        "print(\"Best Accuracy working with LogisticRegression and simple early stop:\")\r\n",
        "best_acc, best_epochs, best_lr, best_l2, best_early_stop_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51YRVVh44ssy"
      },
      "source": [
        "best_acc = 0.0 # best accuracy achieved \r\n",
        "best_lr = 0.0 # best learning rate at best accuracy \r\n",
        "best_l2 = 0.0 # best l2 regularizing weight\r\n",
        "best_epochs = 0 # best number of epochs\r\n",
        "# TODO: implement grid search to make sure the above 4 variable have \r\n",
        "for lr in [1.0, 0.1]:\r\n",
        "  for l2 in [0.001, 0.0001, 0]:\r\n",
        "    for early_stop_decision in [\"complex\"]:\r\n",
        "      model = LogisticRegression(num_features=vocab.get_vocab_size(\"tokens\"))\r\n",
        "      acc, epochs = training_loop(model, iterator, training_data, dev_data, lr=lr, weight_decay = l2, early_stop_decision = early_stop_decision)\r\n",
        "    \r\n",
        "    if acc > best_acc:\r\n",
        "      print(epochs)\r\n",
        "      best_acc = acc\r\n",
        "      best_lr = lr\r\n",
        "      best_l2 = l2\r\n",
        "      best_epochs = epochs\r\n",
        "      best_early_stop_decision = early_stop_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkdK7BqQ4shL"
      },
      "source": [
        "print(\"Best Accuracy working with LogisticRegression and complex early stop:\")\r\n",
        "best_acc, best_epochs, best_lr, best_l2, best_early_stop_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSz2_Tek_KjQ"
      },
      "source": [
        "best_acc = 0.0 # best accuracy achieved \r\n",
        "best_lr = 0.0 # best learning rate at best accuracy \r\n",
        "best_l2 = 0.0 # best l2 regularizing weight\r\n",
        "best_epochs = 0 # best number of epochs\r\n",
        "# TODO: implement grid search to make sure the above 4 variable have \r\n",
        "for lr in [1.0, 0.1]:\r\n",
        "  for l2 in [0.001, 0.0001, 0]:\r\n",
        "    for early_stop_decision in [\"simple\"]:\r\n",
        "      model = LogisticRegressionMeanPooling(num_features=vocab.get_vocab_size(\"tokens\"))\r\n",
        "      acc, epochs = training_loop(model, iterator, training_data, dev_data, lr=lr, weight_decay = l2, early_stop_decision = early_stop_decision)\r\n",
        "    \r\n",
        "    if acc > best_acc:\r\n",
        "      print(epochs)\r\n",
        "      best_acc = acc\r\n",
        "      best_lr = lr\r\n",
        "      best_l2 = l2\r\n",
        "      best_epochs = epochs\r\n",
        "      best_early_stop_decision = early_stop_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5qKK8HX4lYR"
      },
      "source": [
        "print(\"Best Accuracy working with LogisticRegressionMeanPooling and simple early stop:\")\r\n",
        "best_acc, best_epochs, best_lr, best_l2, best_early_stop_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40BWuBzN4j1C"
      },
      "source": [
        "best_acc = 0.0 # best accuracy achieved \r\n",
        "best_lr = 0.0 # best learning rate at best accuracy \r\n",
        "best_l2 = 0.0 # best l2 regularizing weight\r\n",
        "best_epochs = 0 # best number of epochs\r\n",
        "# TODO: implement grid search to make sure the above 4 variable have \r\n",
        "for lr in [1.0, 0.1]:\r\n",
        "  for l2 in [0.001, 0.0001, 0]:\r\n",
        "    for early_stop_decision in [\"complex\"]:\r\n",
        "      model = LogisticRegressionMeanPooling(num_features=vocab.get_vocab_size(\"tokens\"))\r\n",
        "      acc, epochs = training_loop(model, iterator, training_data, dev_data, lr=lr, weight_decay = l2, early_stop_decision = early_stop_decision)\r\n",
        "    \r\n",
        "    if acc > best_acc:\r\n",
        "      print(epochs)\r\n",
        "      best_acc = acc\r\n",
        "      best_lr = lr\r\n",
        "      best_l2 = l2\r\n",
        "      best_epochs = epochs\r\n",
        "      best_early_stop_decision = early_stop_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGhEcCXtnsK5"
      },
      "source": [
        "print(\"Best Accuracy working with LogisticRegressionMeanPooling and complex early stop:\")\r\n",
        "best_acc, best_epochs, best_lr, best_l2, best_early_stop_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRUlRGIWiqW9"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zDrPphzsTBf"
      },
      "source": [
        "## Validity Check\n",
        "This is a way for you to check whether you accidentially renamed answer variables or functions that we will use for automatic evaluation. Note that this is not a comprehensive list and we do not check here whether you accidentially changed the function signatures, so failing this validity check is only a sufficient condition for telling you something went wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEnoXoVjsVQ_"
      },
      "source": [
        "for answer in [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, QFormulation]:\n",
        "  assert isinstance(answer, str)\n",
        "\n",
        "for fun in [\n",
        "    construct_scaled_identity, \n",
        "    mean_diagonal, \n",
        "    bottom_right_matrix,\n",
        "    transpose_sum,\n",
        "    matrixvector1,\n",
        "    matrixvector2,\n",
        "    matrixvector3,\n",
        "    matrixvector4,\n",
        "    matrixvector5,\n",
        "    fw,\n",
        "    bw,\n",
        "    SortBy,\n",
        "    collapse_mr_dot,\n",
        "    LogisticRegressionMeanPooling,\n",
        "    add_features,\n",
        "    accuracy,\n",
        "    training_loop\n",
        "    ]:\n",
        "  assert callable(fun)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}